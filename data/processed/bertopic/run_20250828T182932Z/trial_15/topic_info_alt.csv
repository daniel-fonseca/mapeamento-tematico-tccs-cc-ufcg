Topic,Count,Name,Representation,Representative_Docs
-1,187,-1_software_projeto_modelo_aplicativo,"['software', 'projeto', 'modelo', 'aplicativo', 'tcnicas', 'animais', 'brasil', 'caractersticas', 'abordagem', 'modelos']","['todas as áreas de atividades precisaram se adaptar por causa do distanciamento social provocado pelo novo coronavírus. o ensino à distância (ead) foi a principal alternativa para estudantes e professores durante a quarentena. para que o ensino não presencial ocorresse de forma estruturada e eficiente, o uso de plataformas digitais de apoio ao ensino não presencial e ead foi essencial. ead e ensino não presencial são modalidades distintas, com características em comum. o ensino não presencial diz respeito às atividades de ensino mediadas por tecnologias e orientadas pelos princípios da educação presencial, enquanto o ead, que também utiliza as plataformas digitais, tem seu formato próprio de ensino-aprendizagem. diante da pandemia, houve um aumento dos usuários nas plataformas digitais, e, consequentemente, a coleta de dados pelas mesmas foi intensificada. é importante ressaltar que o fornecimento, tratamento e armazenamento dos dados pessoais dos usuários precisam estar alinhados com a legislação a fim de coibir o uso indiscriminado dos dados coletados. nesse sentido, o presente artigo aborda a temática do uso dos dados pelas plataformas digitais utilizadas no ensino superior durante a pandemia. a pesquisa tem como objetivo analisar as plataformas google classroom, zoom cloud meeting, microsoft teams e slack para saber como as mesmas tratam os dados dos usuários. a conhecida lei geral de proteção de dados (brasil, ), cuja sigla é lgpd, estabelece regras sobre coleta, armazenamento, tratamento e compartilhamento de dados. a metodologia parte da pesquisa das políticas de privacidade das plataformas. desta forma, os resultados esperados atendem a dois propósitos: por um lado, contribuir para que o usuário saiba como são usados e armazenados os seus dados; por outro lado sinalizar recomendações para os usuários das plataformas.', 'modelos preditivos em aprendizado de máquina e processos de descoberta de conhecimento em bases de dados, particularmente em domínios como o basquete, são inestimáveis para obter insights sobre o desempenho dos jogadores. este estudo compara abordagens de aprendizado de máquina supervisionado (modelos de caixa preta e caixa branca, incluindo métodos de conjunto) para analisar dados estatísticos de jogadores de basquete universitário (ncaa). nosso objetivo é identificar jogadores da ncaa com alto potencial para sucesso na nba, determinar quais características dos jogadores mais influenciam as decisões de seleção e como esses modelos chegam a tais conclusões para comparar seus desempenhos e a explicabilidade associada. esta tarefa é desafiadora devido a fatores além das estatísticas, como o contexto do jogador e as considerações do elenco da equipe durante a seleção. o objetivo principal é fornecer aos tomadores de decisão insights cruciais para a seleção de jogadores, ajudar na melhor avaliação de jogadores e desenvolver jovens talentos enfatizando aspectos-chave do jogo. comparamos os resultados de modelos de predição interpretáveis com níveis satisfatórios de precisão. equilibrando interpretabilidade e precisão preditiva, empregamos métodos de classificação de caixa branca, caixa preta e de conjunto, como árvores de decisão, regressão logística, máquina de vetores de suporte, perceptron multicamadas, floresta aleatória e xgboost. além disso, algoritmos genéticos foram usados para reduzir o conjunto de características de cada modelo, retendo apenas as características mais impactantes. comparado aos procedimentos padrão sem seleção de características, todos os modelos mostraram desempenho melhorado. encontramos diferenças mínimas na precisão preditiva entre os melhores modelos de caixa branca e caixa preta. a combinação de algoritmos genéticos e regressão logística superou a precisão preditiva de outros modelos, reduzindo significativamente as características e melhorando a interpretabilidade dos resultados. a análise também destaca as características mais influentes no modelo e como os modelos chegaram a tais conclusões.', 'a observabilidade desempenha um papel importante no desenvolvimento e na manutenção de software. podemos dizer que um sistema é observável quando pode-se entender e explicar qualquer estado em que o mesmo possa entrar, podendo ele ser corriqueiro ou algo totalmente novo. juntamente com métricas e traces, os logs representam um dos pilares da observabilidade, desempenhando um papel vital na depuração dos estados de um sistema. isso ressalta sua importância como fonte de dados e a necessidade de seu tratamento e armazenamento. nesse contexto, o opentelemetry emerge como um framework e conjunto de ferramentas que se propõe a facilitar a coleta e a gestão de dados de observabilidade em sistemas. sendo independente de fornecedores e ferramentas, e adotando um modelo de código aberto, o opentelemetry se revela um software altamente versátil, adaptável às necessidades individuais de seus usuários, tornando-se uma escolha ideal na implementação de observabilidade em sistemas. o foco deste trabalho está no aprimoramento de um módulo utilizado em um coletor opentelemetry, cuja função principal é receber logs em uma plataforma de comércio eletrônico. esse módulo compreende dois componentes: o wal, responsável por detectar falhas no envio de logs ao opensearch e armazenar logs não enviados em um serviço de armazenamento de objetos; e um replayer de logs, que tenta reenviar os logs armazenados posteriormente ao opensearch. entretanto, o replayer de logs enfrenta desafios relacionados a disponibilidade de recursos de hardware, instabilidade em ambientes variáveis e limitações na configuração, o que impacta negativamente em sua eficácia no envio de logs ao opensearch. além disso, a ausência de dados sobre a saúde e o desempenho do wal pode dificultar a manutenção e depuração deste componente, devido à falta de informações relevantes. diante desse cenário, este trabalho tem como objetivo aprimorar o replayer de logs, visando melhorar a disponibilidade e a utilização dos recursos de hardware, aumentar sua confiabilidade no envio de logs ao opensearch e torná-lo mais flexível em termos de configuração. também, pretende-se adicionar capacidades de observabilidade ao mecanismo wal com o objetivo de garantir maior visibilidade do funcionamento do mecanismo e facilitar a depuração do mesmo.']"
0,100,0_software_projetos_nuvem_cdigo,"['software', 'projetos', 'nuvem', 'cdigo', 'aplicaes', 'qualidade', 'desenvolvimento software', 'projeto', 'monitoramento', 'desenvolvedores']","['""um estudo sobre metodologias ágeis e sua correlação com o êxito em projetos de software"" investiga a eficácia das metodologias ágeis em projetos de desenvolvimento de software e sua relação com o sucesso. o estudo aprofunda a compreensão das práticas e princípios fundamentais das metodologias ágeis, como o manifesto ágil e os princípios ágeis. além disso, a pesquisa analisa como diversos fatores, como o apoio executivo, a maturidade emocional das equipes, o envolvimento dos usuários, a otimização de recursos, a qualificação dos membros da equipe e outros elementos, impactam a taxa de sucesso de projetos ágeis. também são investigados os obstáculos e desafios comuns que as organizações enfrentam ao implementar essas metodologias, incluindo a resistência à mudança e conflitos culturais. o trabalho enfatiza a relevância das metodologias ágeis na melhoria do desempenho em projetos de software e fornece diretrizes valiosas para organizações que buscam adotar ou aprimorar suas práticas ágeis. o estudo oferece uma visão abrangente do universo ágil, abordando suas vantagens e desafios, e apresenta recomendações práticas para aprimorar a implementação de abordagens ágeis nas empresas de desenvolvimento de software.', 'no processo de desenvolvimento de software, a aquisição e manutenção de hardware adequado para as necessidades de programação podem resultar em altos custos de investimento de capital. a alternativa de uso de recursos em nuvem oferece flexibilidade, porém o gerenciamento desses recursos pode ser complexo e oneroso, requerendo conhecimentos especializados em operações em nuvem. o problema consiste em gerenciar um ambiente de desenvolvimento na nuvem de forma eficiente, evitando altos custos de aquisição e manutenção de hardware próprio, além de simplificar o gerenciamento de recursos ao alugar máquinas na nuvem, buscando minimizar despesas e eliminar a necessidade de expertise complexa em operações em nuvem. propomos o desenvolvimento de uma ferramenta de linha de comando, destinada a simplificar o gerenciamento do ambiente de desenvolvimento de software. essa ferramenta terá a capacidade de criar, configurar e gerenciar recursos na nuvem de forma automatizada e eficiente. uma característica diferencial é a utilização de instâncias preemptivas oferecidas por provedores de nuvem, permitindo aproveitar recursos ociosos a custos ainda mais baixos, sem comprometer a qualidade do ambiente de desenvolvimento. espera-se que o usuário seja capaz de criar ambientes de desenvolvimento utilizando a ferramenta proposta integrando-a com outras soluções já existentes para desenvolvimento de código. ao oferecer uma solução intuitiva, nossa abordagem visa otimizar o ambiente de desenvolvimento, maximizando a economia e eliminando a necessidade de conhecimentos avançados em operações em nuvem por parte da equipe de desenvolvimento. ao final deste trabalho, a usabilidade da ferramenta foi validada e demonstrou ser eficaz na simplificação do gerenciamento dos ambientes. a maioria dos participantes conseguiu gerenciar ambientes com sucesso, destacando a facilidade de uso e a utilidade da documentação fornecida.', 'o github é a plataforma de hospedagem de código e controle de versão mais utilizada atualmente. diariamente, inúmeros projetos são criados, estendidos e modificados por diferentes usuários. entretanto, muitos projetos que possivelmente seriam do interesse de determinados usuários, acabam por passar despercebidos diante da grande quantidade de projetos disponíveis. neste contexto, surge a necessidade de algum mecanismo que possa auxiliar o usuário a encontrar projetos que podem ser de seu interesse. já existe na literatura trabalhos que buscam analisar fatores de interesse com o objetivo de recomendar projetos, entretanto ainda há margem para utilização de outros fatores e critérios na tentativa de obter resultados melhores. para tanto, o presente trabalho busca utilizar features, algumas já propostas na literatura e outras ainda não utilizadas nesse contexto, disponíveis em projetos do github, com o auxílio de algoritmos de learning to rank, para encontrar relações de interesse em projetos e assim recomendá-los para o usuário. verificamos a efetividade de learning to rank para recomendação de projetos usando os algoritmos lambdamart, random forest e coordinate ascent, utilizando como base repositórios e usuários do github. os resultados mostram que a abordagem de learning to rank para recomendação de projetos é promissora e efetiva, ao mesmo tempo que oferece muito espaço para aprimoramento.']"
1,49,1_modelos_programao_imagens_modelo,"['modelos', 'programao', 'imagens', 'modelo', 'texto', 'natural', 'questes', 'tcnicas', 'processamento', 'linguagem natural']","['nos últimos anos, avanços na área de inteligência artiicial permitiram desenvolvimento de modelos de processamento de linguagem natural em prol de aplicações em diversos contextos, como a automatização do processo de elaboração de questões sobre temas especíicos. atualmente, existem modelos capazes de formular questões sobre um tópico qualquer após receber como entrada um texto relevante. tais projetos possuem grande potencial auxiliar no contexto educacional, entretanto, ainda existe carência de um sistema que forneça a seus usuários uma maneira fácil e intuitiva de utilizar esses modelos. esse trabalho tem como objetivo o desenvolvimento de uma aplicação web que supra essa necessidade, incorporando modelos de processamento de linguagem natural que recebem como entrada um texto e geram para o usuário uma lista de perguntas relevantes ao tema. a utilização da aplicação web também traz a oportunidade de obter feedback dos usuários sobre a qualidade das perguntas geradas, informação que pode ser utilizada para retroalimentação e aprimoramento dos modelos utilizados.', 'avanços recentes em modelos de linguagem de grande escala (llms) expandiram significativamente as capacidades da inteligência artificial (ia) em tarefas de processamento de linguagem natural. no entanto, seu desempenho em domínios especializados, como a ciência da computação, permanece relativamente pouco explorado. este estudo investiga se os llms podem igualar ou superar o desempenho humano no poscomp, um exame brasileiro prestigiado usado para admissões de pós-graduação em ciência da computação. quatro llms-chatgpt- , gemini . advanced, claude sonnet e le chat mistral large-foram avaliados nos exames poscomp de e . a avaliação consistiu em duas análises: uma envolvendo interpretação de imagens e outra somente de texto, para determinar a proficiência dos modelos em lidar com questões complexas típicas do exame. os resultados indicaram que os llms tiveram um desempenho significativamente melhor nas questões baseadas em texto, com a interpretação de imagens representando um grande desafio. por exemplo, na avaliação baseada em imagens, o chatgpt- respondeu corretamente de perguntas, enquanto o gemini . advanced conseguiu apenas respostas corretas. na avaliação baseada em texto de , o chatgpt- liderou com respostas corretas, seguido por gemini . advanced ( ), le chat mistral ( ) e claude sonnet ( ). o exame de mostrou tendências semelhantes.', 'modelos de linguagem de grande escala (llms), como o chatgpt, claude e llama , revolucionaram o processamento de linguagem natural, criando novos casos de uso para aplicações que utilizam esses modelos em seus fluxos de trabalho. no entanto, os altos custos computacionais desses modelos acarretam problemas de custo e latência, impedindo a escalabilidade de funcionalidades baseadas em llm para muitos serviços e produtos, especialmente quando dependem de modelos com melhores capacidades de raciocínio, como o gpt- ou o claude opus. além disso, muitas consultas a esses modelos são duplicadas. o cache tradicional é uma solução natural para esse problema, mas sua incapacidade de determinar se duas consultas são semanticamente equivalentes leva a baixas taxas de cache hit. neste trabalho, propomos explorar o uso de cache semântico, que considera o significado das consultas em vez de sua formulação exata, para melhorar a eficiência de aplicações baseadas em llm. realizamos um experimento usando um conjunto de dados real da alura, uma empresa brasileira de educação, em um cenário onde um aluno responde a uma pergunta e o gpt- corrige a resposta. os resultados mostraram que , % das solicitações feitas ao llm poderiam ter sido atendidas a partir do cache usando um limiar de similaridade de . , com uma melhoria de - vezes na latência. esses resultados demonstram o potencial do cache semântico para melhorar a eficiência de funcionalidades baseadas em llm, reduzindo custos e latência enquanto mantém os benefícios de modelos avançados de linguagem como o gpt- . essa abordagem poderia possibilitar a escalabilidade de funcionalidades baseadas em llm para uma gama mais ampla de aplicações, avançando na adoção desses modelos poderosos em diversos domínios.']"
2,32,2_privacidade_sociais_tweets_mundo,"['privacidade', 'sociais', 'tweets', 'mundo', 'cidadania', 'cidadania digital', 'internet', 'digitais', 'vez', 'tcnicas']","['nos últimos anos, as apostas esportivas têm experimentado um crescimento rápido devido à digitalização e à acessibilidade das plataformas online. contudo, esse aumento também gerou desafios, como a integridade do mercado, o combate a práticas de crime organizado, como lavagem de dinheiro, e a proteção dos consumidores. nesse cenário, os processos de verificação e validação de documentos, conhecidos como kyc (know your customer - conheça seu cliente), tornaram-se essenciais para garantir a conformidade regulatória e a segurança dos usuários. este estudo se concentra nos desafios enfrentados pelas casas de apostas ao implementar os processos de kyc em conformidade com as regulamentações, especialmente na necessidade de análise manual para validar os processos de verificação de kyc. essas verificações manuais são tipicamente demoradas e custosas. diante desse cenário, foi desenvolvida uma ferramenta de automação que utiliza um modelo llm (large language model) para reduzir a dependência de intervenções humanas e aprimora a eficiência no processo de verificação de documentos de kyc. a metodologia utilizada abrangeu uma amostra de dados de usuários do período de de dezembro de a de fevereiro de , levando em consideração as principais falhas identificadas durante esse intervalo de tempo. os resultados obtidos mostram uma melhoria significativa na performance das verificações de análises manuais. dessa forma, da amostra realizada com casos o algoritmo forneceu um veredito para . % casos, com precisão de , % na categorização correta do status da análise do usuário. isso destaca a eficácia geral do algoritmo, tornando o processo mais ágil e eficiente, reduzindo significativamente a carga de trabalho humana e, em contra partida, . % dos casos ainda exigiram análise manual para maior precisão, evidenciando situações em que o algoritmo não conseguiu tomar uma decisão automática. contudo, a pesquisa não apenas oferece uma solução prática e tecnológica para os desafios enfrentados pelas casas de apostas, mas também contribui para o avanço na automação de processos de verificação de documentos em ambientes regulamentados.', 'o presente artigo trata-se de uma análise de sentimentos e emoções expressos em tweets relacionados à guerra da ucrânia, mediante análise dos tópicos discutidos pelos usuários da plataforma twitter. este estudo visa compreender como os usuários reagem ao evento em curso, quais aspectos da guerra as pessoas estão discutindo na plataforma, e como se sentem a respeito deste acontecimento. além disso, visa identificar correlações entre as variáveis presentes nos tweets, como localização, informações de perfil do usuário autor da postagem, e a natureza de suas opiniões. tais análises foram conduzidas através de tarefas de processamento de linguagem natural como análises exploratórias dos dados e a aplicação de classificadores de sentimentos de tweets utilizando modelos de dados pré-treinados. os dados analisados contém tweets coletados desde o início do conflito, que se deu em fevereiro de até outubro de , e foram coletados a partir de hashtags relacionadas à guerra. para a realização das análises de sentimento e emoção foram utilizados a variante roberta. os tweets foram classificados em sentimentos como positivos, negativos ou neutros, e em emoções como alegria, raiva, medo, nojo, otimismo, pessimismo, surpresa e amor. os resultados mostraram que a maioria dos tweets em inglês expressam raiva e antecipação como emoções predominantes, e sentimentos negativos e neutros com maior predominância, atingindo mais de % do da amostragem analisada. algumas das frases mais recorrentes na análise fazem alusão ao apoio à ucrânia e pedindo o fim da guerra. da mesma forma, frases de preocupação com a crise, armas e fatalidades são recorrentes. na maioria das postagens, pessoas demonstram preocupação com o conflito armado e apoio à ucrânia. trabalhos futuros poderiam utilizar mais tweets para abranger a análise e visualizar a correlação de mais atributos relacionados às postagens como os engajamentos e curtidas.', 'em um mundo onde as pessoas têm cada vez mais contato com as tecnologias, surgiram também questões sobre como utilizá-las adequadamente. muitas vezes, essas questões surgem em respostas a perigos percebidos ou comportamentos inapropriados como roubo de identidade, cyberbullying ou disseminação de notícias falsas, especialmente no mundo online. tais abusos podem ser ainda mais prejudiciais aos adolescentes que cada vez mais cedo são inseridos nesse contexto tecnológico. apesar de os usuários terem capacidade de avaliar uma conduta online adequada e identificar riscos, é necessário que eles desenvolvam um senso crítico para utilizar a internet de forma responsável, ou seja, usar o ambiente online sem prejudicar a si mesmos e aos outros. para isso, a cidadania digital surge como um conjunto de normas de comportamento que devem ser seguidas para o uso amigável, seguro e ético das tecnologias. logo, a presente pesquisa tem como objetivo analisar as principais tecnologias digitais utilizadas por adolescentes e, a partir delas, listar formas de uso responsável de acordo com os elementos da cidadania digital a fim de conduzir de forma positiva o comportamento de adolescentes na sociedade e constituir uma cultura de uso seguro das tecnologias digitais.']"
3,31,3_universidade_disciplinas_ufcg_estudantes,"['universidade', 'disciplinas', 'ufcg', 'estudantes', 'federal campina', 'universidade federal', 'universidade federal campina', 'federal', 'federal campina grande', 'discentes']","['ao longo de todo o regime acadêmico é necessário que os discentes façam diversas matrículas, a fim de efetuar a inscrição em disciplinas do período acadêmico vigente. nesse contexto, principalmente com o grande volume de matrículas realizadas, se faz necessário o melhoramento da organização, do gerenciamento, do controle e do acompanhamento das matrículas acadêmicas. atualmente, mesmo diante de uma crescente modernização sistemática, o sistema de matrículas universitárias da universidade federal de campina grande é muito dependente do coordenador acadêmico, que deve manualmente: inscrever turmas, modificar quantidades de vagas e efetuar todo o planejamento de turmas manualmente. além disso, o sistema não avisa aos estudantes sobre turmas ideais para suas matrículas e nem sobre o horário inicial da abertura do período de inscrição das disciplinas. nesse viés, o eureca dashboard, surge como uma ferramenta facilitadora, na qual o coordenador acadêmico pode visualizar a oferta de vagas ideal para as disciplinas, modificar a disponibilização das disciplinas e acompanhar a inscrição dos discentes, tudo isso de forma unificada e centralizada, com um acesso facilitado e seguro. com isso, o trabalho desses gerenciadores será facilitado, e através de uma entrevista, com um usuário alvo, será possível medir o real impacto da plataforma e se ela foi fundamental no gerenciamento de tempo desses profissionais.', 'a unidade acadêmica de sistemas e computação (uasc) possui (onze) laboratórios focados em diversas áreas da tecnologia e estes adotam processos seletivos para alocação de discentes em seus projetos. a maioria das ofertas de vagas feitas pelos docentes para esses projetos é realizada via e-mail acadêmico, onde estas podem passar despercebidas por diferentes motivos e, além disso, não estão centralizadas em uma plataforma que contenha todas as chamadas para projetos. portanto, há uma lacuna a ser resolvida principalmente no que diz respeito à gestão destas seleções, tanto pelos professores, quanto pelos alunos. este trabalho tem a proposta de desenvolver o vivagas, uma plataforma que tem como objetivo principal facilitar a gestão de todo o processo seletivo mencionado anteriormente. o vivagas oferece funcionalidades tanto para professores quanto para alunos. professores publicam vagas, buscam e filtram alunos com base em seus perfis e conhecimentos, e realizam o processo seletivo de forma eficiente. já os alunos têm acesso à visualização de vagas disponíveis, podem filtrá-las com base em critérios como área de atuação e candidatar-se nas vagas disponíveis. a avaliação da plataforma foi realizada utilizando o think aloud protocol, que consiste em um teste de pensamento em voz alta no qual o usuário faz uso do sistema. além disso, testes foram realizados através da versão adaptada do questionário csuq com objetivo de recolher métricas de usabilidade de um dado sistema. também foi utilizado a ferramenta lighthouse que realiza auditorias automatizadas em páginas da web. os resultados do lighthouse indicaram áreas de otimização, mas destacaram a ótima acessibilidade, práticas de desenvolvimento e otimização. o csuq refletiu uma avaliação bastante positiva. da mesma forma, o think aloud protocol forneceu insights valiosos e clareza do fluxo de usabilidade. em suma, a avaliação geral apontou para bons resultados e uma excelente usabilidade do vivagas.', 'os dias que antecedem a semana de matrículas da universidade federal de campina grande é o período em que os alunos dedicam parte do seu tempo organizando seus horários e planejando quais disciplinas pretendem cursar no próximo semestre. nesse período, as coordenações disponibilizam listas, através do sistema de “controle acadêmico” da universidade, com os dados de professores e horários para as disciplinas daquele semestre. esse trabalho tem o intuito de auxiliar o planejamento dos horários para o período de matrículas dos cursos de graduação na universidade federal de campina grande, com o desenvolvimento de um sistema web que permite o aluno organizar e planejar suas disciplinas através de uma interface agradável e que proporcione uma melhor experiência para a matrícula. para verificar a satisfação dos usuários quanto a usabilidade do sistema foi realizado um levantamento, utilizando o computer system usability questionnaire, que analisando as médias das respostas, foi obtido, na maioria dos itens, valores entre e , sendo o valor máximo, que apontam bons indicadores e alto nível de satisfação.']"
4,24,4_pandemia_sade_covid_pandemia covid,"['pandemia', 'sade', 'covid', 'pandemia covid', 'doenas', 'perodo', 'brasil', 'crise', 'gesto', 'oftalmolgicas']","['diante da atípica situação causada pela pandemia de covid- , ocorrida entre o final de e meados de , o ministério da educação autorizou o ensino remoto em cursos outrora presenciais. essa medida possuía o intuito de amenizar possíveis prejuízos causados ao sistema federal de ensino pela pandemia. nesse contexto, a unidade acadêmica de sistemas e computação da universidade federal de campina grande, juntamente com seu corpo docente, se viu diante do desafio de adaptar sua execução curricular ao ambiente virtual. passada a pandemia, surge a necessidade de analisar as consequências dela, em virtude da necessidade de adaptação das metodologias das aulas ao ensino remoto. este trabalho propõe o emprego de técnicas de análise e visualização de dados, a partir da coleta dos dados do sistema de controle acadêmico online (scao), tendo como objetivo contribuir para a análise de efeitos e implicações exercidos pela pandemia de covid- sobre os estudantes de graduação do curso ciência da computação da universidade federal de campina grande. como resultados encontrados, pode-se citar o fato de que o desempenho acadêmico dos alunos, na maior parte das disciplinas, voltou a alcançar os mesmos níveis do período da pré-pandemia, com uma tendência de aumento de trancamentos durante o pós-pandemia, sendo este o período que possui a maior quantidade de trancamentos registrados.', 'durante a pandemia de covid- , foi necessário que as pessoas se isolassem, devido aos altos índices de contágio e eventuais mortes. de acordo com o ministério da saúde, apenas no brasil já foram mais de milhões de casos confirmados e mil mortes, até o início do ano de . com isso, a população precisou abrir mão de suas formas usuais de entretenimento presencial e socialização, o que levou as formas de passatempo digital a aparecerem como forma alternativa de diversão e socialização. entre elas, uma das que mais viu crescimento neste período foi o uso dos jogos digitais, sendo apreciados até por quem nunca tinha tido contato anterior ao período de isolamento. por isso, neste trabalho, pretendemos analisar como o crescimento do uso dos jogos digitais durante a pandemia gerou impactos positivos e negativos na vida de alunos universitários brasileiros após a pandemia. para isso, foi encaminhado para tais estudantes formulários do google forms, permitindo assim a geração de gráficos comparativos a partir da coleta e análise de dados. assim, conseguimos comprovar um relacionamento entre este crescimento aos efeitos observados após o fim da mesma, tanto para quem já tinha contato com jogos como para quem teve sua primeira experiência. ao final da pesquisa, conseguimos obter respostas de estudantes universitários de graduação e pós, conseguindo obter uma boa perspectiva desta conexão e esperando com isso incentivar trabalhos futuros.', 'com o fim da pandemia do covid- , muitas organizações estão considerando a possibilidade de retornar ao trabalho presencial. nesse contexto, é importante avaliar as percepções das equipes de desenvolvimento de software no brasil sobre esse retorno. este estudo investigou como as equipes brasileiras de desenvolvimento de software lidaram com o retorno ao trabalho presencial/híbrido após a pandemia e como essa mudança de trabalho repercutiu no processo de desenvolvimento de software. aplicamos uma pesquisa com participantes de equipes de desenvolvimento de software e investigamos aspectos, como: rotina de trabalho, colaboração, comunicação, produtividade, bem-estar, auxílio oferecido pelas empresas e processo de desenvolvimento de software. realizamos uma análise quantitativa e qualitativa dos resultados da pesquisa e os comparamos com estudos anteriores. nossas principais conclusões sobre os participantes que retornaram ao trabalho presencial/híbrido são: (i) % dos participantes afirmaram que mantém uma rotina de trabalho estável relacionado ao horário padrão da empresa; (ii) , % dos participantes consideram sua equipe colaborativa; (iii) , % estão satisfeitos com a comunicação no regime de trabalho presencial/híbrido em comparação ao work from home (wfh) durante a pandemia de covid- ; (iv) % estão satisfeitos com o seu bem-estar; (v) % estão satisfeitos com sua produtividade; (vi) , % estão satisfeitos com as medidas de segurança adotadas pela empresa no pós pandemia e (vii) , % afirmaram ter mudado no processo de desenvolvimento de software devido ao retorno ao trabalho presencial/híbrido. as principais mudanças positivas no processo estão relacionadas às práticas de: divisão de tarefas, segurança e eficiência.']"
