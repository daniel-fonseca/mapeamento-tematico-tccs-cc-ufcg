Topic,Count,Name,Representation,Representative_Docs
-1,80,-1_animais_jogo_bugs_relatrios,"['animais', 'jogo', 'bugs', 'relatrios', 'software', 'caractersticas', 'programao', 'bug', 'jogadores', 'modelos']","['dentre as atividades típicas de um processo de software, podemos destacar as tarefas de testar, analisar, reportar e corrigir bugs. a realização dessas tarefas é importante para identificar erros comuns ou complexos durante todas as etapas do desenvolvimento, evitando retrabalho e entregando um software com mais qualidade e confiabilidade [ ]. em um relatório de bug, geralmente, seu autor oferece detalhes da anormalidade que vem ocorrendo. tipicamente, um relatório de bug é aberto, o bug é corrigido e o relatório é fechado. contudo, por vezes, é verificado que a correção do bug não foi eficaz, seja por falta de descrição mais objetiva no relatório, seja por dificuldade de entendimento por parte do desenvolvedor. assim, faz-se necessária a reabertura do bug, adicionando tempo no processo de desenvolvimento, tornando o software mais custoso. por isso, é importante investigar o que pode ser feito para mitigar tais problemas. neste trabalho, investigamos as características que levam um bug a ser reaberto. os resultados deste trabalho podem ajudar aos usuários finais e desenvolvedores a melhor escrever relatórios de bugs, bem como aos desenvolvedores a melhor entendê-los e tratá-los. o estudo utilizou um dataset extraído da ferramenta bugzilla.', 'este artigo oferece um estudo sobre a evolução do basquete brasileiro, empregando índices estatísticos de desempenho (ied) como métrica para avaliar o desempenho das equipes no contexto das três temporadas mais recentes do novo basquete brasil (nbb), que abrangem o período de a . o objetivo central desta pesquisa é explorar e analisar o desempenho das equipes de maior destaque no cenário do basquete brasileiro. os dados utilizados nesta análise foram coletados a partir da plataforma da liga nacional de basquete (lnb), a entidade responsável pela organização e gestão do nbb. foram analisados jogos ao longo dessas três temporadas, abrangendo diversos índices estatísticos, que incluem a média e o desvio padrão de diversos parâmetros, tais como pontuação, arremessos (tentados e convertidos), aproveitamento de arremessos de dois pontos, três pontos e lances livres, assistências, rebotes, roubadas de bola, erros, bloqueios (tocos), eficiência e outros índices avançados associados ao esporte do basquete. os dados foram comparados utilizando a análise de variância (anova) e os valores de referência foram calculados a partir de percentis, permitindo uma análise comparativa e a identificação de tendências. nesse contexto, a temporada de emergiu como aquela que registrou os índices mais baixos, especialmente em termos de aproveitamento de arremessos, eficiência, média de pontos e assistências. por outro lado, as outras duas temporadas analisadas apresentaram números similares na maioria dos índices estudados, o que pode sugerir uma certa estabilidade nesses aspectos ao longo desse período. outro ponto observado no estudo é que as equipes que competem no nbb precisam atingir um alto nível de desempenho, conforme evidenciado pelos percentis utilizados como referência. além disso, o estudo revela uma clara tendência ao aumento do uso de arremessos de três pontos nos últimos anos do nbb, alinhando-se com as mudanças observadas no basquete em nível global, onde a bola de três pontos se tornou um elemento fundamental nas estratégias ofensivas.', 'modelos preditivos em aprendizado de máquina e processos de descoberta de conhecimento em bases de dados, particularmente em domínios como o basquete, são inestimáveis para obter insights sobre o desempenho dos jogadores. este estudo compara abordagens de aprendizado de máquina supervisionado (modelos de caixa preta e caixa branca, incluindo métodos de conjunto) para analisar dados estatísticos de jogadores de basquete universitário (ncaa). nosso objetivo é identificar jogadores da ncaa com alto potencial para sucesso na nba, determinar quais características dos jogadores mais influenciam as decisões de seleção e como esses modelos chegam a tais conclusões para comparar seus desempenhos e a explicabilidade associada. esta tarefa é desafiadora devido a fatores além das estatísticas, como o contexto do jogador e as considerações do elenco da equipe durante a seleção. o objetivo principal é fornecer aos tomadores de decisão insights cruciais para a seleção de jogadores, ajudar na melhor avaliação de jogadores e desenvolver jovens talentos enfatizando aspectos-chave do jogo. comparamos os resultados de modelos de predição interpretáveis com níveis satisfatórios de precisão. equilibrando interpretabilidade e precisão preditiva, empregamos métodos de classificação de caixa branca, caixa preta e de conjunto, como árvores de decisão, regressão logística, máquina de vetores de suporte, perceptron multicamadas, floresta aleatória e xgboost. além disso, algoritmos genéticos foram usados para reduzir o conjunto de características de cada modelo, retendo apenas as características mais impactantes. comparado aos procedimentos padrão sem seleção de características, todos os modelos mostraram desempenho melhorado. encontramos diferenças mínimas na precisão preditiva entre os melhores modelos de caixa branca e caixa preta. a combinação de algoritmos genéticos e regressão logística superou a precisão preditiva de outros modelos, reduzindo significativamente as características e melhorando a interpretabilidade dos resultados. a análise também destaca as características mais influentes no modelo e como os modelos chegaram a tais conclusões.']"
0,74,0_software_projetos_testes_desenvolvimento software,"['software', 'projetos', 'testes', 'desenvolvimento software', 'cdigo', 'ferramentas', 'qualidade', 'sistemas', 'projeto', 'processo']","['criar projetos de programação que necessitam de mais de um programador é uma prática comum na comunidade de desenvolvimento de software. isso ocorre porque muitos projetos exigem uma equipe de programadores com diferentes habilidades e conhecimentos para serem concluídos de maneira eficiente e eficaz. é comum ver programadores buscando outros profissionais para colaborar em seus projetos e, assim, alcançar um resultado de alta qualidade e escalabilidade. infelizmente, muitas vezes esse projeto não é levado adiante devido à falta de pessoas capacitadas e dispostas a trabalhar neles. o problema é a dificuldade em encontrar programadores qualificados e interessados em trabalhar em um projeto específico, o que pode levar a uma perda de tempo e recursos para aqueles que estão procurando por colaboradores. nesse contexto, o presente projeto propõe uma solução: uma aplicação web que organiza e conecta programadores para realizar projetos em conjunto. o usuário que se cadastrar no site irá disponibilizar informações sobre suas habilidades e conhecimentos na área de programação, permitindo que seja conectado a projetos disponíveis de acordo com seu perfil. além disso, o usuário também poderá criar seu próprio projeto, informando as tecnologias que serão utilizadas. dessa forma, espera-se que, ao final deste projeto, os programadores que desejam criar algum projeto ou empreender consigam encontrar facilmente outros programadores capacitados e dispostos a trabalhar com eles, superando a dificuldade encontrada atualmente na procura por colaboradores.', 'ao tomar decisões, gerentes de projetos de software precisam considerar diversos fatores, tornando essa uma atividade muito complexa. para auxiliar os gerentes com essas decisões, ferramentas têm sido utilizadas para simular o impacto desses fatores nos resultados do projeto. modelos em dinâmica de sistemas (sd) têm se mostrado uma boa opção para tais simulações por possuírem características dinâmicas e utilizarem sistemas de ""feedback"", aspectos próprios do desenvolvimento de projetos de software. o objetivo deste trabalho é identificar quais os fatores (antecedentes) que influenciam os projetos de software e o que eles influenciam (consequentes) nos projetos que já foram modelados em sd. para isso foi realizado um mapeamento por meio da base de indexação web of science (wos). os artigos foram avaliados por ordem de publicação partindo do mais recente e considerando os critérios de inclusão e exclusão propostos. alguns dos fatores mapeados foram: influência do cliente, promoção da equipe, pressão de cronograma, horas extras (antecedentes); produtividade do time, custo do projeto e duração do projeto (consequentes). a contribuição esperada é auxiliar pesquisadores que pretendem construir modelos sd a encontrarem fatores ainda não modelados ou reaproveitar os fatores já modelados por outros pesquisadores evitando re-trabalho.', '""um estudo sobre metodologias ágeis e sua correlação com o êxito em projetos de software"" investiga a eficácia das metodologias ágeis em projetos de desenvolvimento de software e sua relação com o sucesso. o estudo aprofunda a compreensão das práticas e princípios fundamentais das metodologias ágeis, como o manifesto ágil e os princípios ágeis. além disso, a pesquisa analisa como diversos fatores, como o apoio executivo, a maturidade emocional das equipes, o envolvimento dos usuários, a otimização de recursos, a qualificação dos membros da equipe e outros elementos, impactam a taxa de sucesso de projetos ágeis. também são investigados os obstáculos e desafios comuns que as organizações enfrentam ao implementar essas metodologias, incluindo a resistência à mudança e conflitos culturais. o trabalho enfatiza a relevância das metodologias ágeis na melhoria do desempenho em projetos de software e fornece diretrizes valiosas para organizações que buscam adotar ou aprimorar suas práticas ágeis. o estudo oferece uma visão abrangente do universo ágil, abordando suas vantagens e desafios, e apresenta recomendações práticas para aprimorar a implementação de abordagens ágeis nas empresas de desenvolvimento de software.']"
1,52,1_modelos_linguagem_imagens_modelo,"['modelos', 'linguagem', 'imagens', 'modelo', 'texto', 'llms', 'natural', 'documentos', 'linguagem natural', 'processamento']","['este trabalho de conclusão de curso aborda o desenvolvimento de uma ferramenta web para o projeto de anotação de dados do laboratório de computação inteligente aplicada(lacina). essa ferramenta tem como foco anotações de áudio baseadas em diálogos, com o objetivo de facilitar a criação de conjuntos de dados obtidos para o treinamento de sistemas de diálogo baseados em machine learning. a ferramenta foi projetada para fornecer uma plataforma eficiente e intuitiva, permitindo a identificação de falantes e a marcação de intenções e entidades nos diálogos em áudio. o projeto destaca a importância das anotações de áudio baseadas em diálogos para o avanço dos sistemas de diálogo, e discute as vantagens de se ter conjuntos de dados anotados, bem como as considerações éticas e de privacidade associadas a esse processo. a sessão de desenvolvimento abordou os aspectos técnicos da ferramenta, incluindo a escolha das tecnologias e frameworks utilizados, os desafios enfrentados durante o andamento e as soluções adotadas para superá-los. foram destacadas as funcionalidades desejáveis, como uma interface de usuário amigável, recursos avançados de reprodução e visualização sincronizada de áudio e texto anotados. em seguida, foram discutidas as aplicações práticas da ferramenta no contexto de projetos de machine learning e processamento de linguagem natural. os conjuntos de dados anotados criados com a ferramenta podem ser usados para treinar modelos de reconhecimento de fala e compreensão de linguagem, contribuindo para o desenvolvimento de sistemas de diálogo conversacional. este trabalho visa fornecer uma visão abrangente desta área em constante evolução, destacando seu impacto e seu potencial para impulsionar a pesquisa e o desenvolvimento de aplicações de machine learning baseadas em áudio. a ferramenta web de anotações de áudio baseadas em diálogos representa uma contribuição significativa para o campo do processamento de linguagem natural e sistemas de diálogo, facilitando a criação de conjuntos de dados anotados de alta qualidade e impulsionando o avanço e o desenvolvimento de sistemas de diálogo mais eficientes, naturais e precisos.', 'modelos de linguagem de grande escala (llms), como o chatgpt, claude e llama , revolucionaram o processamento de linguagem natural, criando novos casos de uso para aplicações que utilizam esses modelos em seus fluxos de trabalho. no entanto, os altos custos computacionais desses modelos acarretam problemas de custo e latência, impedindo a escalabilidade de funcionalidades baseadas em llm para muitos serviços e produtos, especialmente quando dependem de modelos com melhores capacidades de raciocínio, como o gpt- ou o claude opus. além disso, muitas consultas a esses modelos são duplicadas. o cache tradicional é uma solução natural para esse problema, mas sua incapacidade de determinar se duas consultas são semanticamente equivalentes leva a baixas taxas de cache hit. neste trabalho, propomos explorar o uso de cache semântico, que considera o significado das consultas em vez de sua formulação exata, para melhorar a eficiência de aplicações baseadas em llm. realizamos um experimento usando um conjunto de dados real da alura, uma empresa brasileira de educação, em um cenário onde um aluno responde a uma pergunta e o gpt- corrige a resposta. os resultados mostraram que , % das solicitações feitas ao llm poderiam ter sido atendidas a partir do cache usando um limiar de similaridade de . , com uma melhoria de - vezes na latência. esses resultados demonstram o potencial do cache semântico para melhorar a eficiência de funcionalidades baseadas em llm, reduzindo custos e latência enquanto mantém os benefícios de modelos avançados de linguagem como o gpt- . essa abordagem poderia possibilitar a escalabilidade de funcionalidades baseadas em llm para uma gama mais ampla de aplicações, avançando na adoção desses modelos poderosos em diversos domínios.', 'avanços recentes em modelos de linguagem de grande escala (llms) expandiram significativamente as capacidades da inteligência artificial (ia) em tarefas de processamento de linguagem natural. no entanto, seu desempenho em domínios especializados, como a ciência da computação, permanece relativamente pouco explorado. este estudo investiga se os llms podem igualar ou superar o desempenho humano no poscomp, um exame brasileiro prestigiado usado para admissões de pós-graduação em ciência da computação. quatro llms-chatgpt- , gemini . advanced, claude sonnet e le chat mistral large-foram avaliados nos exames poscomp de e . a avaliação consistiu em duas análises: uma envolvendo interpretação de imagens e outra somente de texto, para determinar a proficiência dos modelos em lidar com questões complexas típicas do exame. os resultados indicaram que os llms tiveram um desempenho significativamente melhor nas questões baseadas em texto, com a interpretação de imagens representando um grande desafio. por exemplo, na avaliação baseada em imagens, o chatgpt- respondeu corretamente de perguntas, enquanto o gemini . advanced conseguiu apenas respostas corretas. na avaliação baseada em texto de , o chatgpt- liderou com respostas corretas, seguido por gemini . advanced ( ), le chat mistral ( ) e claude sonnet ( ). o exame de mostrou tendências semelhantes.']"
2,42,2_sociais_privacidade_web_tweets,"['sociais', 'privacidade', 'web', 'tweets', 'aplicaes', 'modelo', 'parlamentares', 'eleies', 'aspectos', 'contedo']","['as redes sociais tornaram-se uma importante plataforma de posicionamento político dos parlamentares fora do congresso brasileiro. especialmente o twitter, que atualmente reúne perfis de % dos parlamentares. apesar dessa importância como difusor não oficial do discurso político, existem poucos estudos que analisam o posicionamento, a atuação e a influência dos parlamentares na plataforma (e.g. parlamentares e partidos mais ativos, engajamento dos parlamentares, proposições mais mencionadas, etc.). desta forma, este trabalho tem como principal objetivo analisar quantitativamente a influência e atuação dos parlamentares brasileiros no twitter, além de seu comportamento na rede. análises dessa natureza possibilitam que a população acompanhe e fiscalize o posicionamento e o comportamento de parlamentares fora do congresso sobre temas fundamentais à sociedade. além do mais, permite compreender como as redes sociais podem ser utilizadas como um elemento fundamental no processo de comunicação entre a sociedade civil e o setor político.', 'a internet tem possibilitado uma maior interação entre as pessoas ao redor do mundo, e o twitter tem desempenhado um papel significativo nisso. como uma das redes sociais mais utilizadas do planeta, o twitter permite que os usuários façam postagens expressando diversos aspectos do seu dia a dia. com a aproximação de grandes eventos de reconhecimento mundial(e.g., eventos esportivos), é natural que os usuários também expressem seus comentários sobre os diversos aspectos dessas competições. no entanto, devido à enorme quantidade de comentários gerados diariamente, é possível observar uma variedade de interpretações que refletem diferentes aspectos do evento, como discussões sobre escolha dos técnicos, desempenho das equipes e até mesmo comentários pessoais sobre os participantes. este artigo apresenta uma abordagem para analisar as discussões em torno de um evento, utilizando a copa do mundo como exemplo para validar o método. para isso, foram empregados dados coletados do twitter, os quais foram usados como entrada em técnicas de agrupamento, a fim de identificar potenciais conjuntos de comentários relacionados à competição. como resultado, foram obtidos grupos diversos, cada um com características únicas, abrangendo desde avaliações individuais, até críticas ao país-sede qatar, rivalidades entre seleções, entre outros aspectos. esses resultados indicam a existência de padrões nos comentários sobre um tema específico, sugerindo que os usuários buscam comentar temas do momento e com grande engajamento.', 'o presente artigo trata-se de uma análise de sentimentos e emoções expressos em tweets relacionados à guerra da ucrânia, mediante análise dos tópicos discutidos pelos usuários da plataforma twitter. este estudo visa compreender como os usuários reagem ao evento em curso, quais aspectos da guerra as pessoas estão discutindo na plataforma, e como se sentem a respeito deste acontecimento. além disso, visa identificar correlações entre as variáveis presentes nos tweets, como localização, informações de perfil do usuário autor da postagem, e a natureza de suas opiniões. tais análises foram conduzidas através de tarefas de processamento de linguagem natural como análises exploratórias dos dados e a aplicação de classificadores de sentimentos de tweets utilizando modelos de dados pré-treinados. os dados analisados contém tweets coletados desde o início do conflito, que se deu em fevereiro de até outubro de , e foram coletados a partir de hashtags relacionadas à guerra. para a realização das análises de sentimento e emoção foram utilizados a variante roberta. os tweets foram classificados em sentimentos como positivos, negativos ou neutros, e em emoções como alegria, raiva, medo, nojo, otimismo, pessimismo, surpresa e amor. os resultados mostraram que a maioria dos tweets em inglês expressam raiva e antecipação como emoções predominantes, e sentimentos negativos e neutros com maior predominância, atingindo mais de % do da amostragem analisada. algumas das frases mais recorrentes na análise fazem alusão ao apoio à ucrânia e pedindo o fim da guerra. da mesma forma, frases de preocupação com a crise, armas e fatalidades são recorrentes. na maioria das postagens, pessoas demonstram preocupação com o conflito armado e apoio à ucrânia. trabalhos futuros poderiam utilizar mais tweets para abranger a análise e visualizar a correlação de mais atributos relacionados às postagens como os engajamentos e curtidas.']"
3,39,3_alunos_campina_campina grande_universidade,"['alunos', 'campina', 'campina grande', 'universidade', 'ufcg', 'estudantes', 'computao', 'federal campina', 'universidade federal', 'ensino']","['os dias que antecedem a semana de matrículas da universidade federal de campina grande é o período em que os alunos dedicam parte do seu tempo organizando seus horários e planejando quais disciplinas pretendem cursar no próximo semestre. nesse período, as coordenações disponibilizam listas, através do sistema de “controle acadêmico” da universidade, com os dados de professores e horários para as disciplinas daquele semestre. esse trabalho tem o intuito de auxiliar o planejamento dos horários para o período de matrículas dos cursos de graduação na universidade federal de campina grande, com o desenvolvimento de um sistema web que permite o aluno organizar e planejar suas disciplinas através de uma interface agradável e que proporcione uma melhor experiência para a matrícula. para verificar a satisfação dos usuários quanto a usabilidade do sistema foi realizado um levantamento, utilizando o computer system usability questionnaire, que analisando as médias das respostas, foi obtido, na maioria dos itens, valores entre e , sendo o valor máximo, que apontam bons indicadores e alto nível de satisfação.', 'durante o período da pandemia promovida pelo covid- , a universidade federal de campina grande (ufcg) realizou quatro períodos letivos de forma remota para continuar as atividades de ensino. este trabalho analisa o impacto dessa modalidade de ensino e sua influência no desempenho e no nível de aprendizado dos alunos do curso de ciência da computação. a pesquisa utilizou dados anonimizados de registro de matrículas de a , concentrando-se nas disciplinas obrigatórias do curso. inicialmente, o estudo analisa a evolução das médias das notas dos alunos ao longo dos períodos acadêmicos, destacando um aumento nas médias durante os períodos remotos. além disso, a análise mostra uma diminuição no número de reprovações durante a pandemia em comparação com os períodos presenciais. o estudo também investiga, no regime remoto, as relações entre as disciplinas e o aproveitamento do aprendizado dos alunos. utilizando métodos estatísticos, foram identificadas as disciplinas mais afetadas pelo ensino remoto, tanto positivamente quanto negativamente. disciplinas como teoria da computação e estatística aplicada mostraram uma queda no desempenho, enquanto teoria dos grafos apresentou uma melhoria. em conclusão, o estudo aponta para um impacto negativo no desempenho dos alunos após o período de ensino remoto, sugerindo que fatores como eventuais irregularidades nas avaliações remotas podem ter contribuído para esse resultado.', 'as transformações tecnológicas vividas nos dias atuais impactam diretamente a produção de trabalho dos profissionais em qualquer área do conhecimento. diante disso, a unidade acadêmica de sistemas e computação (uasc) da universidade federal de campina grande (ufcg) oferta a disciplina “introdução à ciência da computação” para os cursos envolvidos com ciências exatas ligados a outras unidades da instituição. porém, tendo em vista que os alunos pertencem a vários cursos, torna-se desafiador aplicar uma metodologia apropriada a essas variedades e atender todas as demandas da formação profissional destes estudantes. sendo assim, neste trabalho investigamos a contribuição desse componente curricular para a formação dos acadêmicos. além disso, identificamos quais são as necessidades requeridas para os alunos que a cursam. para responder isso, desenvolvemos um estudo misto, quantitativo e qualitativo baseado em entrevistas que foram aplicadas a dois grupos: i. alunos egressos e ii. coordenadores dos cursos. os relatos revelam que o principal requisito para os alunos desta disciplina é o conhecimento básico de programação, utilizando uma linguagem de programação que seja mais didática e atualizada. além disso, identificamos nos relatos a necessidade do uso e manipulação de planilhas, requisito esse que foi mais evidenciado pelas áreas que envolve engenharia. ademais, os entrevistados acreditam que a disciplina é de extrema relevância para a formação dos acadêmicos. porém, dentre os relatos analisados, percebe-se que a atual forma como a disciplina é ofertada não está sendo suficiente para suprir as necessidades dos alunos que a cursam, dentre as causas mais importantes que levaram a esta afirmação estão: conteúdo defasado, dessincronização das turmas e dificuldade para aplicar os conhecimentos adquiridos.']"
4,29,4_monitoramento_implantao_segurana_acesso,"['monitoramento', 'implantao', 'segurana', 'acesso', 'blockchain', 'sistemas', 'cliente', 'arquitetura', 'kubernetes', 'ambientes']","['este artigo explora o potencial transformador e as aplicações no mundo real da tecnologia blockchain. inicialmente, este documento discute as principais funcionalidades do blockchain, incluindo seu sistema de consenso distribuído e imutável que suporta criptomoedas como o bitcoin por meio de mecanismos como prova de trabalho. ele entra em contratos inteligentes, enfatizando as capacidades do ethereum que ultrapassam simples transações monetárias para permitir aplicações complexas graças à sua linguagem turing-completa e máquina virtual. um foco significativo é colocado nas implicações dos tokens não fungíveis (nfts) para a propriedade de ativos digitais e físicos, destacando como eles suportam representações digitais únicas no blockchain. outra inovação discutida é o conceito de “soulbound tokens” (sbts), que são projetados para representar atributos intransferíveis como identidade e reputação dentro de uma estrutura descentralizada. por fim, o artigo mostra o papel crucial dos “oráculos”, que ligam a blockchain aos dados do mundo real, ilustrando o seu impacto nos sistemas financeiros tradicionais através de algoritmos e contratos inteligentes. portanto, por meio de uma análise abrangente, o artigo articula o vasto potencial da blockchain para inovar múltiplos setores, enfatizando a importância do desenvolvimento colaborativo para garantir a segurança e a conformidade regulatória no aproveitamento eficaz da tecnologia blockchain.', 'no contexto do projeto do smartcampus, da universidade federal de campina grande (ufcg), a implementação de apis que disponibilizam acesso a informações de eficiência energética apresenta desafios relacionados à segurança e ao controle de acesso. as partes interessadas, incluindo desenvolvedores, usuários finais e operadores de produção, necessitam interagir com essas apis em ambientes variados, criando a necessidade de diferenciar e controlar o acesso de maneira eficaz. este trabalho explora estratégias fundamentadas no modelo zero trust, que preconiza a autenticação contínua e autorização granular, garantindo que cada acesso seja verificado e autenticado. adotando o spire (spiffe runtime environment) em conjunto com serviços de proxy, busca-se assegurar a autenticação segura por meio de identidades criptográficas fornecidas pelo spire, bem como implementar um serviço de autorização personalizado conforme o perfil do usuário. o objetivo é prevenir acesso inadequado, vazamento de dados e manipulações indevidas, garantindo um ambiente seguro e confiável para a implantação do projeto.', 'no contexto do projeto do smartcampus, da universidade federal de campina grande (ufcg), a disponibilização de um kafka contendo informações de consumo de energia apresenta desafios complexos relacionados à segurança e controle de acesso. diversas entidades, incluindo desenvolvedores, usuários finais e operadores dos sistemas de produção, necessitam interagir com esse kafka a partir de ambientes variados, criando a necessidade de diferenciar e controlar o acesso de maneira eficaz. este trabalho explora estratégias fundamentadas no modelo zero trust, que preconiza a autenticação contínua e autorização granular, garantindo que cada acesso seja verificado e autenticado. adotando o spire em conjunto com uma série de outros microsserviços, busca-se assegurar a autenticação segura por meio de identidades spiffe, bem como configurar um serviço de autorização personalizado de acordo com o perfil do usuário. o objetivo é prevenir acesso inadequado, vazamento de dados e manipulações indevidas, visando obter um ambiente seguro e confiável para a implantação do projeto. por fim, também é desejado obter métricas para monitoramento das ferramentas utilizadas, a fim de identificar anomalias e falhas rapidamente.']"
5,23,5_produtos_meio_cidadania digital_cidadania,"['produtos', 'meio', 'cidadania digital', 'cidadania', 'mercado', 'usurio', 'aplicativos', 'anos', 'pblico', 'desafios']","['o dadosjusbr é um projeto sem fins lucrativos com o objetivo de apresentar de forma detalhada e consolidada as informações de remuneração dos órgãos que constituem o sistema de justiça brasileiro, formado pelos ministérios públicos, defensorias, procuradorias e o judiciário com os tribunais e conselhos, juntos somam órgãos. esse processo é chamado de ‘libertação dos dados’ e possui quatro estágios: coleta, validação, empacotamento e armazenamento. é no estágio da coleta que o crescimento do projeto está associado, pois é necessária a codificação dos coletores, um para cada órgão. o dadosjusbr é um projeto de código fonte livre aberto, sendo assim a comunidade pode participar, escrevendo coletores em múltiplas linguagens de programação, como go e python. com o uso de mais uma linguagem de programação, englobando também a tipagem dinâmica onde é mais difícil forçar um esquema considerando o tipo, surgem diversos problemas para restringir o esquema de dados. o principal deles é a consistência na serialização dos dados coletados, que é muito importante para armazenamento e transmissão entre estágios, pois o modo padrão que as linguagens serializam dados é diferente. neste trabalho propusemos e implementamos a utilização de protocol buffers (pb) para tornar mais fácil manter, transmitir e armazenar dados consolidados pelo dadosjusbr. atualmente temos órgãos coletados, dentre eles o mppb, codificado em golang, o site do conselho nacional de justiça (cnj), codificado em python, que foram os nossos coletores de dados que utilizamos neste trabalho. adaptar os crawlers e parsers, alterando todos os campos desses coletores para lidar com o novo formato de transmissão de dados, acarretou em dificuldades inesperadas, como lidar com timestamp entre as duas linguagens e transmitir o dado em pb no formato de texto, conseguindo assim a serialização dos dados em todos os estágios. assim, consolidando a serialização e transmissão dos dados entre coletores de diferentes linguagens, tornando o dadosjusbr mais democrático e abrangente, facilitando a contribuição.', 'o comércio eletrônico é um mercado que aumenta a cada ano, impulsionado pelos avanços tecnológicos que tornam mais cômodo e eficiente o processo de compra. por consequência, o número de vendas cresce, aumentando também a oferta de produtos sendo comercializados na internet. devido ao grande volume de ofertas, a dificuldade do consumidor de encontrar determinado produto cresce, bem como a sua capacidade de identificar e agrupar produtos iguais, com a finalidade de encontrar as melhores ofertas. isso ocorre, pois, dados dois produtos iguais, ou seja, que possuem o mesmo código de barras, são descritos de formas diferentes. para isso, existe uma técnica cujo objetivo é determinar se dois produtos são equivalentes, ou seja, correspondem à mesma entidade no mundo real, utilizando técnicas de aprendizagem de máquina, chamada product matching. no presente trabalho, foram analisados diversos modelos de aprendizagem de máquina, incluindo o bert, com a finalidade de escolher o melhor modelo que será utilizado para identificar produtos os quais sua descrição não corresponde ao seu código de barras. a base de dados utilizada será a base de produtos de notas fiscais emitidas no estado do acre, disponibilizadas pelo tribunal de contas do acre, tce-ac. ao final da implementação, o modelo foi capaz de classificar de maneira satisfatória os produtos inválidos.', 'nos últimos anos, as apostas esportivas têm experimentado um crescimento rápido devido à digitalização e à acessibilidade das plataformas online. contudo, esse aumento também gerou desafios, como a integridade do mercado, o combate a práticas de crime organizado, como lavagem de dinheiro, e a proteção dos consumidores. nesse cenário, os processos de verificação e validação de documentos, conhecidos como kyc (know your customer - conheça seu cliente), tornaram-se essenciais para garantir a conformidade regulatória e a segurança dos usuários. este estudo se concentra nos desafios enfrentados pelas casas de apostas ao implementar os processos de kyc em conformidade com as regulamentações, especialmente na necessidade de análise manual para validar os processos de verificação de kyc. essas verificações manuais são tipicamente demoradas e custosas. diante desse cenário, foi desenvolvida uma ferramenta de automação que utiliza um modelo llm (large language model) para reduzir a dependência de intervenções humanas e aprimora a eficiência no processo de verificação de documentos de kyc. a metodologia utilizada abrangeu uma amostra de dados de usuários do período de de dezembro de a de fevereiro de , levando em consideração as principais falhas identificadas durante esse intervalo de tempo. os resultados obtidos mostram uma melhoria significativa na performance das verificações de análises manuais. dessa forma, da amostra realizada com casos o algoritmo forneceu um veredito para . % casos, com precisão de , % na categorização correta do status da análise do usuário. isso destaca a eficácia geral do algoritmo, tornando o processo mais ágil e eficiente, reduzindo significativamente a carga de trabalho humana e, em contra partida, . % dos casos ainda exigiram análise manual para maior precisão, evidenciando situações em que o algoritmo não conseguiu tomar uma decisão automática. contudo, a pesquisa não apenas oferece uma solução prática e tecnológica para os desafios enfrentados pelas casas de apostas, mas também contribui para o avanço na automação de processos de verificação de documentos em ambientes regulamentados.']"
6,19,6_sade_pandemia_acrnimo_tratamento,"['sade', 'pandemia', 'acrnimo', 'tratamento', 'ia', 'profissionais', 'doenas', 'sistemas', 'pandemia covid', 'aplicativo']","['doenças oftalmológicas, como catarata, glaucoma e retinopatia diabética, representam um desafio significativo para a saúde pública, com potencial de causar perda de visão. no entanto, a maioria desses casos poderia ser evitada ou tratada se diagnosticada precocemente. neste contexto, a imagem de fundo de olho surge como uma ferramenta de diagnóstico eficaz, rápida e não invasiva. a interpretação manual de imagens oftalmológicas é repetitiva e sujeita a erros. assim, sistemas computacionais podem ser utilizados para auxiliar os profissionais na triagem automatizada, reduzindo tempo, erros e esforço na análise das doenças. os sistemas de aprendizado profundo provaram ser eficazes nesse contexto, entretanto, sua falta de transparência tem sido um desafio para a adoção clínica, o que destaca a importância da explicabilidade nos modelos de aprendizado de máquina. este estudo contribui para o avanço da compreensão e interpre-tação de modelos de aprendizado profundo na área da saúde ocular, visando melhorar o diagnóstico e tratamento de condições oftalmológicas. ele compara as técnicas lime e grad-cam aplicadas a diferentes arquiteturas de redes neurais convolucionais (cnns) treinadas para classificar condições oftalmológicas a partir de imagens de fundo de olho. os resultados indicam que o modelo vgg16 se destaca, alcançando uma acurácia de , % no treinamento e , % na validação. além disso, as técnicas de explicabilidade, embora distintas em abordagem, identificaram quase as mesmas regiões de interesse nas imagens oftalmológicas. ainda assim, apesar de haver limitações, como a aleatoriedade do lime e a necessidade de ajustes no grad-cam, o lime destacou áreas críticas de forma mais sutil, enquanto o grad-cam forneceu representações visuais mais diretas e intuitivas.', 'durante a pandemia de covid- , foi necessário que as pessoas se isolassem, devido aos altos índices de contágio e eventuais mortes. de acordo com o ministério da saúde, apenas no brasil já foram mais de milhões de casos confirmados e mil mortes, até o início do ano de . com isso, a população precisou abrir mão de suas formas usuais de entretenimento presencial e socialização, o que levou as formas de passatempo digital a aparecerem como forma alternativa de diversão e socialização. entre elas, uma das que mais viu crescimento neste período foi o uso dos jogos digitais, sendo apreciados até por quem nunca tinha tido contato anterior ao período de isolamento. por isso, neste trabalho, pretendemos analisar como o crescimento do uso dos jogos digitais durante a pandemia gerou impactos positivos e negativos na vida de alunos universitários brasileiros após a pandemia. para isso, foi encaminhado para tais estudantes formulários do google forms, permitindo assim a geração de gráficos comparativos a partir da coleta e análise de dados. assim, conseguimos comprovar um relacionamento entre este crescimento aos efeitos observados após o fim da mesma, tanto para quem já tinha contato com jogos como para quem teve sua primeira experiência. ao final da pesquisa, conseguimos obter respostas de estudantes universitários de graduação e pós, conseguindo obter uma boa perspectiva desta conexão e esperando com isso incentivar trabalhos futuros.', 'diante da atípica situação causada pela pandemia de covid- , ocorrida entre o final de e meados de , o ministério da educação autorizou o ensino remoto em cursos outrora presenciais. essa medida possuía o intuito de amenizar possíveis prejuízos causados ao sistema federal de ensino pela pandemia. nesse contexto, a unidade acadêmica de sistemas e computação da universidade federal de campina grande, juntamente com seu corpo docente, se viu diante do desafio de adaptar sua execução curricular ao ambiente virtual. passada a pandemia, surge a necessidade de analisar as consequências dela, em virtude da necessidade de adaptação das metodologias das aulas ao ensino remoto. este trabalho propõe o emprego de técnicas de análise e visualização de dados, a partir da coleta dos dados do sistema de controle acadêmico online (scao), tendo como objetivo contribuir para a análise de efeitos e implicações exercidos pela pandemia de covid- sobre os estudantes de graduação do curso ciência da computação da universidade federal de campina grande. como resultados encontrados, pode-se citar o fato de que o desempenho acadêmico dos alunos, na maior parte das disciplinas, voltou a alcançar os mesmos níveis do período da pré-pandemia, com uma tendência de aumento de trancamentos durante o pós-pandemia, sendo este o período que possui a maior quantidade de trancamentos registrados.']"
7,19,7_transporte_cidades_brasil_escolar,"['transporte', 'cidades', 'brasil', 'escolar', 'nibus', 'sangue', 'esto', 'acidentes', 'aplicativo', 'nmero']","['com o fim da pandemia do covid- , muitas organizações estão considerando a possibilidade de retornar ao trabalho presencial. nesse contexto, é importante avaliar as percepções das equipes de desenvolvimento de software no brasil sobre esse retorno. este estudo investigou como as equipes brasileiras de desenvolvimento de software lidaram com o retorno ao trabalho presencial/híbrido após a pandemia e como essa mudança de trabalho repercutiu no processo de desenvolvimento de software. aplicamos uma pesquisa com participantes de equipes de desenvolvimento de software e investigamos aspectos, como: rotina de trabalho, colaboração, comunicação, produtividade, bem-estar, auxílio oferecido pelas empresas e processo de desenvolvimento de software. realizamos uma análise quantitativa e qualitativa dos resultados da pesquisa e os comparamos com estudos anteriores. nossas principais conclusões sobre os participantes que retornaram ao trabalho presencial/híbrido são: (i) % dos participantes afirmaram que mantém uma rotina de trabalho estável relacionado ao horário padrão da empresa; (ii) , % dos participantes consideram sua equipe colaborativa; (iii) , % estão satisfeitos com a comunicação no regime de trabalho presencial/híbrido em comparação ao work from home (wfh) durante a pandemia de covid- ; (iv) % estão satisfeitos com o seu bem-estar; (v) % estão satisfeitos com sua produtividade; (vi) , % estão satisfeitos com as medidas de segurança adotadas pela empresa no pós pandemia e (vii) , % afirmaram ter mudado no processo de desenvolvimento de software devido ao retorno ao trabalho presencial/híbrido. as principais mudanças positivas no processo estão relacionadas às práticas de: divisão de tarefas, segurança e eficiência.', 'em queimadas-pb, centenas de estudantes dependem do ônibus escolar diariamente para alcançar suas escolas e universidades em campina grande. as informações de horários, rotas e motoristas são atualmente compartilhadas por meio de grupos no whatsapp e resultam em desinformação e estresse todos os dias devido ao grande número de mensagens, pois há centenas de alunos inseridos neles de diferentes escolas e faculdades, principalmente em dias quando há alguma mudança. o problema central é a ausência de uma plataforma que forneça informações unificadas e atualizadas sobre o transporte estudantil. essa falta de informações organizadas impacta negativamente o desempenho escolar dos alunos, causando frustrações, faltas e gastos adicionais com transporte. a proposta consiste em desenvolver uma plataforma web acessível e responsiva, que sirva como fonte confiável para os estudantes obterem esses dados de forma precisa, eliminando a necessidade de depender de grupos de mensagens. a metodologia abrange entrevistar alunos, motoristas e a secretaria de transportes para levantar requisitos, juntamente com o desenvolvimento de uma plataforma web, com foco em usabilidade e interface intuitiva. espera-se que a implementação reduza significativamente a desorganização, desinformação, o estresse e os atrasos enfrentados pelos estudantes, melhorando a frequência escolar e, consequentemente, o desempenho acadêmico.', 'o avanço das tecnologias da informação e comunicação exigem transformações nas organizações em todo o mundo. em paralelo a isso, as micro e pequenas empresas (mpes) cresceram de forma acelerada e correspondem a % dos negócios brasileiros. a crise da covid- escancarou muitos desafios ao microempreendedor; se antes da pandemia ter uma presença digital era fundamental para maximizar as vendas, no período de isolamento social se tornou essencial à sobrevivência das micro e pequenas empresas. entretanto, ainda há muitos desafios a serem ultrapassados para digitalizar a economia e os setores produtivos, por exemplo, a escassez de funcionários com conhecimento e habilidades no ambiente digital, a falta de capital financeiro, dentre outros. este estudo explora os fatores que determinam a falta de democratização e maturidade digital das mpes, bem como indicadores e dados relacionados à questão que auxiliam na compreensão desses desafios. de acordo com a problemática, foi realizada uma pesquisa bibliográfica sobre o assunto, utilizando palavras-chave como diretrizes de busca. além disso, foram obtidos e analisados dados descritivos, qualitativos e quantitativos, obtidos junto aos micro e pequenos empreendedores, através de entrevistas e um questionário. a partir disso, no final do estudo, foi possível fazer um levantamento dos principais desafios da inclusão e maturidade digital das mpes e apresentar algumas das ferramentas motivacionais necessárias para mitigar esses obstáculos, por exemplo, a necessidade de uma educação digital complementar para os funcionários das mpes.']"
8,16,8_nuvem_servio_infraestrutura_gerenciamento,"['nuvem', 'servio', 'infraestrutura', 'gerenciamento', 'recursos nuvem', 'custos', 'computao', 'aplicaes', 'serverless', 'provedores']","['o modelo de computação serverless fortaleceu a tendência da computação em nuvem de tornar transparente o gerenciamento da infraestrutura. ao simplificar o gerenciamento, o modelo serverless deixa a responsabilidade de implantação e escalonamento para a plataforma. aliado a isso, com um modelo de cobrança que considera somente o tempo despendido com a execução de requisições, há um forte incentivo para o uso eficiente dos recursos. essa busca por eficiência, traz à tona o problema de cold-start, que se configura como um atraso na execução de aplicações serverless. dentre as soluções propostas para lidar com o cold-start, se destacam as baseadas no método de snapshot. apesar da exploração desse método, existe uma carência de trabalhos que avaliam os trade-ofs de cada proposta. nessa direção, este trabalho compara duas estratégias de mitigação do cold-start: prebaking e seuss. avaliamos o desempenho das estratégias experimentalmente com funções de diferentes níveis de complexidade: noop, uma função que converte markdown para html, e uma que carrega mb de dependências. resultados preliminares indicam que prebaking apresentou desempenho % e % superior para inicializar noop e markdown, respectivamente e processou a primeira requisição de markdown com um tempo % inferior ao seuss.', 'no processo de desenvolvimento de software, a aquisição e manutenção de hardware adequado para as necessidades de programação podem resultar em altos custos de investimento de capital. a alternativa de uso de recursos em nuvem oferece flexibilidade, porém o gerenciamento desses recursos pode ser complexo e oneroso, requerendo conhecimentos especializados em operações em nuvem. o problema consiste em gerenciar um ambiente de desenvolvimento na nuvem de forma eficiente, evitando altos custos de aquisição e manutenção de hardware próprio, além de simplificar o gerenciamento de recursos ao alugar máquinas na nuvem, buscando minimizar despesas e eliminar a necessidade de expertise complexa em operações em nuvem. propomos o desenvolvimento de uma ferramenta de linha de comando, destinada a simplificar o gerenciamento do ambiente de desenvolvimento de software. essa ferramenta terá a capacidade de criar, configurar e gerenciar recursos na nuvem de forma automatizada e eficiente. uma característica diferencial é a utilização de instâncias preemptivas oferecidas por provedores de nuvem, permitindo aproveitar recursos ociosos a custos ainda mais baixos, sem comprometer a qualidade do ambiente de desenvolvimento. espera-se que o usuário seja capaz de criar ambientes de desenvolvimento utilizando a ferramenta proposta integrando-a com outras soluções já existentes para desenvolvimento de código. ao oferecer uma solução intuitiva, nossa abordagem visa otimizar o ambiente de desenvolvimento, maximizando a economia e eliminando a necessidade de conhecimentos avançados em operações em nuvem por parte da equipe de desenvolvimento. ao final deste trabalho, a usabilidade da ferramenta foi validada e demonstrou ser eficaz na simplificação do gerenciamento dos ambientes. a maioria dos participantes conseguiu gerenciar ambientes com sucesso, destacando a facilidade de uso e a utilidade da documentação fornecida.', 'o laboratório de sistemas distribuídos (lsd) é responsável por gerenciar a nuvem privada do uasc/ufcg. para manter essa nuvem privada, o lsd enfrentou desaios no gerenciamento dos recursos da nuvem, especialmente na visualização de dados e na geração de novos relatórios para os responsáveis pelos projetos que usam esses recursos. anteriormente, a ferramenta utilizada era o shylock, mas ela apresentava diiculdades na geração de relatórios sobre a quantidade e o consumo de recursos da nuvem, pois gerava apenas um relatório por dia. além disso, a criação de relatórios era um processo complexo que exigia a criação de um novo modelo jinja e a codiicação de variáveis. dado os déicits encontrados no sistema atual, o redash foi a plataforma selecionada para supri-los. o redash resolve a diiculdade de visualização dos dados analisados através da criação de dashboards para a visualização e monitoramento dos recursos da nuvem em tempo real. ao incluir as informações dos dashboards criados no redash no luxo de trabalho de gerenciamento e monitoramento da nuvem, a facilidade de tomar decisões com base em dados reais e atualizados permitiu uma melhor tomada de decisões sobre o gerenciamento dos recursos da nuvem.']"
9,15,9_social_alunos_curso_estresse,"['social', 'alunos', 'curso', 'estresse', 'ingressantes', 'atividades', 'desempenho', 'computao', 'evaso', 'instrutores']","['a evasão é um dos maiores problemas do ensino superior público brasileiro, representando um grande desperdício de recursos, como também uma grande perda social. há muitos estudos sobre causas e soluções para este problema, porém sabe-se pouco sobre as trajetórias seguidas por esses alunos após a evasão, para entender melhor a extensão dessas perdas. em particular, para profissões ligadas à área de tecnologia da informação, que não possuem regulamentação no brasil, não é necessário uma formação superior para ingressar no mercado, possibilitando que pessoas que não finalizaram seus cursos sejam absorvidas. nesse contexto, este trabalho pretendeu mapear as trajetórias seguidas pelos evadidos do curso de ciência da computação da universidade federal de campina grande, a fim de verificar em que medida os recursos investidos foram realmente desperdiçados. foi apresentada uma análise preliminar, quantitativa, utilizando a base de dados da instituição, identificando grupos e suas características. posteriormente, dados profissionais e acadêmicos foram coletados da rede social linkedin, agregando informações aos grupos encontrados. a partir da análise qualitativa dessas informações, a pesquisa mostrou que parte dos alunos utilizam os conhecimentos adquiridos no curso, seja assumindo cargos na área de tecnologia da informação, seja em outros cursos de graduação similares. esses resultados, de certa forma, desmistificam a ideia de que os recursos alocados para alunos que evadem de um curso de graduação em ciência da computação são integralmente perdidos.', 'a evasão nas instituições de ensino superior no brasil é um problema que gera impactos de cunhos financeiro e social, para toda a população. no curso de computação da universidade federal de campina grande (ufcg) a evasão é motivo de preocupação de professores e gestores, sobretudo em relação aos alunos ingressantes do curso. o programa de educação tutorial (pet) em computação da ufcg desenvolveu uma abordagem complementar e colaborativa de formação de alunos veteranos e ingressantes, denominada gesto, com o intuito de diminuir os índices de evasão dos estudantes ingressantes nesta fase desafiadora de início da experiência universitária. a abordagem é baseada na prática do voluntariado e na construção social de sentidos do estudo de computação, através de atividades de desenvolvimento de habilidades técnicas (hardskills) e não-técnicas (softskills) escolhidas pelos alunos em um portfólio que lhes é oferecido semestralmente. este trabalho apresenta uma análise dos impactos desta abordagem a partir do estudo de três atividades escolhidas pelos ingressantes dentre aquelas disponíveis no portfólio oferecido no semestre letivo de . . resultados das avaliações feitas pelos alunos participantes, ingressantes e veteranos, indicam que tal abordagem pode, juntamente com outras ações, diminuir a evasão no curso de ciência da computação na ufcg.', 'o estresse é uma resposta normal do corpo a situações desafiadoras ou exigentes, mas quando os níveis de estresse são muito altos ou duram por um longo período, podem levar a problemas de saúde mental, como ansiedade e depressão. estudos mostram que estudantes universitários experimentam altos níveis de estresse relacionados a pressão acadêmica e mudanças na vida social e financeira, entre outros fatores. neste trabalho, conduzimos uma pesquisa com os concluintes do curso de ciência da computação na universidade federal de campina grande (ufcg) durante o semestre . . a pesquisa buscou entender a relação entre fatores de estresse (saúde, social e acadêmico), o nível de estresse percebido em diferentes momentos do semestre (início, meio e fim) e seu impacto no desempenho acadêmico. os resultados mostram que os estudantes apresentam altos níveis de estresse ao longo de todo o semestre, e o principal fator de estresse que afeta o desempenho acadêmico é a pressão da família para conseguir um bom emprego. além disso, uma correlação negativa significativa foi identificada entre o estresse percebido no final do semestre e o desempenho acadêmico. contudo, o estresse no início e meio do período não mostrou relação estatisticamente significativa com o desempenho dos alunos.']"
10,15,10_cdigo_reviso_comentrios_prs,"['cdigo', 'reviso', 'comentrios', 'prs', 'reviso cdigo', 'processo', 'refatoramentos', 'github', 'comentrios reviso', 'projetos']","['o github, atualmente a maior plataforma para hospedagem de código e controle de versionamento, possui um enorme fluxo diário de interações entre usuários e repositórios. com o número de repositórios hospedados na casa dos milhões, alguns projetos que poderiam ser do interesse de alguns usuários acabam passando despercebidos, assim como projetos que necessitam de desenvolvedores, acabam ficando no ostracismo. para esses casos, surge a necessidade de algum mecanismo que possa facilitar a escolha de projetos, pelo usuário. na literatura outros trabalhos, já realizaram estudos sobre esse contexto, recomendando projetos com diferentes abordagens. entretanto, ainda há espaço para novos estudos, utilizando novos aspectos, na tentativa de verificar e validar outros resultados. por isso, esse trabalho busca encontrar projetos relevantes para o usuário, baseando-se nos interesses do mesmo, na plataforma github, utilizando um conjunto de features com o auxílio de algoritmos de learning to rank. analisamos a efetividade learning to rank, no contexto de recomendação de projetos, utilizando os algoritmos ranknet, adarank e listnet, usando como espaço amostral repositórios e usuários do github. os resultados mostram, a relevância da variável resposta e que a abordagem de learning to rank para recomendação de projetos oferece muito espaço para exploração.', 'a compilação é um processo essencial no desenvolvimento de linhas de produto de software, como o linux. entretanto, identificar erros de compilação em linhas de produto de software (lps) não é trivial, já que os compiladores tradicionais não são conscientes de variação. abordagens anteriores foram propostas que identificam alguns desses erros de compilação usando técnicas avançadas que requerem um esforço dos programadores em usarem. este estudo avalia a eficácia de modelos de linguagem de grande porte (llms), especificamente o chatgpt e le chat mistral, na identificação de erros de compilação em lps. inicialmente foram testados produtos nas linguagens c++, java e c, e posteriormente lps em c, abrangendo tipos diferentes de erros de compilação. os dois llms foram avaliados com base na sua capacidade de reconhecer e diagnosticar corretamente os erros. o chatgpt conseguiu identificar % e % dos erros de compilação em produtos e lps, enquanto que o le chat mistral obteve % e %, respectivamente. a análise revelou que, embora os llms possam identificar uma gama de erros de compilação, desafios específicos permanecem, especialmente em ambientes de lps com alta variabilidade. o estudo sugere a necessidade de refinamentos contínuos nos modelos de llm para melhorar sua precisão e utilidade em cenários de desenvolvimento de software complexos.', 'com o passar dos anos, a revisão de código vem mudando; antes, era feita uma inspeção manual (rigorosa e síncrona), já nos dias atuais, é feita uma revisão mais moderna (assíncrona e menos rigorosa). atualmente, o git, através da plataforma github, é o sistema de controle de versões mais popular, favorecendo diversas discussões relacionadas a mudanças no código-fonte. com o auxílio de ferramentas como refactoringminer, que fornece a detecção de refatoramentos aplicados aos códigos-fonte e, utilizando-se de uma amostra de repositórios provenientes do projeto do apache no github, este trabalho, através de inspeções manuais de comentários de revisão, visa entender e caracterizar os comentários que induziram refatoramentos nos prs, com o intuito de entender as características próprias e diferenças dos comentários de revisão em prs com e sem refatoramentos. através das hipóteses levantadas, tentamos complementar o entendimento da parte qualitativa dos comentários de revisão, abordados anteriormente de forma similar em outra pesquisa, que analisava dados qualitativos e quantitativos de prs que induziram refatoramentos e de prs que não induziram refatoramentos, com a intenção de entender melhor as diferenças entre os dois tipos de prs, no nível de pull request.']"
