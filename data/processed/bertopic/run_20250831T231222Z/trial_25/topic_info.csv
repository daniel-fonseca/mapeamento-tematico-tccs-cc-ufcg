Topic,Count,Name,Representation,Representative_Docs
-1,265,-1_software_tcnicas_aplicaes_cdigo,"['software', 'tcnicas', 'aplicaes', 'cdigo', 'modelo', 'alunos', 'modelos', 'abordagem', 'programao', 'dessas']","['a evasão nas instituições de ensino superior no brasil é um problema que gera impactos de cunhos financeiro e social, para toda a população. no curso de computação da universidade federal de campina grande (ufcg) a evasão é motivo de preocupação de professores e gestores, sobretudo em relação aos alunos ingressantes do curso. o programa de educação tutorial (pet) em computação da ufcg desenvolveu uma abordagem complementar e colaborativa de formação de alunos veteranos e ingressantes, denominada gesto, com o intuito de diminuir os índices de evasão dos estudantes ingressantes nesta fase desafiadora de início da experiência universitária. a abordagem é baseada na prática do voluntariado e na construção social de sentidos do estudo de computação, através de atividades de desenvolvimento de habilidades técnicas (hardskills) e não-técnicas (softskills) escolhidas pelos alunos em um portfólio que lhes é oferecido semestralmente. este trabalho apresenta uma análise dos impactos desta abordagem a partir do estudo de três atividades escolhidas pelos ingressantes dentre aquelas disponíveis no portfólio oferecido no semestre letivo de . . resultados das avaliações feitas pelos alunos participantes, ingressantes e veteranos, indicam que tal abordagem pode, juntamente com outras ações, diminuir a evasão no curso de ciência da computação na ufcg.', 'a internet tem possibilitado uma maior interação entre as pessoas ao redor do mundo, e o twitter tem desempenhado um papel significativo nisso. como uma das redes sociais mais utilizadas do planeta, o twitter permite que os usuários façam postagens expressando diversos aspectos do seu dia a dia. com a aproximação de grandes eventos de reconhecimento mundial(e.g., eventos esportivos), é natural que os usuários também expressem seus comentários sobre os diversos aspectos dessas competições. no entanto, devido à enorme quantidade de comentários gerados diariamente, é possível observar uma variedade de interpretações que refletem diferentes aspectos do evento, como discussões sobre escolha dos técnicos, desempenho das equipes e até mesmo comentários pessoais sobre os participantes. este artigo apresenta uma abordagem para analisar as discussões em torno de um evento, utilizando a copa do mundo como exemplo para validar o método. para isso, foram empregados dados coletados do twitter, os quais foram usados como entrada em técnicas de agrupamento, a fim de identificar potenciais conjuntos de comentários relacionados à competição. como resultado, foram obtidos grupos diversos, cada um com características únicas, abrangendo desde avaliações individuais, até críticas ao país-sede qatar, rivalidades entre seleções, entre outros aspectos. esses resultados indicam a existência de padrões nos comentários sobre um tema específico, sugerindo que os usuários buscam comentar temas do momento e com grande engajamento.', 'nos últimos anos, o uso de plataformas de ensino ganhou mais espaço nas instituições de educação, seja de modo complementar ao ensino presencial, ou para cursos totalmente remotos. dessa forma, é de grande importância o acesso às informações e funcionalidades nesses serviços à usuários com algum tipo de deficiência. no entanto, grande parte dos sistemas ainda não são projetados priorizando requisitos e padrões de usabilidade e acessibilidade para pessoas com deficiência, onde se pode destacar o grupo dos deficientes visuais, que segundo a organização mundial da saúde, no mundo, são milhões de pessoas. na universidade federal de campina grande (ufcg), uma das plataformas usadas para o ensino remoto é o moodle. neste contexto, este artigo apresenta um estudo para avaliar o nível de acessibilidade visual da plataforma moodle usando ferramentas automáticas e testes de usabilidade com participantes quem tinham algum grau de deficiência visual, com o intuito de encontrar possíveis problemas de interação. os resultados mostram que a plataforma não pode ser considerada totalmente acessível a todo e qualquer aluno com deficiência visual para usar plenamente todos os serviços disponibilizados nele, devido às violações das diretrizes utilizadas nas avaliações automáticas e de usabilidade.']"
0,38,0_pandemia_sade_animais_pessoas,"['pandemia', 'sade', 'animais', 'pessoas', 'brasil', 'privacidade', 'aplicativo', 'digitais', 'desafios', 'covid']","['em um mundo onde as pessoas têm cada vez mais contato com as tecnologias, surgiram também questões sobre como utilizá-las adequadamente. muitas vezes, essas questões surgem em respostas a perigos percebidos ou comportamentos inapropriados como roubo de identidade, cyberbullying ou disseminação de notícias falsas, especialmente no mundo online. tais abusos podem ser ainda mais prejudiciais aos adolescentes que cada vez mais cedo são inseridos nesse contexto tecnológico. apesar de os usuários terem capacidade de avaliar uma conduta online adequada e identificar riscos, é necessário que eles desenvolvam um senso crítico para utilizar a internet de forma responsável, ou seja, usar o ambiente online sem prejudicar a si mesmos e aos outros. para isso, a cidadania digital surge como um conjunto de normas de comportamento que devem ser seguidas para o uso amigável, seguro e ético das tecnologias. logo, a presente pesquisa tem como objetivo analisar as principais tecnologias digitais utilizadas por adolescentes e, a partir delas, listar formas de uso responsável de acordo com os elementos da cidadania digital a fim de conduzir de forma positiva o comportamento de adolescentes na sociedade e constituir uma cultura de uso seguro das tecnologias digitais.', 'nos últimos anos, as apostas esportivas têm experimentado um crescimento rápido devido à digitalização e à acessibilidade das plataformas online. contudo, esse aumento também gerou desafios, como a integridade do mercado, o combate a práticas de crime organizado, como lavagem de dinheiro, e a proteção dos consumidores. nesse cenário, os processos de verificação e validação de documentos, conhecidos como kyc (know your customer - conheça seu cliente), tornaram-se essenciais para garantir a conformidade regulatória e a segurança dos usuários. este estudo se concentra nos desafios enfrentados pelas casas de apostas ao implementar os processos de kyc em conformidade com as regulamentações, especialmente na necessidade de análise manual para validar os processos de verificação de kyc. essas verificações manuais são tipicamente demoradas e custosas. diante desse cenário, foi desenvolvida uma ferramenta de automação que utiliza um modelo llm (large language model) para reduzir a dependência de intervenções humanas e aprimora a eficiência no processo de verificação de documentos de kyc. a metodologia utilizada abrangeu uma amostra de dados de usuários do período de de dezembro de a de fevereiro de , levando em consideração as principais falhas identificadas durante esse intervalo de tempo. os resultados obtidos mostram uma melhoria significativa na performance das verificações de análises manuais. dessa forma, da amostra realizada com casos o algoritmo forneceu um veredito para . % casos, com precisão de , % na categorização correta do status da análise do usuário. isso destaca a eficácia geral do algoritmo, tornando o processo mais ágil e eficiente, reduzindo significativamente a carga de trabalho humana e, em contra partida, . % dos casos ainda exigiram análise manual para maior precisão, evidenciando situações em que o algoritmo não conseguiu tomar uma decisão automática. contudo, a pesquisa não apenas oferece uma solução prática e tecnológica para os desafios enfrentados pelas casas de apostas, mas também contribui para o avanço na automação de processos de verificação de documentos em ambientes regulamentados.', 'durante a pandemia de covid- , foi necessário que as pessoas se isolassem, devido aos altos índices de contágio e eventuais mortes. de acordo com o ministério da saúde, apenas no brasil já foram mais de milhões de casos confirmados e mil mortes, até o início do ano de . com isso, a população precisou abrir mão de suas formas usuais de entretenimento presencial e socialização, o que levou as formas de passatempo digital a aparecerem como forma alternativa de diversão e socialização. entre elas, uma das que mais viu crescimento neste período foi o uso dos jogos digitais, sendo apreciados até por quem nunca tinha tido contato anterior ao período de isolamento. por isso, neste trabalho, pretendemos analisar como o crescimento do uso dos jogos digitais durante a pandemia gerou impactos positivos e negativos na vida de alunos universitários brasileiros após a pandemia. para isso, foi encaminhado para tais estudantes formulários do google forms, permitindo assim a geração de gráficos comparativos a partir da coleta e análise de dados. assim, conseguimos comprovar um relacionamento entre este crescimento aos efeitos observados após o fim da mesma, tanto para quem já tinha contato com jogos como para quem teve sua primeira experiência. ao final da pesquisa, conseguimos obter respostas de estudantes universitários de graduação e pós, conseguindo obter uma boa perspectiva desta conexão e esperando com isso incentivar trabalhos futuros.']"
1,35,1_software_projetos_cdigo_bugs,"['software', 'projetos', 'cdigo', 'bugs', 'sutes', 'bug', 'desenvolvedores', 'relatrios', 'code', 'geis']","['dentre as atividades típicas de um processo de software, podemos destacar as tarefas de testar, analisar, reportar e corrigir bugs. a realização dessas tarefas é importante para identificar erros comuns ou complexos durante todas as etapas do desenvolvimento, evitando retrabalho e entregando um software com mais qualidade e confiabilidade [ ]. em um relatório de bug, geralmente, seu autor oferece detalhes da anormalidade que vem ocorrendo. tipicamente, um relatório de bug é aberto, o bug é corrigido e o relatório é fechado. contudo, por vezes, é verificado que a correção do bug não foi eficaz, seja por falta de descrição mais objetiva no relatório, seja por dificuldade de entendimento por parte do desenvolvedor. assim, faz-se necessária a reabertura do bug, adicionando tempo no processo de desenvolvimento, tornando o software mais custoso. por isso, é importante investigar o que pode ser feito para mitigar tais problemas. neste trabalho, investigamos as características que levam um bug a ser reaberto. os resultados deste trabalho podem ajudar aos usuários finais e desenvolvedores a melhor escrever relatórios de bugs, bem como aos desenvolvedores a melhor entendê-los e tratá-los. o estudo utilizou um dataset extraído da ferramenta bugzilla.', 'o github é a plataforma de hospedagem de código e controle de versão mais utilizada atualmente. diariamente, inúmeros projetos são criados, estendidos e modificados por diferentes usuários. entretanto, muitos projetos que possivelmente seriam do interesse de determinados usuários, acabam por passar despercebidos diante da grande quantidade de projetos disponíveis. neste contexto, surge a necessidade de algum mecanismo que possa auxiliar o usuário a encontrar projetos que podem ser de seu interesse. já existe na literatura trabalhos que buscam analisar fatores de interesse com o objetivo de recomendar projetos, entretanto ainda há margem para utilização de outros fatores e critérios na tentativa de obter resultados melhores. para tanto, o presente trabalho busca utilizar features, algumas já propostas na literatura e outras ainda não utilizadas nesse contexto, disponíveis em projetos do github, com o auxílio de algoritmos de learning to rank, para encontrar relações de interesse em projetos e assim recomendá-los para o usuário. verificamos a efetividade de learning to rank para recomendação de projetos usando os algoritmos lambdamart, random forest e coordinate ascent, utilizando como base repositórios e usuários do github. os resultados mostram que a abordagem de learning to rank para recomendação de projetos é promissora e efetiva, ao mesmo tempo que oferece muito espaço para aprimoramento.', 'o github, atualmente a maior plataforma para hospedagem de código e controle de versionamento, possui um enorme fluxo diário de interações entre usuários e repositórios. com o número de repositórios hospedados na casa dos milhões, alguns projetos que poderiam ser do interesse de alguns usuários acabam passando despercebidos, assim como projetos que necessitam de desenvolvedores, acabam ficando no ostracismo. para esses casos, surge a necessidade de algum mecanismo que possa facilitar a escolha de projetos, pelo usuário. na literatura outros trabalhos, já realizaram estudos sobre esse contexto, recomendando projetos com diferentes abordagens. entretanto, ainda há espaço para novos estudos, utilizando novos aspectos, na tentativa de verificar e validar outros resultados. por isso, esse trabalho busca encontrar projetos relevantes para o usuário, baseando-se nos interesses do mesmo, na plataforma github, utilizando um conjunto de features com o auxílio de algoritmos de learning to rank. analisamos a efetividade learning to rank, no contexto de recomendação de projetos, utilizando os algoritmos ranknet, adarank e listnet, usando como espaço amostral repositórios e usuários do github. os resultados mostram, a relevância da variável resposta e que a abordagem de learning to rank para recomendação de projetos oferece muito espaço para exploração.']"
2,31,2_modelos_linguagem_imagens_modelo,"['modelos', 'linguagem', 'imagens', 'modelo', 'natural', 'linguagem natural', 'processamento', 'llms', 'processamento linguagem natural', 'processamento linguagem']","['nos últimos anos, avanços na área de inteligência artiicial permitiram desenvolvimento de modelos de processamento de linguagem natural em prol de aplicações em diversos contextos, como a automatização do processo de elaboração de questões sobre temas especíicos. atualmente, existem modelos capazes de formular questões sobre um tópico qualquer após receber como entrada um texto relevante. tais projetos possuem grande potencial auxiliar no contexto educacional, entretanto, ainda existe carência de um sistema que forneça a seus usuários uma maneira fácil e intuitiva de utilizar esses modelos. esse trabalho tem como objetivo o desenvolvimento de uma aplicação web que supra essa necessidade, incorporando modelos de processamento de linguagem natural que recebem como entrada um texto e geram para o usuário uma lista de perguntas relevantes ao tema. a utilização da aplicação web também traz a oportunidade de obter feedback dos usuários sobre a qualidade das perguntas geradas, informação que pode ser utilizada para retroalimentação e aprimoramento dos modelos utilizados.', 'avanços recentes em modelos de linguagem de grande escala (llms) expandiram significativamente as capacidades da inteligência artificial (ia) em tarefas de processamento de linguagem natural. no entanto, seu desempenho em domínios especializados, como a ciência da computação, permanece relativamente pouco explorado. este estudo investiga se os llms podem igualar ou superar o desempenho humano no poscomp, um exame brasileiro prestigiado usado para admissões de pós-graduação em ciência da computação. quatro llms-chatgpt- , gemini . advanced, claude sonnet e le chat mistral large-foram avaliados nos exames poscomp de e . a avaliação consistiu em duas análises: uma envolvendo interpretação de imagens e outra somente de texto, para determinar a proficiência dos modelos em lidar com questões complexas típicas do exame. os resultados indicaram que os llms tiveram um desempenho significativamente melhor nas questões baseadas em texto, com a interpretação de imagens representando um grande desafio. por exemplo, na avaliação baseada em imagens, o chatgpt- respondeu corretamente de perguntas, enquanto o gemini . advanced conseguiu apenas respostas corretas. na avaliação baseada em texto de , o chatgpt- liderou com respostas corretas, seguido por gemini . advanced ( ), le chat mistral ( ) e claude sonnet ( ). o exame de mostrou tendências semelhantes.', 'modelos de linguagem de grande escala (llms), como o chatgpt, claude e llama , revolucionaram o processamento de linguagem natural, criando novos casos de uso para aplicações que utilizam esses modelos em seus fluxos de trabalho. no entanto, os altos custos computacionais desses modelos acarretam problemas de custo e latência, impedindo a escalabilidade de funcionalidades baseadas em llm para muitos serviços e produtos, especialmente quando dependem de modelos com melhores capacidades de raciocínio, como o gpt- ou o claude opus. além disso, muitas consultas a esses modelos são duplicadas. o cache tradicional é uma solução natural para esse problema, mas sua incapacidade de determinar se duas consultas são semanticamente equivalentes leva a baixas taxas de cache hit. neste trabalho, propomos explorar o uso de cache semântico, que considera o significado das consultas em vez de sua formulação exata, para melhorar a eficiência de aplicações baseadas em llm. realizamos um experimento usando um conjunto de dados real da alura, uma empresa brasileira de educação, em um cenário onde um aluno responde a uma pergunta e o gpt- corrige a resposta. os resultados mostraram que , % das solicitações feitas ao llm poderiam ter sido atendidas a partir do cache usando um limiar de similaridade de . , com uma melhoria de - vezes na latência. esses resultados demonstram o potencial do cache semântico para melhorar a eficiência de funcionalidades baseadas em llm, reduzindo custos e latência enquanto mantém os benefícios de modelos avançados de linguagem como o gpt- . essa abordagem poderia possibilitar a escalabilidade de funcionalidades baseadas em llm para uma gama mais ampla de aplicações, avançando na adoção desses modelos poderosos em diversos domínios.']"
3,28,3_nuvem_arquitetura_infraestrutura_implantao,"['nuvem', 'arquitetura', 'infraestrutura', 'implantao', 'gerenciamento', 'aplicaes', 'monitoramento', 'servio', 'kubernetes', 'software']","['o laboratório de sistemas distribuídos (lsd) é responsável por gerenciar a nuvem privada do uasc/ufcg. para manter essa nuvem privada, o lsd enfrentou desaios no gerenciamento dos recursos da nuvem, especialmente na visualização de dados e na geração de novos relatórios para os responsáveis pelos projetos que usam esses recursos. anteriormente, a ferramenta utilizada era o shylock, mas ela apresentava diiculdades na geração de relatórios sobre a quantidade e o consumo de recursos da nuvem, pois gerava apenas um relatório por dia. além disso, a criação de relatórios era um processo complexo que exigia a criação de um novo modelo jinja e a codiicação de variáveis. dado os déicits encontrados no sistema atual, o redash foi a plataforma selecionada para supri-los. o redash resolve a diiculdade de visualização dos dados analisados através da criação de dashboards para a visualização e monitoramento dos recursos da nuvem em tempo real. ao incluir as informações dos dashboards criados no redash no luxo de trabalho de gerenciamento e monitoramento da nuvem, a facilidade de tomar decisões com base em dados reais e atualizados permitiu uma melhor tomada de decisões sobre o gerenciamento dos recursos da nuvem.', 'a liteme é uma empresa de inteligência energética em ascensão que desagrega dados de consumo de energia de seus clientes via modelo nialm, distinguindo o consumo de cada um dos aparelhos registrados, e os processa para oferecer seus serviços. isso faz da desagregação um dos alicerces do negócio, e conforme a liteme se expande, dados de mais clientes precisam ser desagregados, o que leva à necessidade de replicar o desagregador nialm. atualmente o desagregador faz parte de uma arquitetura monolítica fortemente acoplada com o backend robusto da empresa, chamado de núcleo. ele realiza as operações mais custosas da plataforma, que elevam o requisito de hardware para executá-la, e atua como servidor sempre disponível. esse acoplamento prejudica a escalabilidade do nialm, que não pode ser replicado sozinho. uma arquitetura distribuída de microsserviço que permita separar o nialm do núcleo, mantendo comunicação entre os dois, pode fornecer ao desagregador melhor escalabilidade, desacoplamento, menores requisitos de hardware e de disponibilidade. este trabalho propõe uma arquitetura de desagregação distribuída para substituir a monolítica, de forma a executar em nuvem pública com uso de instâncias oportunistas (e.g. ""spots"" na aws). a arquitetura foi validada com apoio da empresa e testada em ambiente simulado. após análise, foi possível alcançar os objetivos e reduzir custos de hospedagem em nuvem em até , % em comparação à arquitetura monolítica.', 'no processo de desenvolvimento de software, a aquisição e manutenção de hardware adequado para as necessidades de programação podem resultar em altos custos de investimento de capital. a alternativa de uso de recursos em nuvem oferece flexibilidade, porém o gerenciamento desses recursos pode ser complexo e oneroso, requerendo conhecimentos especializados em operações em nuvem. o problema consiste em gerenciar um ambiente de desenvolvimento na nuvem de forma eficiente, evitando altos custos de aquisição e manutenção de hardware próprio, além de simplificar o gerenciamento de recursos ao alugar máquinas na nuvem, buscando minimizar despesas e eliminar a necessidade de expertise complexa em operações em nuvem. propomos o desenvolvimento de uma ferramenta de linha de comando, destinada a simplificar o gerenciamento do ambiente de desenvolvimento de software. essa ferramenta terá a capacidade de criar, configurar e gerenciar recursos na nuvem de forma automatizada e eficiente. uma característica diferencial é a utilização de instâncias preemptivas oferecidas por provedores de nuvem, permitindo aproveitar recursos ociosos a custos ainda mais baixos, sem comprometer a qualidade do ambiente de desenvolvimento. espera-se que o usuário seja capaz de criar ambientes de desenvolvimento utilizando a ferramenta proposta integrando-a com outras soluções já existentes para desenvolvimento de código. ao oferecer uma solução intuitiva, nossa abordagem visa otimizar o ambiente de desenvolvimento, maximizando a economia e eliminando a necessidade de conhecimentos avançados em operações em nuvem por parte da equipe de desenvolvimento. ao final deste trabalho, a usabilidade da ferramenta foi validada e demonstrou ser eficaz na simplificação do gerenciamento dos ambientes. a maioria dos participantes conseguiu gerenciar ambientes com sucesso, destacando a facilidade de uso e a utilidade da documentação fornecida.']"
4,26,4_alunos_universidade_disciplinas_estudantes,"['alunos', 'universidade', 'disciplinas', 'estudantes', 'universidade federal campina', 'federal campina', 'universidade federal', 'computao', 'ufcg', 'federal']","['durante o período da pandemia promovida pelo covid- , a universidade federal de campina grande (ufcg) realizou quatro períodos letivos de forma remota para continuar as atividades de ensino. este trabalho analisa o impacto dessa modalidade de ensino e sua influência no desempenho e no nível de aprendizado dos alunos do curso de ciência da computação. a pesquisa utilizou dados anonimizados de registro de matrículas de a , concentrando-se nas disciplinas obrigatórias do curso. inicialmente, o estudo analisa a evolução das médias das notas dos alunos ao longo dos períodos acadêmicos, destacando um aumento nas médias durante os períodos remotos. além disso, a análise mostra uma diminuição no número de reprovações durante a pandemia em comparação com os períodos presenciais. o estudo também investiga, no regime remoto, as relações entre as disciplinas e o aproveitamento do aprendizado dos alunos. utilizando métodos estatísticos, foram identificadas as disciplinas mais afetadas pelo ensino remoto, tanto positivamente quanto negativamente. disciplinas como teoria da computação e estatística aplicada mostraram uma queda no desempenho, enquanto teoria dos grafos apresentou uma melhoria. em conclusão, o estudo aponta para um impacto negativo no desempenho dos alunos após o período de ensino remoto, sugerindo que fatores como eventuais irregularidades nas avaliações remotas podem ter contribuído para esse resultado.', 'os dias que antecedem a semana de matrículas da universidade federal de campina grande é o período em que os alunos dedicam parte do seu tempo organizando seus horários e planejando quais disciplinas pretendem cursar no próximo semestre. nesse período, as coordenações disponibilizam listas, através do sistema de “controle acadêmico” da universidade, com os dados de professores e horários para as disciplinas daquele semestre. esse trabalho tem o intuito de auxiliar o planejamento dos horários para o período de matrículas dos cursos de graduação na universidade federal de campina grande, com o desenvolvimento de um sistema web que permite o aluno organizar e planejar suas disciplinas através de uma interface agradável e que proporcione uma melhor experiência para a matrícula. para verificar a satisfação dos usuários quanto a usabilidade do sistema foi realizado um levantamento, utilizando o computer system usability questionnaire, que analisando as médias das respostas, foi obtido, na maioria dos itens, valores entre e , sendo o valor máximo, que apontam bons indicadores e alto nível de satisfação.', 'as transformações tecnológicas vividas nos dias atuais impactam diretamente a produção de trabalho dos profissionais em qualquer área do conhecimento. diante disso, a unidade acadêmica de sistemas e computação (uasc) da universidade federal de campina grande (ufcg) oferta a disciplina “introdução à ciência da computação” para os cursos envolvidos com ciências exatas ligados a outras unidades da instituição. porém, tendo em vista que os alunos pertencem a vários cursos, torna-se desafiador aplicar uma metodologia apropriada a essas variedades e atender todas as demandas da formação profissional destes estudantes. sendo assim, neste trabalho investigamos a contribuição desse componente curricular para a formação dos acadêmicos. além disso, identificamos quais são as necessidades requeridas para os alunos que a cursam. para responder isso, desenvolvemos um estudo misto, quantitativo e qualitativo baseado em entrevistas que foram aplicadas a dois grupos: i. alunos egressos e ii. coordenadores dos cursos. os relatos revelam que o principal requisito para os alunos desta disciplina é o conhecimento básico de programação, utilizando uma linguagem de programação que seja mais didática e atualizada. além disso, identificamos nos relatos a necessidade do uso e manipulação de planilhas, requisito esse que foi mais evidenciado pelas áreas que envolve engenharia. ademais, os entrevistados acreditam que a disciplina é de extrema relevância para a formação dos acadêmicos. porém, dentre os relatos analisados, percebe-se que a atual forma como a disciplina é ofertada não está sendo suficiente para suprir as necessidades dos alunos que a cursam, dentre as causas mais importantes que levaram a esta afirmação estão: conteúdo defasado, dessincronização das turmas e dificuldade para aplicar os conhecimentos adquiridos.']"
