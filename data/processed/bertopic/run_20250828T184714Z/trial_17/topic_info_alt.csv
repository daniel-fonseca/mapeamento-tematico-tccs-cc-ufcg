Topic,Count,Name,Representation,Representative_Docs
-1,78,-1_dados_trabalho_sistema_so,"['dados', 'trabalho', 'sistema', 'so', 'desenvolvimento', 'objetivo', 'projetos', 'aplicao', 'grande', 'uso']","['o github é a plataforma de hospedagem de código e controle de versão mais utilizada atualmente. diariamente, inúmeros projetos são criados, estendidos e modificados por diferentes usuários. entretanto, muitos projetos que possivelmente seriam do interesse de determinados usuários, acabam por passar despercebidos diante da grande quantidade de projetos disponíveis. neste contexto, surge a necessidade de algum mecanismo que possa auxiliar o usuário a encontrar projetos que podem ser de seu interesse. já existe na literatura trabalhos que buscam analisar fatores de interesse com o objetivo de recomendar projetos, entretanto ainda há margem para utilização de outros fatores e critérios na tentativa de obter resultados melhores. para tanto, o presente trabalho busca utilizar features, algumas já propostas na literatura e outras ainda não utilizadas nesse contexto, disponíveis em projetos do github, com o auxílio de algoritmos de learning to rank, para encontrar relações de interesse em projetos e assim recomendá-los para o usuário. verificamos a efetividade de learning to rank para recomendação de projetos usando os algoritmos lambdamart, random forest e coordinate ascent, utilizando como base repositórios e usuários do github. os resultados mostram que a abordagem de learning to rank para recomendação de projetos é promissora e efetiva, ao mesmo tempo que oferece muito espaço para aprimoramento.', 'criar projetos de programação que necessitam de mais de um programador é uma prática comum na comunidade de desenvolvimento de software. isso ocorre porque muitos projetos exigem uma equipe de programadores com diferentes habilidades e conhecimentos para serem concluídos de maneira eficiente e eficaz. é comum ver programadores buscando outros profissionais para colaborar em seus projetos e, assim, alcançar um resultado de alta qualidade e escalabilidade. infelizmente, muitas vezes esse projeto não é levado adiante devido à falta de pessoas capacitadas e dispostas a trabalhar neles. o problema é a dificuldade em encontrar programadores qualificados e interessados em trabalhar em um projeto específico, o que pode levar a uma perda de tempo e recursos para aqueles que estão procurando por colaboradores. nesse contexto, o presente projeto propõe uma solução: uma aplicação web que organiza e conecta programadores para realizar projetos em conjunto. o usuário que se cadastrar no site irá disponibilizar informações sobre suas habilidades e conhecimentos na área de programação, permitindo que seja conectado a projetos disponíveis de acordo com seu perfil. além disso, o usuário também poderá criar seu próprio projeto, informando as tecnologias que serão utilizadas. dessa forma, espera-se que, ao final deste projeto, os programadores que desejam criar algum projeto ou empreender consigam encontrar facilmente outros programadores capacitados e dispostos a trabalhar com eles, superando a dificuldade encontrada atualmente na procura por colaboradores.', 'este trabalho de conclusão de curso aborda o desenvolvimento de uma ferramenta web para o projeto de anotação de dados do laboratório de computação inteligente aplicada(lacina). essa ferramenta tem como foco anotações de áudio baseadas em diálogos, com o objetivo de facilitar a criação de conjuntos de dados obtidos para o treinamento de sistemas de diálogo baseados em machine learning. a ferramenta foi projetada para fornecer uma plataforma eficiente e intuitiva, permitindo a identificação de falantes e a marcação de intenções e entidades nos diálogos em áudio. o projeto destaca a importância das anotações de áudio baseadas em diálogos para o avanço dos sistemas de diálogo, e discute as vantagens de se ter conjuntos de dados anotados, bem como as considerações éticas e de privacidade associadas a esse processo. a sessão de desenvolvimento abordou os aspectos técnicos da ferramenta, incluindo a escolha das tecnologias e frameworks utilizados, os desafios enfrentados durante o andamento e as soluções adotadas para superá-los. foram destacadas as funcionalidades desejáveis, como uma interface de usuário amigável, recursos avançados de reprodução e visualização sincronizada de áudio e texto anotados. em seguida, foram discutidas as aplicações práticas da ferramenta no contexto de projetos de machine learning e processamento de linguagem natural. os conjuntos de dados anotados criados com a ferramenta podem ser usados para treinar modelos de reconhecimento de fala e compreensão de linguagem, contribuindo para o desenvolvimento de sistemas de diálogo conversacional. este trabalho visa fornecer uma visão abrangente desta área em constante evolução, destacando seu impacto e seu potencial para impulsionar a pesquisa e o desenvolvimento de aplicações de machine learning baseadas em áudio. a ferramenta web de anotações de áudio baseadas em diálogos representa uma contribuição significativa para o campo do processamento de linguagem natural e sistemas de diálogo, facilitando a criação de conjuntos de dados anotados de alta qualidade e impulsionando o avanço e o desenvolvimento de sistemas de diálogo mais eficientes, naturais e precisos.']"
0,17,0_testes_sutes_teste_threads,"['testes', 'sutes', 'teste', 'threads', 'faltas', 'aplicaes', 'refatoramento', 'web', 'cdigo', 'sutes teste']","['objetos sintéticos são utilizados no desenvolvimento de software para simular o comportamento de objetos reais de forma controlada. durante o desenvolvimento de servidores back-end, existe uma constante necessidade de modelar testes com objetos simulados para garantir o funcionamento adequado de funcionalidades dos mesmos. todavia, essa tarefa acaba por se tornar repetitiva e complicada conforme a aplicação cresce. ademais, testes que envolvam endpoints de aplicações se mostram mais complexos por ter necessidade de modelar requisições completas do tipo http. a biblioteca mocktests tem por finalidades principais promover uma elaboração de testes sem a necessidade de inserção repetitiva de alguns componentes a cada caso de teste, simplificando assim diversos aspectos e solucionando empecilhos que podem surgir durante o desenvolvimento. este presente trabalho tem por finalidade relatar as etapas do desenvolvimento da biblioteca mocktests, concebida para solucionar percalços durante a criação de testes envolvendo endpoints de aplicações. os resultados obtidos demonstraram como a ferramenta se adequou ao ser usada em testes de uma aplicação backend real, e como a mesma solucionou diversos problemas encontrados na logística de construção dos respectivos testes.', 'desenvolver sistemas com alta qualidade envolve atividades que permitam fácil manutenção e forneçam confiança sobre o código produzido. testes de software se relacionam com a confiabilidade, já refatoramentos, com manutenibilidade. por definição, edições de refatoramento objetivam melhorar estrutura do código, mas preservando seu comportamento. porém, refatoramentos mal feitos podem alterar o comportamento do sistema, são as chamadas faltas de refatoramento. tais faltas, podem não ser detectadas por suítes de teste pouco confiáveis. uma alternativa para criação sistemática de suítes de teste é a utilização de ferramentas de geração automática. este trabalho tem como objetivo avaliar a efetividade de suítes de teste geradas manual e automaticamente para detectar faltas de refatoramento do tipo extract method. para isso, foram selecionados projetos escritos em java, com suítes de teste geradas manualmente, novas suítes de testes foram criadas automaticamente com as ferramentas randoop e evosuite, um conjunto de faltas foram injetadas nos sistemas. as suítes manuais detectaram , % das faltas injetadas, enquanto a suíte randoop detectou apenas , % e a evosuite , %. a randoop obteve uma taxa de detecção baixa, a evosuite, no entanto, obteve um resultado significantemente comparável ao de suítes manuais.', 'refatoramentos são a prática em que desenvolvedores alteram seu código de forma que não altere o comportamento do sistema. tal prática costuma vir acompanhada do uso de suítes de regressão para detectar mudanças de comportamento indesejadas em um sistema. porém, tais suítes de teste podem não garantir a detecção de faltas, criando um falso senso de segurança durante refatoramentos, visto que a suíte pode não perceber certas alterações. neste trabalho, propomos uma abordagem que tem por objetivo avaliar tais suítes de teste na sua capacidade de detectar faltas de refatoramento, assim como as comparar com alternativas de geração de suítes de teste automatizadas. para isso, foi realizado um estudo relacionado às faltas do refatoramento extract method, e o desenvolvimento de uma ferramenta que facilita a avaliação de uma suíte de testes, por meio de um plugin da ide eclipse feito com a linguagem de programação java. a partir disso, foram criados mutantes de refatoramento (faltas) e utilizados em um estudo quantitativo, no qual avaliamos as suítes de regressão de diferentes projetos open source, assim como suítes de testes geradas pela ferramenta de geração evosuite. nossos estudos mostraram que existe uma possível relação entre cobertura de testes e detecção de mutantes de refatoramento em um sistema, assim como uma negligência de casos menos comuns durante o desenvolvimento dessas suítes, visto que cerca de , % das faltas de refatoramento injetadas não foram detectadas nas suítes manuais, e , % nas suítes automatizadas, demonstrando que existe espaço para melhoria das suites de teste focadas neste contexto.']"
1,14,1____,"['', '', '', '', '', '', '', '', '', '']","['', '', '']"
2,12,2_transporte_brasil_trabalho_sangue,"['transporte', 'brasil', 'trabalho', 'sangue', 'dados', 'anlise', 'esto', 'mpes', 'esto satisfeitos', 'trabalho presencialhbrido']","['o departamento de informática do sistema único de saúde (datasus) disponibiliza dados fundamentais para análises da situação sanitária do brasil, embasando tomadas de decisão e o desenvolvimento de programas de intervenção em saúde. no entanto, esses dados são fornecidos em um formato não diretamente compatível com ferramentas populares, como excel ou google sheets, dificultando sua acessibilidade para pesquisadores, analistas e profissionais da área médica. além disso, enfrenta-se o desafio atrelado ao significativo volume dos dados, caracterizando-os a nível de big data. a escala massiva desses dados demanda abordagens eficientes de engenharia de dados para lidar com processamento e armazenamento, a fim de proporcionar uma estrutura robusta e escalável para a manipulação dos mesmos. diante dessa problemática, este trabalho propõe o desenvolvimento de um processo automatizado para extração de tabelas, disponíveis em arquivos no formato dbc e provenientes de bases de dados do datasus, o armazenamento em um data warehouse e a disponibilização dos dados por meio de uma api. espera-se que essa iniciativa facilite a análise de dados de saúde no brasil, oferecendo acesso simplificado e dados prontos para análise a pesquisadores, analistas e profissionais da área médica.', 'o avanço das tecnologias da informação e comunicação exigem transformações nas organizações em todo o mundo. em paralelo a isso, as micro e pequenas empresas (mpes) cresceram de forma acelerada e correspondem a % dos negócios brasileiros. a crise da covid- escancarou muitos desafios ao microempreendedor; se antes da pandemia ter uma presença digital era fundamental para maximizar as vendas, no período de isolamento social se tornou essencial à sobrevivência das micro e pequenas empresas. entretanto, ainda há muitos desafios a serem ultrapassados para digitalizar a economia e os setores produtivos, por exemplo, a escassez de funcionários com conhecimento e habilidades no ambiente digital, a falta de capital financeiro, dentre outros. este estudo explora os fatores que determinam a falta de democratização e maturidade digital das mpes, bem como indicadores e dados relacionados à questão que auxiliam na compreensão desses desafios. de acordo com a problemática, foi realizada uma pesquisa bibliográfica sobre o assunto, utilizando palavras-chave como diretrizes de busca. além disso, foram obtidos e analisados dados descritivos, qualitativos e quantitativos, obtidos junto aos micro e pequenos empreendedores, através de entrevistas e um questionário. a partir disso, no final do estudo, foi possível fazer um levantamento dos principais desafios da inclusão e maturidade digital das mpes e apresentar algumas das ferramentas motivacionais necessárias para mitigar esses obstáculos, por exemplo, a necessidade de uma educação digital complementar para os funcionários das mpes.', 'com o fim da pandemia do covid- , muitas organizações estão considerando a possibilidade de retornar ao trabalho presencial. nesse contexto, é importante avaliar as percepções das equipes de desenvolvimento de software no brasil sobre esse retorno. este estudo investigou como as equipes brasileiras de desenvolvimento de software lidaram com o retorno ao trabalho presencial/híbrido após a pandemia e como essa mudança de trabalho repercutiu no processo de desenvolvimento de software. aplicamos uma pesquisa com participantes de equipes de desenvolvimento de software e investigamos aspectos, como: rotina de trabalho, colaboração, comunicação, produtividade, bem-estar, auxílio oferecido pelas empresas e processo de desenvolvimento de software. realizamos uma análise quantitativa e qualitativa dos resultados da pesquisa e os comparamos com estudos anteriores. nossas principais conclusões sobre os participantes que retornaram ao trabalho presencial/híbrido são: (i) % dos participantes afirmaram que mantém uma rotina de trabalho estável relacionado ao horário padrão da empresa; (ii) , % dos participantes consideram sua equipe colaborativa; (iii) , % estão satisfeitos com a comunicação no regime de trabalho presencial/híbrido em comparação ao work from home (wfh) durante a pandemia de covid- ; (iv) % estão satisfeitos com o seu bem-estar; (v) % estão satisfeitos com sua produtividade; (vi) , % estão satisfeitos com as medidas de segurança adotadas pela empresa no pós pandemia e (vii) , % afirmaram ter mudado no processo de desenvolvimento de software devido ao retorno ao trabalho presencial/híbrido. as principais mudanças positivas no processo estão relacionadas às práticas de: divisão de tarefas, segurança e eficiência.']"
3,12,3_programao_objetos_conceitos_disciplina,"['programao', 'objetos', 'conceitos', 'disciplina', 'programao orientada', 'aluno', 'competncias', 'orientada', 'paradigma', 'feedback']","['o pensamento computacional fundamenta-se nas competências adquiridas por meio da ciência da computação. com base nisso, surgiu o compensar, uma ferramenta web voltada para o desenvolvimento do pensamento computacional em conjunto com a matemática, direcionada à educação básica. sua estrutura baseia-se na classificação das questões matemáticas conforme as competências do pensamento computacional que podem ser estimuladas durante o processo de resolução. no entanto, diversos fatores contribuíram para sua transformação em um sistema legado e inoperante. a ausência de contribuições ao longo de mais de anos resultou na obsolescência das tecnologias empregadas tanto no frontend quanto no backend da ferramenta. além disso, a inatividade do serviço de hospedagem do compensar, devido à ausência de requisições por parte dos usuários, culminou na perda do banco de dados devido à falta de consultas. a identificação desses problemas ressaltou a necessidade de uma reengenharia de software em conjunto com a engenharia reversa, visando reconstruir o sistema com tecnologias mais recentes. esse processo foi dividido em etapas que culminaram no desenvolvimento de uma nova versão do compensar, agora disponível para acesso público. em uma dessas etapas, foram realizadas avaliações para compreender o nível de satisfação do cliente com o software entregue, bem como análises de desempenho e indexação em motores de busca. os resultados obtidos foram positivos. na análise de desempenho, o compensar recebeu boas avaliações em categorias como acessibilidade, práticas recomendadas e seo (search engine optimization). esses resultados contribuíram para a visibilidade da ferramenta nos motores de busca, os quais utilizam dados dessas categorias, como o carregamento de conteúdo e a acessibilidade, na atribuição de pontuação do compensar nos motores de busca.', 'ensinar o paradigma de programação orientada a objetos costuma ser um desafio para os professores. a principal dificuldade é muitas vezes atribuída à mentalidade que o paradigma exige. essa mentalidade envolve raciocinar sobre elementos da realidade em termos de classes, objetos, atributos, polimorfismo etc. em suma, é uma mentalidade que requer boas habilidades de abstração. várias metodologias, abordagens e ferramentas já foram propostas para ajudar os alunos a alcançar a mentalidade necessária para aplicar esse paradigma, mas o aprendizado continua difícil. diante disso, uma ferramenta que até então nunca havia sido considerada para o ensino de programação é o trivium medieval. o trivium consiste nas três artes liberais de gramática, lógica e retórica. o syllabus e a estrutura das aulas do trivium podem ser um modelo interessante para ser aplicado em cursos de programação orientada a objetos, pois abordam de forma bastante didática conceitos fundamentais idênticos ao do paradigma orientado a objetos. a demonstração da correlação entre os dois assuntos é um dos objetivos deste trabalho. além disso, conjecturamos que ensinar os conceitos fundamentais da gramática antes ou paralelamente ao ensino do paradigma orientado a objetos parece ser mais eficiente do que começar logo pela prática de programação, como costuma ser feito em cursos de programação. este artigo propõe duas abordagens para o ensino do paradigma orientado a objetos. eles consistem na estruturação do curso de programação orientada a objetos com base na filosofia e metodologia educacional clássica, a fim de facilitar a compreensão do paradigma.', 'cursos introdutórios de programação costumam ser desafiadores, especialmente para aqueles sem experiência prévia em lógica de programação. ao ingressar em um curso de tecnologia, é necessário o aprendizado de terminologias relacionadas à ciência da computação e adaptar-se aos diferentes conceitos e paradigmas. se tratando de programação orientada a objetos (poo), utilizando java como linguagem de programação, a dificuldade é ainda maior. seu amplo conjunto de conceitos e sua sintaxe verbosa fazem parte de uma transição desafiadora, seguida de erros quanto aos conceitos aprendidos, refletindo muitas vezes a permanência do aluno no curso. sendo assim, com o objetivo de compreender e analisar estes erros, este trabalho se propõe a seguir os mesmos passos de um estudo anterior conduzido em uma turma da disciplina de laboratório de programação ii no curso de ciência da computação da universidade federal de campina grande. a dificuldade de assimilação de conceitos de java implica na observação de erros em atividades práticas e avaliativas, como foi constatado no estudo mencionado, que analisou principalmente o volume de código das submissões dos alunos. a intenção é fazer um comparativo entre os resultados, investigando padrões e tendências que possam variar conforme há mudança de amostra. uma vez identificando tais erros, bem como a sua frequência e impacto na nota do aluno, este estudo pode contribuir para potenciais adaptações nos métodos de ensino para que haja uma melhor aproximação na solução destes problemas, fortalecendo a compreensão dos conceitos fundamentais de programação orientada a objetos, uma vez que é recorrente em outras disciplinas da grade curricular.']"
4,12,4_busca_regio_regies_soluo,"['busca', 'regio', 'regies', 'soluo', 'gis', 'nodegis', 'mapas', 'geogrficas', 'ferramenta', 'dados']","['a representação de regiões geográficas tem sido alvo de pesquisas nos últimos tempos, pois é a peça chave para a realização de diversas tarefas, como a busca por regiões similares. tal representação, porém, não é tarefa trivial, uma vez que pode envolver inúmeras variáveis no processo. a tendência atual é que essas representações sejam feitas através de vetores de alta dimensão, conhecidos como embeddings. porém, operações de busca por estes costumam ser custosas para a máquina em termos de tempo de processamento e consumo de disco. neste artigo experimentou-se diferentes manipulações nesses vetores a fim de diminuir o consumo de recursos computacionais no momento da busca sem comprometer significativamente a relevância dos resultados produzidos por ela. técnicas de redução de dimensionalidade dos vetores e quantização de seus elementos foram executadas, além de comparações entre a busca exata por vizinhos mais próximos e a busca aproximada por estes. observou-se que a busca aproximada por vizinhos mais próximos reduz o tempo de busca em aproximadamente , %, mantendo uma boa aproximação com os resultados do baseline. a técnica de quantização dos embeddings apresentou a segunda maior interseção com o baseline e reduziu consideravelmente o consumo de disco pelos índices. técnicas como a redução de dimensionalidades não apresentaram grandes alterações no tempo de busca e tiveram interseções baixíssimas com o baseline da pesquisa.', 'diante da vasta presença da informação espacial nas aplicações atuais, surgiram várias ferramentas que fomentam o desenvolvimento de aplicações web de sistemas de informações geográficas (gis). apesar de haver um grande número de soluções, muitas são complexas, requerendo habilidades específicas dos desenvolvedores. portanto, há a necessidade de uma ferramenta que simplifique o processo de desenvolver e publicar uma aplicação web gis, com a vantagem de ser de código aberto. neste trabalho é apresentado o nodegis, uma ferramenta de código aberto que provê uma interface gráfica para o desenvolvimento de aplicações de web gis, sem escrita de código ou configuração complexa de servidores. o nodegis utiliza-se de uma arquitetura baseada em contêineres, fazendo uso de rest, e facilitando o deployment de uma aplicação web gis. a ferramenta apresentada permite ao usuário plotar mapas vetoriais, realizar operações de overlay e de personalização de camadas, zooming, panning, tooltip, consultas em atributos convencionais e espaciais. o nodegis também pode ser utilizado no ensino de gis, não necessitando instalação de software por parte dos alunos.', 'aplicações de recuperação de informação estão cada dia mais robustas, versáteis e atendendo aos mais diversos ins. na área de recuperação de informação geográica (gir), embora muitas abordagens tenham sido propostas para lidar com diversos problemas, alguns ainda permanecem insuicientemente explorados, a exemplo de abordagens que permitam buscar regiões que sejam similares a uma região de interesse de forma rápida e escalável. dada uma região espacial e uma região de consulta, uma pesquisa por região similar visa encontrar as k regiões mais semelhantes à região de consulta na região espacial. neste trabalho é apresentada uma abordagem de busca por similaridade em mapas utilizando um algoritmo de similaridade textual com dados armazenados e indexados. para isso, desenvolveu-se um algoritmo baseado no uso de índices no elasticsearch[ ] para armazenamento dos dados geográicos. as consultas são textuais, utilizando o conceito de índice invertido junto com o algoritmo de similaridade bm25[ ]. nesse algoritmo, as informações de points of interest (poi) presente nas regiões dos mapas são convertidas em dados textuais que são a base para realização de consultas. a abordagem proposta é baseada em um estudo de caso utilizando dados de grandes cidades obtidos através openstreetmaps1.']"
5,12,5_dados_vulnerabilidades_desenvolvimento_qualidade,"['dados', 'vulnerabilidades', 'desenvolvimento', 'qualidade', 'ferramentas', 'software', 'banco dados', 'banco', 'processo', 'requisitos']","['um dos maiores problemas encontrados em aplicações que estão envolvidas no ecossistema de big data está relacionado à disponibilidade e qualidade de dados para modelos de ia e outras análises direcionadas. aplicações com esse foco necessitam de dados que disponham de alta qualidade, já que o resultado de seus serviços depende da integridade da informação usada no processo. quando pensamos em dados textuais, devemos saber que a informação fornecida para aplicações que envolvem processamento de texto, devem ser as melhores possíveis. desta forma, foi desenvolvido uma aplicação que trata da gerência da coleta e tratamento contínuo de dados textuais. o contexto da aplicação está fixo na coleta de dados textuais da rede social reddit. através da api fornecida pela rede, é feita a ingestão de dados de uma comunidade específica. com base nos dados coletados, a ferramenta trata de fazer todo o orquestramento de tarefas que gerenciam a coleta, tratamento e disponibilização desses dados. para teste da ferramenta, os dados disponíveis são passados para um modelo de pln, que usa lda para mapear tópicos com base nos textos extraídos do site. a aplicação se baseia nos conceitos de streaming de dados e processamento de texto, de forma contínua e automática, a fim de manter uma base de dados sólida e de qualidade para análises de texto.', 'o desenvolvimento de aplicações android é uma tarefa complexa. a variedade de dispositivos, e suas diferentes configurações, são um exemplo das razões que dificultam esse desenvolvimento. por causa disso, um processo de desenvolvimento bem planejado é importante para garantir a qualidade do software. a falta desse processo pode aumentar a incidência de bugs e as chances de requisitos do sistema não serem satisfeitos. esse trabalho tem como objetivo implantar um processo de qualidade de software no contexto do desenvolvimento de uma aplicação android. essa aplicação tem como propósito ser utilizada por agentes de saúde que a alimentarão com dados que serão processados e modelados a fim de emitir alertas antecipados sobre a incidência de populações de mosquitos transmissores de doenças. o processo de qualidade será definido a partir de uma pesquisa na literatura sobre boas práticas em desenvolvimento android, um estudo sobre o domínio do problema e sobre regras de negócio da aplicação. a partir dessa pesquisa, será discutida uma estratégia para a criação de um processo que engloba o desenvolvimento de testes, versionamento de código, escolha dos requisitos não funcionais e procedimentos para a implantação da aplicação. ao final do trabalho, será aplicado um questionário aos supervisores para que eles possam avaliar a eficácia do processo implantado. espera-se que, depois de adotado o processo de qualidade, exista uma maior clareza e fluidez nas entregas da aplicação, e que seja minimizada a quantidade de problemas.', 'vulnerabilidades de segurança em sistemas computacionais são problemas geralmente complexos de tratar e que mesmo com a melhora no processo de desenvolvimento tendem a persistir. a falta de interesse na remoção dessas vulnerabilidades durante o desenvolvimento pode se tornar um contratempo no futuro (débito técnico), resultar em acessos indevidos, expor dados dos usuários e reverter em custos financeiros para a empresa. desta forma, faz-se necessário que preocupações com a identificação e remoção dessas vulnerabilidades existam durante todo o processo de desenvolvimento do software. neste trabalho, serão utilizadas ferramentas de análise estática (check marx, black duck e jfrog xray) para analisar a evolução, distribuição por classe de risco e tempo de vida de vulnerabilidades de segurança em um projeto desenvolvido por uma empresa de grande porte em parceria com o laboratório de sistemas distribuídos. o projeto em questão consiste de um serviço que oferece gerenciamento para recursos de observabilidade. a partir dos resultados obtidos foi constatado que vulnerabilidades que afetam componentes open-source encontradas pelas ferramentas black duck e jfrog xray estavam sendo tratadas. no entanto, as vulnerabilidades de segurança que afetam pontos de código do projeto encontrados na ferramenta check marx, não estavam sendo abordadas.']"
6,11,6_dados_privacidade_notcias_informaes,"['dados', 'privacidade', 'notcias', 'informaes', 'fake', 'news', 'fake news', 'falsas', 'sensveis', 'notcias falsas']","['atualmente, a grande gama de plataformas, aplicativos e operações online disponíveis para a resolução de diferentes problemas resulta em um tráfego de grande volume de dados de usuários, inclusive dados sensíveis e de identificação. para proteger a privacidade dos usuários, um direito assegurado por leis em todo o mundo (leis de proteção de dados), é necessária uma atenção maior a esses dados para não serem publicados. no entanto, identificar as informações sensíveis entre tantos outros tipos de dados, pode não ser uma tarefa trivial. estudos já existentes propõem a aplicação de técnicas de processamento de linguagem natural (pln) para identificação automática de informações pessoais identificáveis (personal identifiable information, pii) em documentos em português. o objetivo deste trabalho é propor, através de uma prova de conceito, uma abordagem complementar às utilizadas nos estudos relacionados, através da tarefa de extração de relação de pln. para tal, foi criado um componente que combina um modelo de linguagem especializado na língua portuguesa e camadas adicionais de extração de relação. para o treinamento e avaliação do componente, foi gerada uma base de dados sensíveis sintéticos com o auxílio de um large language model (llm). os resultados foram satisfatórios, com métricas de precisão, recall e f1-score acima de %, indicando que a abordagem pode ser uma boa proposta para detecção automática de informações sensíveis pessoais.', 'a privacidade do usuário é uma das maiores preocupações dos desenvolvedores de aplicações hoje em dia. com o advento de novas regulamentações e ciber ataques se tornando mais comuns e caros, a demanda por novas tecnologias que podem ajudar a reduzir ou mitigar o risco de exposição da informação está aumentando. como os humanos são os mais suscetíveis à falha na maioria dos sistemas, é importante procurar um método para reduzir potenciais erros humanos ou exposição intencional de informações. neste artigo, a computação confidencial é estudada como uma forma de prevenir tais vazamentos de dados executando aplicações dentro de um ambiente de execução confiável.. neste contexto, um ambiente de execução confiável é definido como uma área segura de um processador principal, o que garante que o código e os dados carregados internamente são protegidos com respeito à confidencialidade e integridade. para avaliação, um sistema de informação usando scone foi implementado e uma série de testes de segurança e desempenho em uma aplicação genérica foram executados. os resultados mostraram uma melhoria considerável na segurança do aplicativo e uma deterioração considerável no desempenho da aplicação. os resultados sugerem que a computação confidencial pode proteger os aplicativos contra os ataques de nível de administrador mencionados, mas seu uso deve se adequar a certos casos de uso onde o desempenho não é fundamental para o aplicativo comportamento ou o fato de que são necessários mais recursos para funcionar no mesmo nível de desempenho.', 'em um mundo onde as pessoas têm cada vez mais contato com as tecnologias, surgiram também questões sobre como utilizá-las adequadamente. muitas vezes, essas questões surgem em respostas a perigos percebidos ou comportamentos inapropriados como roubo de identidade, cyberbullying ou disseminação de notícias falsas, especialmente no mundo online. tais abusos podem ser ainda mais prejudiciais aos adolescentes que cada vez mais cedo são inseridos nesse contexto tecnológico. apesar de os usuários terem capacidade de avaliar uma conduta online adequada e identificar riscos, é necessário que eles desenvolvam um senso crítico para utilizar a internet de forma responsável, ou seja, usar o ambiente online sem prejudicar a si mesmos e aos outros. para isso, a cidadania digital surge como um conjunto de normas de comportamento que devem ser seguidas para o uso amigável, seguro e ético das tecnologias. logo, a presente pesquisa tem como objetivo analisar as principais tecnologias digitais utilizadas por adolescentes e, a partir delas, listar formas de uso responsável de acordo com os elementos da cidadania digital a fim de conduzir de forma positiva o comportamento de adolescentes na sociedade e constituir uma cultura de uso seguro das tecnologias digitais.']"
7,11,7_digital_dados_cidadania digital_cidadania,"['digital', 'dados', 'cidadania digital', 'cidadania', 'coletores', 'anos', 'educao', 'digitais', 'desafios', 'lei']","['a cidadania digital envolve o desenvolvimento de competências eficazes e éticas no uso das tecnologias, sempre respeitando princípios fundamentais, como privacidade, segurança e responsabilidade. a educação digital desempenha um papel crucial como parte integrante da cidadania digital, indo além do mero uso das tecnologias e sendo a base para a formação de cidadãos digitalmente conscientes. a educação digital é a chave para preparar as gerações presentes e futuras, capacitando-as a navegar com sucesso no mundo digital, compreender e aplicar as tecnologias de forma ética e eficaz. a crescente demanda no mercado de trabalho por profissionais com habilidades digitais destaca a importância da educação digital em todos os níveis, desde programas técnicos até a educação superior. a essência de uma aprendizagem eficaz reside na concepção de programas de capacitação envolventes, que não apenas desenvolvam habilidades técnicas, mas também promovam uma cidadania digital consciente. dentro desse contexto, este trabalho tem como objetivo contribuir para a promoção da educação digital, fornecendo um guia prático repleto de orientações para o desenvolvimento de cursos, com metodologias ativas e no contexto de extensão, que gerem um impacto significativo na comunidade da cidade de campina grande.', 'nos últimos anos, as apostas esportivas têm experimentado um crescimento rápido devido à digitalização e à acessibilidade das plataformas online. contudo, esse aumento também gerou desafios, como a integridade do mercado, o combate a práticas de crime organizado, como lavagem de dinheiro, e a proteção dos consumidores. nesse cenário, os processos de verificação e validação de documentos, conhecidos como kyc (know your customer - conheça seu cliente), tornaram-se essenciais para garantir a conformidade regulatória e a segurança dos usuários. este estudo se concentra nos desafios enfrentados pelas casas de apostas ao implementar os processos de kyc em conformidade com as regulamentações, especialmente na necessidade de análise manual para validar os processos de verificação de kyc. essas verificações manuais são tipicamente demoradas e custosas. diante desse cenário, foi desenvolvida uma ferramenta de automação que utiliza um modelo llm (large language model) para reduzir a dependência de intervenções humanas e aprimora a eficiência no processo de verificação de documentos de kyc. a metodologia utilizada abrangeu uma amostra de dados de usuários do período de de dezembro de a de fevereiro de , levando em consideração as principais falhas identificadas durante esse intervalo de tempo. os resultados obtidos mostram uma melhoria significativa na performance das verificações de análises manuais. dessa forma, da amostra realizada com casos o algoritmo forneceu um veredito para . % casos, com precisão de , % na categorização correta do status da análise do usuário. isso destaca a eficácia geral do algoritmo, tornando o processo mais ágil e eficiente, reduzindo significativamente a carga de trabalho humana e, em contra partida, . % dos casos ainda exigiram análise manual para maior precisão, evidenciando situações em que o algoritmo não conseguiu tomar uma decisão automática. contudo, a pesquisa não apenas oferece uma solução prática e tecnológica para os desafios enfrentados pelas casas de apostas, mas também contribui para o avanço na automação de processos de verificação de documentos em ambientes regulamentados.', 'o dadosjusbr é um projeto sem fins lucrativos com o objetivo de apresentar de forma detalhada e consolidada as informações de remuneração dos órgãos que constituem o sistema de justiça brasileiro, formado pelos ministérios públicos, defensorias, procuradorias e o judiciário com os tribunais e conselhos, juntos somam órgãos. esse processo é chamado de ‘libertação dos dados’ e possui quatro estágios: coleta, validação, empacotamento e armazenamento. é no estágio da coleta que o crescimento do projeto está associado, pois é necessária a codificação dos coletores, um para cada órgão. o dadosjusbr é um projeto de código fonte livre aberto, sendo assim a comunidade pode participar, escrevendo coletores em múltiplas linguagens de programação, como go e python. com o uso de mais uma linguagem de programação, englobando também a tipagem dinâmica onde é mais difícil forçar um esquema considerando o tipo, surgem diversos problemas para restringir o esquema de dados. o principal deles é a consistência na serialização dos dados coletados, que é muito importante para armazenamento e transmissão entre estágios, pois o modo padrão que as linguagens serializam dados é diferente. neste trabalho propusemos e implementamos a utilização de protocol buffers (pb) para tornar mais fácil manter, transmitir e armazenar dados consolidados pelo dadosjusbr. atualmente temos órgãos coletados, dentre eles o mppb, codificado em golang, o site do conselho nacional de justiça (cnj), codificado em python, que foram os nossos coletores de dados que utilizamos neste trabalho. adaptar os crawlers e parsers, alterando todos os campos desses coletores para lidar com o novo formato de transmissão de dados, acarretou em dificuldades inesperadas, como lidar com timestamp entre as duas linguagens e transmitir o dado em pb no formato de texto, conseguindo assim a serialização dos dados em todos os estágios. assim, consolidando a serialização e transmissão dos dados entre coletores de diferentes linguagens, tornando o dadosjusbr mais democrático e abrangente, facilitando a contribuição.']"
8,10,8_web_tags_ecommerce_produtos,"['web', 'tags', 'ecommerce', 'produtos', 'busca', 'pesquisa', 'gtm', 'performance', 'react', 'pwa']","['a busca por produtos é uma funcionalidade fundamental que permite aos usuários localizar e adquirir itens específicos, sendo aplicada em diversos contextos, como e-commerces e sites de comparação de preços. este estudo compara abordagens léxicas e semânticas para a realização dessa funcionalidade. embora a busca léxica possua vantagens em termos de tempo de resposta, ela não captura relações semânticas entre palavras além das similaridades léxicas. por outro lado, a busca semântica destaca-se ao capturar semanticamente a relação entre os termos, porém, além de possuir maior complexidade, ela também pode ser mais lenta. neste trabalho, analisamos as abordagens léxico-semânticas para dados de produtos, especificamente em dois conjuntos de dados: catálogo de materiais do governo federal e descrições de produtos presentes em notas fiscais. comparamos as estratégias de busca considerando a relevância dos resultados e o tempo de resposta. os conjuntos de dados possuem características distintas, com o catálogo de materiais sendo mais formal e estruturado, enquanto as notas fiscais contêm textos mais curtos e informais, frequentemente com siglas e abreviações. este estudo comparativo busca identificar os trade-offs entre as abordagens léxicas e semânticas, bem como encontrar as estratégias mais adequadas para cada tipo de dado. os resultados contribuem para a seleção de mecanismos de busca mais eficazes em catálogos de produtos, considerando diferentes formas de organização dos dados.', 'com a ascensão de javascript como uma das principais linguagem de programação para web, diversos frameworks surgiram para auxiliar a criação de projetos complexos e escaláveis. uma tendência dos dias atuais é a utilização da web através de dispositivos móveis. neste contexto, ocorre um aumento na demanda por aplicações leves que substituam as aplicações nativas. essa demanda resultou em uma metodologia que combina recursos oferecidos pelos navegadores com as vantagens do uso de um celular. o progressive web app (pwa) é uma metodologia de desenvolvimento que busca trazer para aplicações web, acessadas por navegadores móveis, a mesma experiência (leve e responsiva) vivenciada nos aplicativos nativos. atualmente, existe uma enorme variedade de bibliotecas e frameworks, o que causa dúvidas na tomada de decisão para a construção de um pwa. o objetivo deste trabalho é realizar um estudo comparativo entre três tecnologias, para a implementação de um pwa. para isso, foi desenvolvido um mesmo sistema utilizando angular, vue e react para coletar métricas e realizar análises sobre suas vantagens e desvantagens. como as tecnologias seguem o mesmo paradigma, o estudo apresenta as características, de cada uma, associando essas a implementação de um pwa que consume uma api rest. por fim, foram estabelecidos guidelines para ajudar programadores web a escolher entre as várias tecnologias disponíveis.', 'com o crescente avanço do ecossistema web nos últimos anos, tem havido um surgimento constante de novos frameworks e bibliotecas. entre eles, o reactjs, lançado em pela meta, destacou-se como uma poderosa ferramenta para a criação de interfaces interativas, complexas e eficientes, baseadas em componentes. atualmente, o reactjs possui uma comunidade ativa de desenvolvedores e conta com uma sólida documentação, consolidando-se como uma das principais tecnologias utilizadas no desenvolvimento web. devido à sua ampla adoção em grandes empresas de diversos setores ao redor do mundo e ao crescente uso em novas aplicações web, a meta e outras empresas estão constantemente buscando maneiras de facilitar e agilizar o início de projetos em react. como resultado, surgiram ferramentas como o create-react-app (desenvolvido pela própria meta), vite e parcel. o objetivo deste trabalho é analisar e comparar essas bibliotecas, considerando diferentes parâmetros, como o tempo de instalação utilizando os gerenciadores de pacotes mais comuns, npm e yarn, facilidade de configuração do projeto, estrutura de pastas e arquivos criados, bem como a qualidade e legibilidade do código necessário para sua utilização. como contribuição, serão apresentadas sugestões para o aprimoramento dessas aplicações, levando em consideração os aspectos avaliados. o estudo visa oferecer insights valiosos para desenvolvedores que desejam iniciar projetos em react, auxiliando na escolha da melhor ferramenta e fornecendo recomendações para uma experiência mais eficiente e eficaz.']"
9,10,9_msica_artistas_musical_musicais,"['msica', 'artistas', 'musical', 'musicais', 'msicas', 'spotify', 'longo', 'plugins', 'libras', 'leitura']","['a área de recuperação de informações musicais (music information retrieval ou mir) engloba uma variedade de tópicos, incluindo a transcrição musical, a separação de fontes sonoras, o reconhecimento de instrumentos e/ou gêneros musicais. um exemplo prático de um desses campos é o spotify, que utiliza sistemas de recomendação capazes de aprender o padrão de conteúdo reproduzido e sugere aos usuários músicas similares. no entanto, o reconhecimento de instrumentos ainda pode ser desafiador de acordo com o conjunto de dados utilizado, dificultando o reconhecimento de padrões. nesse contexto, essa pesquisa tem como objetivo treinar um modelo capaz de detectar e identificar instrumentos, além de avaliá-lo em diferentes conjuntos de dados amplamente conhecidos na área de mir. para isso, foram utilizados áudios do openmic- no treinamento e os modelos foram avaliados em três conjuntos de dados, sendo estes: mtg-jamendo, nsynth e áudios de apresentações ao vivo com instrumentos separados utilizando o demucs. a acurácia será um dos critérios utilizados para avaliar o desempenho do modelo. ao abordar essa problemática, espera-se contribuir para avanços na área de mir, permitindo recomendações musicais mais personalizadas por meio do aprimoramento da precisão em sistemas de recomendação. além disso, deseja-se fornecer insights para a comunidade de mir, auxiliando na análise musical e em campos relacionados, a fim de permitir aplicações cada vez mais eficientes.', 'plugins, programas auxiliares na produção musical, são muito desejados por artistas nos dias de hoje por expandirem as possibilidades na criação de música. em seu processo criativo, necessidades específicas aparecem frequentemente, então a idéia de criar seu próprio plugin pode ser muito atrativa. porém, o processo de programação de plugins pode ser consideravelmente contra intuitivo, pois para sequer poder ouvir os resultados que estes podem trazer à composição, normalmente é necessário escrever e compilar o programa de forma separada, para somente então testar. este projeto, então, visa demonstrar a viabilidade de uma alternativa existente, muito mais interativa, apropriada para músicos que tenham menos afinidade com tecnologias de programação. a solução proposta envolve utilizar dois programas, plugdata e hvcc, para criar plugins de diversos tipos, sem a necessidade de escrever código e podendo fazer os testes destes em tempo real. o plugdata fornece ao usuário um ambiente para construir diversas funcionalidades úteis para uma composição, sendo possível ouvir os resultados em tempo real. utilizando então o outro programa, hvcc, é possível converter essa construção do plugdata em código na linguagem c++, compilar esse código e transformá-lo em um plugin nos formatos desejados. assim, a criação de plugins torna-se muito mais acessível aos artistas musicais. para demonstrar isso, foram criados três plugins, de funcionalidades diferentes, os quais foram utilizados na composição de uma faixa musical original.', 'embora a música esteja presente no cotidiano de boa parte da humanidade, durante a pandemia de covid- , muitos artistas da indústria fonográfica viram suas opções para faturar com música diminuírem. com a proibição aos shows e a baixa venda de produtos físicos, o mercado musical teve que se adaptar a diferentes modelos de negócio para sobreviver. neste cenário, o spotify, serviço digital que paga aos artistas por quantidade de streams[ ], vem representando um fôlego para o ramo, que já vinha sofrendo duros golpes pela pirataria. em abril de , o serviço já possuía mais de milhões de usuários ativos mensais[ ]; havia pagado bilhões de dólares aos artistas e gravadoras[ ]; e, no brasil, país que é um dos maiores consumidores de streaming - tanto no modelo gratuito, quanto no pago[ ] - é um dos grandes responsáveis pelo crescimento, nos últimos anos, de mais de % da renda com músicas gravadas - o maior desde , segundo a federação internacional da indústria fonográfica[ ]. em , a relevância do streaming foi tão alta que representou mais da metade da receita mundial de música gravada[ ]. com isso, pode-se afirmar que, hoje em dia, a quantidade de streamings de uma música passou a ser parâmetro de sucesso e até a ser um indicativo do que a sociedade, em geral, ouve. diante disso, este presente artigo apresenta uma análise de dados quantitativa das músicas mais ouvidas no spotify brasil, no período de até o presente ano, com objetivo de ajudar a compreender o gosto musical dos(as) brasileiros(as) através do estudo das características musicais de cada canção. com isso, buscamos identificar os padrões apresentados por essas características ao longo dos anos. os resultados indicam que, na amostra estudada, os(as) brasileiros(as) tendem a preferir canções com duração mais curta , instrumentalidade baixa, volume alto (em decibéis), e baixa presença de palavras faladas. palavras-chave: spotify, análise de dados, música, mercado fonográfico.']"
10,10,10_bugs_relatrios_bug_software,"['bugs', 'relatrios', 'bug', 'software', 'relatrio', 'relatrios bugs', 'erros', 'compilao', 'brs', 'erros compilao']","['a compilação é um processo essencial no desenvolvimento de linhas de produto de software, como o linux. entretanto, identificar erros de compilação em linhas de produto de software (lps) não é trivial, já que os compiladores tradicionais não são conscientes de variação. abordagens anteriores foram propostas que identificam alguns desses erros de compilação usando técnicas avançadas que requerem um esforço dos programadores em usarem. este estudo avalia a eficácia de modelos de linguagem de grande porte (llms), especificamente o chatgpt e le chat mistral, na identificação de erros de compilação em lps. inicialmente foram testados produtos nas linguagens c++, java e c, e posteriormente lps em c, abrangendo tipos diferentes de erros de compilação. os dois llms foram avaliados com base na sua capacidade de reconhecer e diagnosticar corretamente os erros. o chatgpt conseguiu identificar % e % dos erros de compilação em produtos e lps, enquanto que o le chat mistral obteve % e %, respectivamente. a análise revelou que, embora os llms possam identificar uma gama de erros de compilação, desafios específicos permanecem, especialmente em ambientes de lps com alta variabilidade. o estudo sugere a necessidade de refinamentos contínuos nos modelos de llm para melhorar sua precisão e utilidade em cenários de desenvolvimento de software complexos.', 'por dia, novos aplicativos são lançados na playstore e com eles inúmeros bugs, definidos como um erro ou falha em um software ou sistema, que causa um resultado inesperado ou incorreto, ou se comporta de maneira não intencional. para controlar esses bugs surgiram os bugs reports, relatórios feitos por usuários ou profissionais contratados para testar uma aplicação. esses relatórios ajudam tanto aos desenvolvedores a dar manutenção no sistema como a organizar a ordem de prioridade e como cada bug será atacado. esses relatórios são muitas vezes criados com base em feedbacks e avaliações, que apontam alguma falha ou problema. esses feedbacks são lidos e interpretados por pessoas de cunho mais técnico, para aí sim se tornarem bug reports, o que gera uma camada a mais entre o cliente final e o desenvolvedor. neste trabalho, propomos uma ferramenta de integração pensada em conectar o aplicativo com uma ferramenta de bug tracking, aproximando mais a relação entre o cliente e o desenvolvedor. o objetivo é oferecer autonomia para o usuário inal do aplicativo criar um bug report, já preenchendo os dados mais comuns e que não demandam um conhecimento técnico, e após isso já gerar um relatório, dando facilidade ao luxo de criação de relatórios de bugs por usuários de uma aplicação e consequentemente a sua investigação e correção pelos desenvolvedores responsáveis.', 'dentre as atividades típicas de um processo de software, podemos destacar as tarefas de testar, analisar, reportar e corrigir bugs. a realização dessas tarefas é importante para identificar erros comuns ou complexos durante todas as etapas do desenvolvimento, evitando retrabalho e entregando um software com mais qualidade e confiabilidade [ ]. em um relatório de bug, geralmente, seu autor oferece detalhes da anormalidade que vem ocorrendo. tipicamente, um relatório de bug é aberto, o bug é corrigido e o relatório é fechado. contudo, por vezes, é verificado que a correção do bug não foi eficaz, seja por falta de descrição mais objetiva no relatório, seja por dificuldade de entendimento por parte do desenvolvedor. assim, faz-se necessária a reabertura do bug, adicionando tempo no processo de desenvolvimento, tornando o software mais custoso. por isso, é importante investigar o que pode ser feito para mitigar tais problemas. neste trabalho, investigamos as características que levam um bug a ser reaberto. os resultados deste trabalho podem ajudar aos usuários finais e desenvolvedores a melhor escrever relatórios de bugs, bem como aos desenvolvedores a melhor entendê-los e tratá-los. o estudo utilizou um dataset extraído da ferramenta bugzilla.']"
11,10,11_pandemia_sade_pandemia covid_covid,"['pandemia', 'sade', 'pandemia covid', 'covid', 'crise', 'violncia', 'acidentes', 'perodo', 'durante', 'dados']","['em outubro de , a secretaria de saúde de pernambuco emitiu uma declaração de estado de emergência para uma epidemia de microcefalia. em novembro do mesmo ano foi identificado o vírus da zika (zikv) como responsável pela microcefalia ocorrida em bebês. nesse contexto, o cer (centro especializado em reabilitação) da cidade de campina grande-pb, atua na reabilitação de crianças com microcefalia por zika vírus (msc-zikv). parte do tratamento é conduzido por meio de procedimentos de estimulação motora a serem continuados em ambiente domiciliar. após entrevistas e visitas realizadas ao cer, descobriu-se que esses cuidados não vêm sendo feitos de forma adequada, havendo uma diferença entre as tarefas prescritas e as que são realizadas de fato pelos cuidadores. tais diferenças podem comprometer a eficácia do tratamento e prejudicar a evolução do paciente. este trabalho tem como objetivo o desenvolvimento de uma aplicação capaz de auxiliar os cuidadores no processo de estimulação motora de crianças com msc-zikv. a aplicação foi desenvolvida em duas plataformas distintas, sendo elas web e mobile. a aplicação web tem como objetivo possibilitar aos profissionais de saúde do cer o cadastro de atividades e prescrevêlas aos seus pacientes. já a parte mobile é usada pelos cuidadores, a fim de obter melhores instruções quanto às atividades a serem executadas em ambiente domiciliar e o registro dessas atividades.', 'durante a pandemia de covid- , foi necessário que as pessoas se isolassem, devido aos altos índices de contágio e eventuais mortes. de acordo com o ministério da saúde, apenas no brasil já foram mais de milhões de casos confirmados e mil mortes, até o início do ano de . com isso, a população precisou abrir mão de suas formas usuais de entretenimento presencial e socialização, o que levou as formas de passatempo digital a aparecerem como forma alternativa de diversão e socialização. entre elas, uma das que mais viu crescimento neste período foi o uso dos jogos digitais, sendo apreciados até por quem nunca tinha tido contato anterior ao período de isolamento. por isso, neste trabalho, pretendemos analisar como o crescimento do uso dos jogos digitais durante a pandemia gerou impactos positivos e negativos na vida de alunos universitários brasileiros após a pandemia. para isso, foi encaminhado para tais estudantes formulários do google forms, permitindo assim a geração de gráficos comparativos a partir da coleta e análise de dados. assim, conseguimos comprovar um relacionamento entre este crescimento aos efeitos observados após o fim da mesma, tanto para quem já tinha contato com jogos como para quem teve sua primeira experiência. ao final da pesquisa, conseguimos obter respostas de estudantes universitários de graduação e pós, conseguindo obter uma boa perspectiva desta conexão e esperando com isso incentivar trabalhos futuros.', 'diante da atípica situação causada pela pandemia de covid- , ocorrida entre o final de e meados de , o ministério da educação autorizou o ensino remoto em cursos outrora presenciais. essa medida possuía o intuito de amenizar possíveis prejuízos causados ao sistema federal de ensino pela pandemia. nesse contexto, a unidade acadêmica de sistemas e computação da universidade federal de campina grande, juntamente com seu corpo docente, se viu diante do desafio de adaptar sua execução curricular ao ambiente virtual. passada a pandemia, surge a necessidade de analisar as consequências dela, em virtude da necessidade de adaptação das metodologias das aulas ao ensino remoto. este trabalho propõe o emprego de técnicas de análise e visualização de dados, a partir da coleta dos dados do sistema de controle acadêmico online (scao), tendo como objetivo contribuir para a análise de efeitos e implicações exercidos pela pandemia de covid- sobre os estudantes de graduação do curso ciência da computação da universidade federal de campina grande. como resultados encontrados, pode-se citar o fato de que o desempenho acadêmico dos alunos, na maior parte das disciplinas, voltou a alcançar os mesmos níveis do período da pré-pandemia, com uma tendência de aumento de trancamentos durante o pós-pandemia, sendo este o período que possui a maior quantidade de trancamentos registrados.']"
12,10,12_ensino_evaso_curso_ingressantes,"['ensino', 'evaso', 'curso', 'ingressantes', 'ufcg', 'plataformas', 'alunos', 'computao', 'acessibilidade', 'grande']","['a evasão nas instituições de ensino superior no brasil é um problema que gera impactos de cunhos financeiro e social, para toda a população. no curso de computação da universidade federal de campina grande (ufcg) a evasão é motivo de preocupação de professores e gestores, sobretudo em relação aos alunos ingressantes do curso. o programa de educação tutorial (pet) em computação da ufcg desenvolveu uma abordagem complementar e colaborativa de formação de alunos veteranos e ingressantes, denominada gesto, com o intuito de diminuir os índices de evasão dos estudantes ingressantes nesta fase desafiadora de início da experiência universitária. a abordagem é baseada na prática do voluntariado e na construção social de sentidos do estudo de computação, através de atividades de desenvolvimento de habilidades técnicas (hardskills) e não-técnicas (softskills) escolhidas pelos alunos em um portfólio que lhes é oferecido semestralmente. este trabalho apresenta uma análise dos impactos desta abordagem a partir do estudo de três atividades escolhidas pelos ingressantes dentre aquelas disponíveis no portfólio oferecido no semestre letivo de . . resultados das avaliações feitas pelos alunos participantes, ingressantes e veteranos, indicam que tal abordagem pode, juntamente com outras ações, diminuir a evasão no curso de ciência da computação na ufcg.', 'todas as áreas de atividades precisaram se adaptar por causa do distanciamento social provocado pelo novo coronavírus. o ensino à distância (ead) foi a principal alternativa para estudantes e professores durante a quarentena. para que o ensino não presencial ocorresse de forma estruturada e eficiente, o uso de plataformas digitais de apoio ao ensino não presencial e ead foi essencial. ead e ensino não presencial são modalidades distintas, com características em comum. o ensino não presencial diz respeito às atividades de ensino mediadas por tecnologias e orientadas pelos princípios da educação presencial, enquanto o ead, que também utiliza as plataformas digitais, tem seu formato próprio de ensino-aprendizagem. diante da pandemia, houve um aumento dos usuários nas plataformas digitais, e, consequentemente, a coleta de dados pelas mesmas foi intensificada. é importante ressaltar que o fornecimento, tratamento e armazenamento dos dados pessoais dos usuários precisam estar alinhados com a legislação a fim de coibir o uso indiscriminado dos dados coletados. nesse sentido, o presente artigo aborda a temática do uso dos dados pelas plataformas digitais utilizadas no ensino superior durante a pandemia. a pesquisa tem como objetivo analisar as plataformas google classroom, zoom cloud meeting, microsoft teams e slack para saber como as mesmas tratam os dados dos usuários. a conhecida lei geral de proteção de dados (brasil, ), cuja sigla é lgpd, estabelece regras sobre coleta, armazenamento, tratamento e compartilhamento de dados. a metodologia parte da pesquisa das políticas de privacidade das plataformas. desta forma, os resultados esperados atendem a dois propósitos: por um lado, contribuir para que o usuário saiba como são usados e armazenados os seus dados; por outro lado sinalizar recomendações para os usuários das plataformas.', 'as instituições de ensino superior (ies) desempenham um papel fundamental na formação de profissionais qualificados, contribuindo significativamente para o desenvolvimento do país. no entanto, elas enfrentam desafios que afetam sua capacidade de cumprir esse papel de forma adequada. a gestão eficiente de seus recursos é uma das preocupações mais persistentes. muitos estudos avaliam a eficiência de vários aspectos da gestão através de uma avaliação do custo monetário associado a diferentes atividades. embora seja crucial ter uma visão clara desses aspectos, é igualmente importante considerar fatores específicos, como, por exemplo, a carga horária de docentes dedicada diretamente à formação dos discentes. apesar de existir um empenho significativo no sentido de otimizar esse custo, é notável a falta de estudos que detalhem como a trajetória acadêmica do discente afeta seu custo na ies. compreender como esse custo interfere na eficiência da instituição é crucial para tomar medidas direcionadas à melhoria da gestão e otimização dos recursos disponíveis. neste trabalho foram propostas métricas para medir a eficiência de cursos de graduação da universidade federal de campina grande (ufcg), durante um período de anos, no que diz respeito à alocação de docentes às atividades de ensino. como resultado da mensuração realizada, pode-se observar que cursos com um número de ingressantes baixo, assim como cursos com altas taxas de evasão e de retenção, têm um custo alto, enfatizando a necessidade de tratar cuidadosamente cada um dos aspectos relacionados ao cálculo da eficiência dos cursos analisados.']"
13,9,13_modelos_poda_arestas_acurcia,"['modelos', 'poda', 'arestas', 'acurcia', 'ml', 'cdigo', 'complexidade', 'redes neurais', 'neurais', 'redes']","['com a recente adoção de inteligências artificiais generativas no campo da ciência da computação, surge o questionamento dos limites dessas ferramentas na geração de código e no desempenho desse código na resolução de problemas de programação de alta complexidade, como desafios envolvendo programação dinâmica. neste estudo, analisaremos o desempenho de dois modelos de ia(inteligência artificial), o bard e o chatgpt, ao submetê-los a problemas de programação de diversos níveis de complexidade. utilizaremos os prompts de ambas as ferramentas em um estudo de caso comparativo que aplicará o código gerado por esses modelos em problemas de programação obtidos de juízes online, incluindo codeforces, atcoder, codechef e leetcode. compararemos os resultados desses modelos em competições online realizadas regularmente pelas plataformas citadas, os textos das questões serão submetidos para as ferramentas enquanto as competições estiverem acontecendo, garantindo que os problemas sejam inéditos e desconhecidos pelas ias. este estudo visa obter dados consistentes para avaliar a capacidade do bard e do chatgpt . na resolução de problemas de programação com enunciados desconhecidos por ambas as ferramentas. os resultados contribuirão para um entendimento mais aprofundado do desempenho dessas ias em competições de programação e para pesquisas futuras relacionadas ao uso de ia na resolução de desafios computacionais complexos.', 'no contexto de teoria dos grafos, é comum encontrarmos otimizações centradas no łinterior do algoritmož, de maneira a tentar reduzir a complexidade assintótica do mesmo. porém, em determinados casos, não é possível reduzir a complexidade do algoritmo, mas isso não deine o mínimo de operações a ser feitas para resolver determinado problema. isso acontece pois podem ser feitas manipulações no próprio grafo recebido como entrada, possibilitando que um algoritmo especíico possa executar de maneira mais eiciente. nesse contexto, apresentamos uma otimização que permite criar arestas de um vértice para todos os vértices de determinado intervalo num grafo ou até criar arestas entre todos os vértices de dois intervalos, com um custo equivalente ao de adicionar apenas o(log(u) + log(v) arestas, sendo u e v o tamanho do primeiro e segundo intervalos respectivamente. isso pode ser feito através da adição de duas árvores de segmentos no grafo, preservando as distâncias e a conectividade do mesmo. após isso, veriicamos o nível de diminuição de arestas obtido através desse algoritmo em grafos densos, onde podem existir várias arestas entre intervalos de vértices, ainda que de maneira aleatória. com essa otimização, conseguimos diminuir a quantidade de arestas em cerca de % em grafos com pelo menos % das arestas possíveis presentes.', 'o avanço dos modelos de deep learning tem proporcionado resultados excepcionais em tarefas de visão computacional e processamento de linguagem natural. no entanto, o aumento do tamanho e complexidade desses modelos traz desafios significativos em termos de infraestrutura e custos operacionais. nesse contexto, a técnica de poda em redes neurais profundas surge como uma solução para reduzir o tamanho dos modelos, mantendo níveis similares de acurácia. este estudo investiga o impacto da utilização de métricas de explicabilidade (conductance e layer-wise relevance propagation) como critério de poda, comparando-as com a poda por magnitude de pesos e a poda aleatória. diferentes percentuais de poda são avaliados, considerando tanto a poda de oneshot quanto a poda iterativa. os resultados mostram uma correlação positiva entre o uso de métricas de explicabilidade e a melhoria na qualidade dos modelos podados, incluindo maior acurácia, menor variância e a capacidade de realizar podas mais agressivas sem perda significativa de acurácia. esses métodos promissores têm o potencial de melhorar a operacionalização e reduzir os custos associados aos modelos de deep learning em larga escala.']"
14,9,14_reviso_comentrios_cdigo_reviso cdigo,"['reviso', 'comentrios', 'cdigo', 'reviso cdigo', 'prs', 'refatoramentos', 'github', 'comentrios reviso', 'repositrios', 'code']","['os comentários feitos em pull requests (prs) de repositórios git, podem induzir a refatoramentos, que são melhorias feitas no código-fonte, preservando o seu comportamento. com base nos comentários de revisão, desenvolvedores são capazes de decidir por refatorar o código-fonte. este trabalho tem como objetivo analisar prs de repositórios java do projeto apache [ ] no github à luz dos comentários dos revisores e refatoramentos realizados em tempo de revisão de código (minerados previamente pelo refactoringminer - uma ferramenta estado-da-arte para a detecção de refatoramentos). para tanto, propõem-se uma análise manual de comentários de revisão em uma base de dados que contém prs. foram definidos pontos específicos a serem analisados em cada comentário, buscando atingir objetivos específicos definidos para ajudar na pesquisa. essa análise foi feita de forma manual, lendo, caracterizando e verificando o comportamento desses pontos levantados e fazendo anotações sobre cada pull request. ao final deste trabalho foram notados diversos padrões que se repetiam entre os prs, esses padrões foram compilados e serão apresentados, e com isso espera-se entender de que forma os comentários de revisão influenciam nos refatoramentos de código-fonte.', 'com o passar dos anos, a revisão de código vem mudando; antes, era feita uma inspeção manual (rigorosa e síncrona), já nos dias atuais, é feita uma revisão mais moderna (assíncrona e menos rigorosa). atualmente, o git, através da plataforma github, é o sistema de controle de versões mais popular, favorecendo diversas discussões relacionadas a mudanças no código-fonte. com o auxílio de ferramentas como refactoringminer, que fornece a detecção de refatoramentos aplicados aos códigos-fonte e, utilizando-se de uma amostra de repositórios provenientes do projeto do apache no github, este trabalho, através de inspeções manuais de comentários de revisão, visa entender e caracterizar os comentários que induziram refatoramentos nos prs, com o intuito de entender as características próprias e diferenças dos comentários de revisão em prs com e sem refatoramentos. através das hipóteses levantadas, tentamos complementar o entendimento da parte qualitativa dos comentários de revisão, abordados anteriormente de forma similar em outra pesquisa, que analisava dados qualitativos e quantitativos de prs que induziram refatoramentos e de prs que não induziram refatoramentos, com a intenção de entender melhor as diferenças entre os dois tipos de prs, no nível de pull request.', 'o github, atualmente a maior plataforma para hospedagem de código e controle de versionamento, possui um enorme fluxo diário de interações entre usuários e repositórios. com o número de repositórios hospedados na casa dos milhões, alguns projetos que poderiam ser do interesse de alguns usuários acabam passando despercebidos, assim como projetos que necessitam de desenvolvedores, acabam ficando no ostracismo. para esses casos, surge a necessidade de algum mecanismo que possa facilitar a escolha de projetos, pelo usuário. na literatura outros trabalhos, já realizaram estudos sobre esse contexto, recomendando projetos com diferentes abordagens. entretanto, ainda há espaço para novos estudos, utilizando novos aspectos, na tentativa de verificar e validar outros resultados. por isso, esse trabalho busca encontrar projetos relevantes para o usuário, baseando-se nos interesses do mesmo, na plataforma github, utilizando um conjunto de features com o auxílio de algoritmos de learning to rank. analisamos a efetividade learning to rank, no contexto de recomendação de projetos, utilizando os algoritmos ranknet, adarank e listnet, usando como espaço amostral repositórios e usuários do github. os resultados mostram, a relevância da variável resposta e que a abordagem de learning to rank para recomendação de projetos oferece muito espaço para exploração.']"
15,9,15_financeira_financeiro_fiscais_mercado,"['financeira', 'financeiro', 'fiscais', 'mercado', 'notas', 'resultados', 'elasticsearch', 'reteno', 'dados', 'notas fiscais']","['as novas tecnologias estão cada vez mais presentes no cotidiano da humanidade, consequentemente isso acarreta grande aumento na produção de dados digitais. dessa forma, os sistemas de gerenciamento de bancos de dados estão sendo cada vez mais demandados. nesse âmbito, especificamente em ambientes modularizados, notou-se uma necessidade em analisar os desafios em lidar com o controle de versão nos esquemas relacionais. a construção desta pesquisa foi dividida em três etapas: uma análise de um caso real de uma grande empresa de soluções fiscais; um estudo sobre as alternativas atualmente disponíveis no mercado; e, finalmente, a sugestão de soluções iniciais para o versionamento neste tipo de ambiente. a partir dos estudos realizados, foi visto que o sistema da empresa é extremamente denso considerando as questões de práticas, processos e ferramentas utilizadas. logo, apesar de existir alternativas no mercado, adaptar uma integração de uma solução externa ao sistema da companhia acarretaria mudanças radicais e em alguns casos até inviáveis. assim, é recomendado buscar uma solução mesclada, para manter os processos da companhia e aderir a ferramentas externas que otimizem a qualidade e a eficiência do controle de versão.', 'é notório o crescimento do ingresso de investidores individuais no mercado financeiro. muitos desses investidores não conseguem discernir corretamente onde devem investir o seu dinheiro para obter uma maior rentabilidade. neste contexto, este trabalho teve como objetivo desenvolver uma aplicação para auxiliar nas decisões de compra e venda de ativos por meio de uma rede neural treinada de forma supervisionada sobre dados extraídos das cotações dos ativos. a aplicação desenvolvida busca apresentar uma forma intuitiva para guiar as decisões diárias de investimento (day trading) no tocante às ações que devem permanecer, entrar ou sair da carteira de cada usuário. o princípio é prever a cotação do dia seguinte da ação, com recomendação de venda se o preço previsto for diminuir e de compra se o preço for aumentar. a aplicação informa também uma estimativa sobre quanto cada operação de compra ou venda produzirá de lucro. para isso, foram feitos experimentos com três ativos diferentes que atendiam as principais movimentações do mercado e foi possível notar um acréscimo no valor inicial investido pelo usuário de aproximadamente % a % no montante final. a ferramenta desenvolvida poderá auxiliar tanto usuários iniciantes no mercado de ações quanto usuários mais experientes.', 'o objetivo do tce-ac é fiscalizar as despesas e receitas dos municípios e do estado do acre. para tanto, nos últimos anos tem modernizado a sua forma de trabalho. em particular, o acesso rápido aos preços praticados é fundamental para a fiscalização e também para a população em geral. para isso, o banco de preços é utilizado, sendo alimentado por uma base de dados em constante crescimento e que, atualmente, conta com dezenas de milhões de registros de notas fiscais. diante desse cenário, por utilizar de banco de dados relacionais para a realização das consultas e devido a grande massa de dados existente, o sistema em questão acaba demorando para produzir resultados em diversas situações, além de retornar resultados pouco relevantes em algumas situações. para solucionar o problema, propõe-se a implantação do elasticsearch como o motor de busca do sistema. o elasticsearch utiliza técnicas de indexação e possui ferramentas que otimizam a execução e resultados das queries realizadas. além disso, serão implementadas estratégias para a carga contínua dos dados, além da documentação dos desafios enfrentados durante a implementação. para avaliar a solução proposta, foram realizadas medições de estatísticas referentes ao tempo de resposta e qualidade das consultas antes e depois da implantação do elasticsearch. a qualidade dos resultados foi verificada por meio de técnicas como ndcg (normalized discounted cumulative gain) e f1-score, a partir da definição dos documentos relevantes ou não para cada consulta. como resultado, foi possível notar uma diminuição em vezes do tempo de respostas das consultas realizadas no elasticsearch quando comparado com os resultados envolvendo o sql server. além disso, também foi possível observar uma melhora na relevância dos resultados retornados de cerca de %, chegando a um ndcg de , % em média, para consultas com resultados, utilizadas por padrão no sistema.']"
16,8,16_recursos_nuvem_serverless_armazenamento,"['recursos', 'nuvem', 'serverless', 'armazenamento', 'cold', 'cold start', 'start', 'recursos nuvem', 'funes', 'modelo']","['plataformas serverless são um modelo de negócio bastante atrativo por abstrair do desenvolvedor a infraestrutura do servidor e permití-lo focar na lógica da sua aplicação. adotamos aqui o faas (funções como serviço), que é uma modalidade de plataformas serverless em que o cliente implementa funções que são executadas na nuvem de forma stateless. porém, devido ao modelo de cobrança dessas plataformas, em que se paga apenas pelo tempo em que a função de fato está sendo executada, não é vantajoso para o provedor que funções permaneçam ociosas ocupando os recursos da nuvem. esse modelo traz consigo o problema do cold start: muitas vezes, o tempo de instanciação de uma função pode ser muito lento, o que acaba por afastar os clientes do faas. previamente, a técnica do prebaking, baseada em criar snapshots de funções em execução e restaurá-los posteriormente, de forma a reduzir o cold start, mostrou-se bastante eficaz para a runtime de java. neste trabalho, verificamos que para a runtime de python a melhora também é significativa: conseguimos reduzir o tempo de instanciação de uma função em até %.', 'o modelo de computação serverless fortaleceu a tendência da computação em nuvem de tornar transparente o gerenciamento da infraestrutura. ao simplificar o gerenciamento, o modelo serverless deixa a responsabilidade de implantação e escalonamento para a plataforma. aliado a isso, com um modelo de cobrança que considera somente o tempo despendido com a execução de requisições, há um forte incentivo para o uso eficiente dos recursos. essa busca por eficiência, traz à tona o problema de cold-start, que se configura como um atraso na execução de aplicações serverless. dentre as soluções propostas para lidar com o cold-start, se destacam as baseadas no método de snapshot. apesar da exploração desse método, existe uma carência de trabalhos que avaliam os trade-ofs de cada proposta. nessa direção, este trabalho compara duas estratégias de mitigação do cold-start: prebaking e seuss. avaliamos o desempenho das estratégias experimentalmente com funções de diferentes níveis de complexidade: noop, uma função que converte markdown para html, e uma que carrega mb de dependências. resultados preliminares indicam que prebaking apresentou desempenho % e % superior para inicializar noop e markdown, respectivamente e processou a primeira requisição de markdown com um tempo % inferior ao seuss.', 'o laboratório de sistemas distribuídos (lsd) é responsável por gerenciar a nuvem privada do uasc/ufcg. para manter essa nuvem privada, o lsd enfrentou desaios no gerenciamento dos recursos da nuvem, especialmente na visualização de dados e na geração de novos relatórios para os responsáveis pelos projetos que usam esses recursos. anteriormente, a ferramenta utilizada era o shylock, mas ela apresentava diiculdades na geração de relatórios sobre a quantidade e o consumo de recursos da nuvem, pois gerava apenas um relatório por dia. além disso, a criação de relatórios era um processo complexo que exigia a criação de um novo modelo jinja e a codiicação de variáveis. dado os déicits encontrados no sistema atual, o redash foi a plataforma selecionada para supri-los. o redash resolve a diiculdade de visualização dos dados analisados através da criação de dashboards para a visualização e monitoramento dos recursos da nuvem em tempo real. ao incluir as informações dos dashboards criados no redash no luxo de trabalho de gerenciamento e monitoramento da nuvem, a facilidade de tomar decisões com base em dados reais e atualizados permitiu uma melhor tomada de decisões sobre o gerenciamento dos recursos da nuvem.']"
17,8,17_modelos_linguagem_processamento linguagem natural_processamento linguagem,"['modelos', 'linguagem', 'processamento linguagem natural', 'processamento linguagem', 'linguagem natural', 'perguntas', 'natural', 'llms', 'processamento', 'texto']","['nos últimos anos, avanços na área de inteligência artiicial permitiram desenvolvimento de modelos de processamento de linguagem natural em prol de aplicações em diversos contextos, como a automatização do processo de elaboração de questões sobre temas especíicos. atualmente, existem modelos capazes de formular questões sobre um tópico qualquer após receber como entrada um texto relevante. tais projetos possuem grande potencial auxiliar no contexto educacional, entretanto, ainda existe carência de um sistema que forneça a seus usuários uma maneira fácil e intuitiva de utilizar esses modelos. esse trabalho tem como objetivo o desenvolvimento de uma aplicação web que supra essa necessidade, incorporando modelos de processamento de linguagem natural que recebem como entrada um texto e geram para o usuário uma lista de perguntas relevantes ao tema. a utilização da aplicação web também traz a oportunidade de obter feedback dos usuários sobre a qualidade das perguntas geradas, informação que pode ser utilizada para retroalimentação e aprimoramento dos modelos utilizados.', 'a arquitetura baseada em transformers, desenvolvida para resolver problemas de machine learning relacionados ao processamento de linguagem natural, expandiu-se para outras áreas, como a previsão de séries temporais. diferentes modelos, baseados em regressão ou árvores de decisão, são utilizados nesse campo, como o arima , xgboost e prophet, por exemplo. cada abordagem possui suas especificidades em termos de precisão e eficiência computacional, o que requer estudos para determinar a melhor a ser adotada em diferentes cenários. nesse contexto, este trabalho visa apresentar resultados comparativos e analíticos de diferentes abordagens para previsão de séries temporais. para tal, foi realizado um estudo comparativo analisando o uso da arquitetura baseada em transformers e seu desempenho frente a modelos tradicionais, utilizando dados referentes aos registros de consumo energético da universidade federal de campina grande (ufcg). a partir dessa análise, foi possível analisar as abordagens em relação à precisão e eficiência computacional, verificando se a complexidade de uma abordagem mais sofisticada, como a dos transformers, para previsão de séries temporais, se mostra superior em relação às outras abordagens populares utilizadas para esse fim.', 'avanços recentes em modelos de linguagem de grande escala (llms) expandiram significativamente as capacidades da inteligência artificial (ia) em tarefas de processamento de linguagem natural. no entanto, seu desempenho em domínios especializados, como a ciência da computação, permanece relativamente pouco explorado. este estudo investiga se os llms podem igualar ou superar o desempenho humano no poscomp, um exame brasileiro prestigiado usado para admissões de pós-graduação em ciência da computação. quatro llms-chatgpt- , gemini . advanced, claude sonnet e le chat mistral large-foram avaliados nos exames poscomp de e . a avaliação consistiu em duas análises: uma envolvendo interpretação de imagens e outra somente de texto, para determinar a proficiência dos modelos em lidar com questões complexas típicas do exame. os resultados indicaram que os llms tiveram um desempenho significativamente melhor nas questões baseadas em texto, com a interpretação de imagens representando um grande desafio. por exemplo, na avaliação baseada em imagens, o chatgpt- respondeu corretamente de perguntas, enquanto o gemini . advanced conseguiu apenas respostas corretas. na avaliação baseada em texto de , o chatgpt- liderou com respostas corretas, seguido por gemini . advanced ( ), le chat mistral ( ) e claude sonnet ( ). o exame de mostrou tendências semelhantes.']"
18,8,18_acrnimo_sade_tratamento_feridas,"['acrnimo', 'sade', 'tratamento', 'feridas', 'oftalmolgicas', 'aplicativo', 'medicamentos', 'reconhecimento', 'feridmetro', 'fraturas']","['a cada segundos uma vértebra da coluna é fraturada no mundo e com o envelhecimento natural da população esse tempo tende a diminuir. o principal método para analisar tais fraturas é a tomografia computadorizada (também conhecida como ct) e o tempo dispendido para a sua análise pode ser determinante para minimizar a ocorrência de sequelas nos pacientes. sendo assim, o emprego de técnicas como redes neurais convolucionais pode auxiliar o profissional de saúde a tomar uma decisão com maior agilidade. neste contexto, este artigo visa investigar uma abordagem fundamentada em uma rede neural convolucional para localizar fraturas em fatias de ct. a base de dados de detecção de fraturas da coluna cervical da rsna (sociedade radiológica da américa do norte) , disponibilizada em uma competição da plataforma kaggle, foi utilizada para fins de treinamento e validação experimental de uma rede neural proposta. espera-se que a investigação realizada neste trabalho e o modelo obtido possam ajudar profissionais de saúde na tarefa de detecção e localização de fraturas na coluna cervical.', 'doenças oftalmológicas, como catarata, glaucoma e retinopatia diabética, representam um desafio significativo para a saúde pública, com potencial de causar perda de visão. no entanto, a maioria desses casos poderia ser evitada ou tratada se diagnosticada precocemente. neste contexto, a imagem de fundo de olho surge como uma ferramenta de diagnóstico eficaz, rápida e não invasiva. a interpretação manual de imagens oftalmológicas é repetitiva e sujeita a erros. assim, sistemas computacionais podem ser utilizados para auxiliar os profissionais na triagem automatizada, reduzindo tempo, erros e esforço na análise das doenças. os sistemas de aprendizado profundo provaram ser eficazes nesse contexto, entretanto, sua falta de transparência tem sido um desafio para a adoção clínica, o que destaca a importância da explicabilidade nos modelos de aprendizado de máquina. este estudo contribui para o avanço da compreensão e interpre-tação de modelos de aprendizado profundo na área da saúde ocular, visando melhorar o diagnóstico e tratamento de condições oftalmológicas. ele compara as técnicas lime e grad-cam aplicadas a diferentes arquiteturas de redes neurais convolucionais (cnns) treinadas para classificar condições oftalmológicas a partir de imagens de fundo de olho. os resultados indicam que o modelo vgg16 se destaca, alcançando uma acurácia de , % no treinamento e , % na validação. além disso, as técnicas de explicabilidade, embora distintas em abordagem, identificaram quase as mesmas regiões de interesse nas imagens oftalmológicas. ainda assim, apesar de haver limitações, como a aleatoriedade do lime e a necessidade de ajustes no grad-cam, o lime destacou áreas críticas de forma mais sutil, enquanto o grad-cam forneceu representações visuais mais diretas e intuitivas.', 'o acrônimo timers é utilizado por profissionais da saúde para direcionar e avaliar a conduta adequada em cada situação de tratamento e avaliação de feridas de acordo com o estado da enfermidade. seu objetivo é auxiliar na ava-liação inicial e proporcionar meios de tratamentos corretos e eficazes, de acordo com os parâmetros avaliados. o aplicativo feridômetro é um aplicativo portátil desenvolvido inicialmente pela professora lidiany galdino e pelo aluno adiel andrade rocha, o projeto está em código aberto e tem como finalidade ser uma ferramenta de auxílio aos profissionais e alunos que buscam aprimorar seus conhecimentos sobre o acrônimo. neste trabalho, serão desenvolvidas novas funcionalidades para o aplicativo, com o intuito de melhorar a experiência do usuário, criar novas ferramentas de aprendizado e canal de comunicação entre os usuários, assim como, complementar o conteúdo vinculado ao aplicativo referente ao acrônimo. a partir dessas integrações, os usuários terão mais facilidade no aprendizado, comunicação entre outros usuários e terão todo o conteúdo explicativo sobre o acrônimo.']"
19,8,19_alunos_professores_aluno_sistema,"['alunos', 'professores', 'aluno', 'sistema', 'esquemas', 'ufcg', 'computao', 'professor', 'acadmico', 'curso']","['corrigir e fornecer feedbacks a respeito dos esquemas sql criados pelos alunos de cursos de banco de dados pode ser um processo longo e árduo para os professores. isso se deve à complexidade dos esquemas, que podem ser escritos de diversas formas, mesmo com um roteiro, que contém os requisitos funcionais sobre o que deve ser implementado, exigindo que o professor tenha bastante atenção durante a correção do esquema para identiicar possíveis problemas ou erros no esquema do aluno. além disso, os critérios de correção dos esquemas costumam ser bastante especíicos, exigindo que o professor descreva exatamente no que o aluno errou e o que poderia ter feito de melhor. dessa forma, pode ser difícil para os professores de cursos de banco de dados acompanharem o ritmo de entrega dos esquemas dos alunos e, consequentemente - garantir que cada aluno receba um feedback a respeito do esquema criado, sem atrasos. neste trabalho foi desenvolvido uma aplicação de linha de comando que visa ser uma ferramenta para auxiliar a correção dos esquemas criados pelos alunos, com base em critérios de correção à serem deinidos pelo professor. espera-se que a aplicação auxilie os professores de cursos de banco de dados nas correções de esquemas sql, tornando-a correção mais automatizada e menos manual.', 'a unidade acadêmica de sistemas e computação (uasc) possui (onze) laboratórios focados em diversas áreas da tecnologia e estes adotam processos seletivos para alocação de discentes em seus projetos. a maioria das ofertas de vagas feitas pelos docentes para esses projetos é realizada via e-mail acadêmico, onde estas podem passar despercebidas por diferentes motivos e, além disso, não estão centralizadas em uma plataforma que contenha todas as chamadas para projetos. portanto, há uma lacuna a ser resolvida principalmente no que diz respeito à gestão destas seleções, tanto pelos professores, quanto pelos alunos. este trabalho tem a proposta de desenvolver o vivagas, uma plataforma que tem como objetivo principal facilitar a gestão de todo o processo seletivo mencionado anteriormente. o vivagas oferece funcionalidades tanto para professores quanto para alunos. professores publicam vagas, buscam e filtram alunos com base em seus perfis e conhecimentos, e realizam o processo seletivo de forma eficiente. já os alunos têm acesso à visualização de vagas disponíveis, podem filtrá-las com base em critérios como área de atuação e candidatar-se nas vagas disponíveis. a avaliação da plataforma foi realizada utilizando o think aloud protocol, que consiste em um teste de pensamento em voz alta no qual o usuário faz uso do sistema. além disso, testes foram realizados através da versão adaptada do questionário csuq com objetivo de recolher métricas de usabilidade de um dado sistema. também foi utilizado a ferramenta lighthouse que realiza auditorias automatizadas em páginas da web. os resultados do lighthouse indicaram áreas de otimização, mas destacaram a ótima acessibilidade, práticas de desenvolvimento e otimização. o csuq refletiu uma avaliação bastante positiva. da mesma forma, o think aloud protocol forneceu insights valiosos e clareza do fluxo de usabilidade. em suma, a avaliação geral apontou para bons resultados e uma excelente usabilidade do vivagas.', 'as transformações tecnológicas vividas nos dias atuais impactam diretamente a produção de trabalho dos profissionais em qualquer área do conhecimento. diante disso, a unidade acadêmica de sistemas e computação (uasc) da universidade federal de campina grande (ufcg) oferta a disciplina “introdução à ciência da computação” para os cursos envolvidos com ciências exatas ligados a outras unidades da instituição. porém, tendo em vista que os alunos pertencem a vários cursos, torna-se desafiador aplicar uma metodologia apropriada a essas variedades e atender todas as demandas da formação profissional destes estudantes. sendo assim, neste trabalho investigamos a contribuição desse componente curricular para a formação dos acadêmicos. além disso, identificamos quais são as necessidades requeridas para os alunos que a cursam. para responder isso, desenvolvemos um estudo misto, quantitativo e qualitativo baseado em entrevistas que foram aplicadas a dois grupos: i. alunos egressos e ii. coordenadores dos cursos. os relatos revelam que o principal requisito para os alunos desta disciplina é o conhecimento básico de programação, utilizando uma linguagem de programação que seja mais didática e atualizada. além disso, identificamos nos relatos a necessidade do uso e manipulação de planilhas, requisito esse que foi mais evidenciado pelas áreas que envolve engenharia. ademais, os entrevistados acreditam que a disciplina é de extrema relevância para a formação dos acadêmicos. porém, dentre os relatos analisados, percebe-se que a atual forma como a disciplina é ofertada não está sendo suficiente para suprir as necessidades dos alunos que a cursam, dentre as causas mais importantes que levaram a esta afirmação estão: conteúdo defasado, dessincronização das turmas e dificuldade para aplicar os conhecimentos adquiridos.']"
20,8,20_ocr_documentos_prprocessamento_extrao,"['ocr', 'documentos', 'prprocessamento', 'extrao', 'textos', 'texto', 'informaes', 'pdf', 'imagens', 'documentos pessoais']","['este estudo aborda a importância da extração precisa de informações em documentos pdf, destacando os desafios enfrentados devido à falta de uniformidade na estrutura e layout desses documentos. a extração de texto em documentos pdf, especialmente em contextos como diários oficiais, é crucial para automatizar processos e otimizar a análise de informações relevantes. a métrica rouge é utilizada para avaliar a qualidade da extração de texto pelas ferramentas e a importância de extrair todas as informações do texto original preservando a ordem de leitura. diante da ineficiência e alto custo associado à extração manual de texto de documentos em formato pdf, este estudo visa proporcionar percepções significativas que auxiliam na escolha da ferramenta mais adequada, considerando os diferentes cenários de aplicação na extração de texto. a avaliação das ferramentas escolhidas, juntamente com a mensuração dos resultados através de métricas pertinentes à avaliação dos textos extraídos, aprimora a eficácia e a eficiência na análise dessas ferramentas.', 'sistemas de extração de informação auxiliam humanos na busca de informação específica em documentos. no entanto, a maioria destes sistemas não dão suporte a documentos no formato portable document format (pdf), que é largamente utilizado. em um documento pdf, o conteúdo do texto é misturado com metadados ou dados semi-estruturados, que dificultam os algoritmos de processamento de linguagem natural (pln) na extração da informação requerida. o tribunal de contas do estado do acre (tce-ac) é o órgão fiscalizador e controlador do uso do dinheiro público e da administração orçamentária e financeira do estado do acre, responsável por analisar e julgar as contas públicas dos jurisdicionados. os jurisdicionados devem publicar informações relacionadas às licitações tanto no sistema de gerenciamento de licitações do tce-ac como também no diário oficial do estado do acre (doe), que usa o formato pdf. é de responsabilidade do tce-ac verificar se as informações da licitação estão nos dois lugares, gerando assim, um grande trabalho manual. neste trabalho, apresentamos uma solução de pln com objetivo de extrair os atos do doe, categorizar automaticamente os atos como licitação ou não, em caso afirmativo, serão utilizadas técnicas avançadas de pln para processar e extrair as entidades e informações da licitação para que seja possível auxiliar o tce-ac a verificar se a licitação', 'o reconhecimento óptico de caracteres (ocr) desempenha um papel fundamental na digitalização e processamento de documentos pessoais, no entanto, enfrenta desafios significativos de precisão e eficiência, visto que as ferramentas que realizam ocr ainda dependem muito da qualidade da entrada de dados e das condições em que os documentos são escaneados ou fotografados. para aperfeiçoar o reconhecimento óptico de caracteres (ocr), propõe-se a utilização da combinação de técnicas de pré-processamento e pós-processamento a fim de melhorar a qualidade do ocr. o processo inicia-se através da coleta de um conjunto de dados representativo de imagens de documentos pessoais. após a coleta, realiza-se o pré-processamento e pós-processamento das imagens, seguindo então do ocr e a utilização de uma métrica que avalia o ocr obtido. as técnicas de pré-processamento incluíram modificação do dpi das imagens, suavização da imagem e conversão para escala de cinza, seguida pela aplicação do ocr. além disso, houve um pós-processamento para remover a acentuação do texto extraído e convertê-lo em letras maiúsculas. os resultados indicaram que o pré-processamento melhorou significativamente a precisão do ocr para documentos de identidade (rg), aumentando o f1-score de . (sem pré-processamento) para . (com pré-processamento). para imagens de cpf, o pré-processamento resultou em uma precisão de . % e uma taxa de erro de . %, enquanto o ocr sem pré-processamento teve uma precisão de . % e uma taxa de erro de . %. este estudo visa investigar técnicas com o propósito de melhorar o reconhecimento óptico de caracteres em documentos pessoais, contribuindo para maior precisão do ocr, com potenciais benefícios para aplicações que realizam a extração de conteúdo de imagens de documentos pessoais.']"
21,8,21_estresse_social_telas_aplicativo,"['estresse', 'social', 'telas', 'aplicativo', 'sites', 'acessibilidade', 'provedores', 'pessoas', 'queixas', 'idosos']","['o estresse é uma resposta normal do corpo a situações desafiadoras ou exigentes, mas quando os níveis de estresse são muito altos ou duram por um longo período, podem levar a problemas de saúde mental, como ansiedade e depressão. estudos mostram que estudantes universitários experimentam altos níveis de estresse relacionados a pressão acadêmica e mudanças na vida social e financeira, entre outros fatores. neste trabalho, conduzimos uma pesquisa com os concluintes do curso de ciência da computação na universidade federal de campina grande (ufcg) durante o semestre . . a pesquisa buscou entender a relação entre fatores de estresse (saúde, social e acadêmico), o nível de estresse percebido em diferentes momentos do semestre (início, meio e fim) e seu impacto no desempenho acadêmico. os resultados mostram que os estudantes apresentam altos níveis de estresse ao longo de todo o semestre, e o principal fator de estresse que afeta o desempenho acadêmico é a pressão da família para conseguir um bom emprego. além disso, uma correlação negativa significativa foi identificada entre o estresse percebido no final do semestre e o desempenho acadêmico. contudo, o estresse no início e meio do período não mostrou relação estatisticamente significativa com o desempenho dos alunos.', 'o envelhecimento populacional é um fenômeno global que traz consigo desafios significativos relacionados ao bem-estar e à qualidade de vida dos idosos. a solidão e o isolamento social são problemas complexos que afetam a saúde mental, emocional e social dessa parcela da população. nesse contexto, o aplicativo ""hora do fuxico"" é apresentado como solução inovadora para lidar com a carência de entretenimento e interação social por parte dos idosos. o aplicativo permite que os idosos se organizem em grupos e agendem encontros presenciais ou virtuais com base em temas de seu interesse. sendo assim, esses encontros servem como ambiente onde eles podem compartilhar histórias, experiências e interesses em comum. isso promove um senso de comunidade e apoio mútuo dentro do aplicativo, contribuindo para a saúde mental dos idosos, ao combater o isolamento social e proporcionar uma experiência enriquecedora de interação social. utilizando uma abordagem metodológica que incluiu pesquisa de mercado, design centrado na usabilidade, desenvolvimento de software e testes, o aplicativo foi desenvolvido com sucesso. sua diferenciação se dá na sua abordagem centrada nas necessidades específicas da comunidade idosa, oferecendo uma experiência única de interação e entretenimento.', 'existem diversas barreiras a serem encontradas e resolvidas em sites eletrônicos que atingem principalmente as pessoas com deficiência. ao utilizar a web e seus recursos, as pessoas com deficiência ou limitações deparam-se com obstáculos que dificultam e, muitas vezes, impossibilitam o acesso a páginas. o conceito de acessibilidade digital pressupõe que os sites e portais sejam projetados de modo que todas as pessoas possam perceber, entender e interagir de maneira efetiva nos sítios eletrônicos. o governo federal do brasil tem investido em serviços e distribuição de informação pela internet e, atualmente, existe um número considerável de sites oficiais do governo federal com serviços e informações que podem ser extraídas através de ferramentas e documentos que auxiliam e orientam profissionais na construção, adequação, avaliação e correção de páginas, sites e serviços, garantindo assim o controle da navegação e o pleno acesso, independentemente das suas capacidades físico-motoras e perceptivas, culturais e sociais. partindo dessa proposta, surgiu a necessidade de verificar a acessibilidade dos sites oficiais da universidade federal de campina grande. para isso, será realizado um estudo sobre os conceitos de acessibilidade, para que possa ser fornecido sugestões de boas práticas na inclusão de ferramentas de acessibilidade, assim como recomendações para que esses sites se tornem acessíveis. as análises serão feitas a partir do levantamento de uma ferramentas existente para medição de acessibilidade, o google lighthouse - ferramenta automatizada de código aberto para medir a qualidade das páginas da web, que audita o desempenho, a acessibilidade e a otimização do mecanismo de pesquisa de páginas da web.']"
22,8,22_parlamentares_eleies_dados_posicionamento,"['parlamentares', 'eleies', 'dados', 'posicionamento', 'transparncia', 'reunies', 'sobre', 'sociedade', 'votaes', 'urnas']","['o poder legislativo no brasil é uma das três funções essenciais do estado. no entanto, há um desafio evidente em relação ao acompanhamento das discussões nos órgãos públicos por parte da população. isso se deve à extensão considerável e ao volume significativo dessas reuniões, tornando-as inacessíveis para muitos cidadãos. para enfrentar esse desafio, este estudo utilizou as notas taquigráficas do senado federal do ano de , que são transcrições dos debates parlamentares, com o objetivo de avaliar o potencial de grandes modelos de linguagem (do inglês, large language models-llms), de detectar tópicos relevantes discutidos pelos parlamentares e o posicionamento deles em relação a esses tópicos, classificando-os como a favor, neutro ou contra. foram realizados experimentos, ambos utilizando o modelo gpt- . -turbo, para as tarefas mencionadas. o primeiro experimento empregou uma técnica de compressão de dados antes de fornecer a entrada para o gpt e abrangeu reuniões de diferentes tamanhos. o segundo experimento não envolveu compressão e focou apenas em reuniões pequenas. os resultados indicam que o modelo teve um desempenho superior para reuniões pequenas. além disso, em um panorama geral para reuniões independentes de tamanho, o modelo teve um desempenho superior na tarefa de detecção de tópicos, com uma precisão média de aproximadamente %, enquanto na detecção de posicionamento teve um desempenho razoável com uma precisão média de aproximadamente %.', 'as redes sociais tornaram-se uma importante plataforma de posicionamento político dos parlamentares fora do congresso brasileiro. especialmente o twitter, que atualmente reúne perfis de % dos parlamentares. apesar dessa importância como difusor não oficial do discurso político, existem poucos estudos que analisam o posicionamento, a atuação e a influência dos parlamentares na plataforma (e.g. parlamentares e partidos mais ativos, engajamento dos parlamentares, proposições mais mencionadas, etc.). desta forma, este trabalho tem como principal objetivo analisar quantitativamente a influência e atuação dos parlamentares brasileiros no twitter, além de seu comportamento na rede. análises dessa natureza possibilitam que a população acompanhe e fiscalize o posicionamento e o comportamento de parlamentares fora do congresso sobre temas fundamentais à sociedade. além do mais, permite compreender como as redes sociais podem ser utilizadas como um elemento fundamental no processo de comunicação entre a sociedade civil e o setor político.', 'após os históricos e correntes eventos políticos envolvendo corrupção e uso inadequado do dinheiro público, a transparência na administração pública tornou-se um tema popular e ações, como vidinha de balada[ ] e brasil.io[ ], foram criadas para fiscalizar e compreender o uso do dinheiro no poder executivo e legislativo. porém, pouco tem sido feito para melhorar a transparência dos gastos no sistema judiciário brasileiro. as conhecidas lei de acesso à informação (brasil, ) e lei de responsabilidade fiscal (brasil, ) exigem que os órgãos públicos disponibilizem seus dados de gastos na internet, mas não especifica a forma como devem ser disponibilizados, levando a dados descentralizados e não estruturados. projetos recentes de recuperação e libertação de dados, como o dadosjusbr [ ], buscam disponibilizar dados estruturados sobre remunerações do sistema judiciário brasileiro. entretanto, mesmo com esse nível de organização, a compreensão da sociedade sobre os dados é limitada. por exemplo, não é trivial entender como é constituída a remuneração de um funcionário, quais benefícios são usados com mais frequência e por quais cargos, se em determinada época do ano é mais comum o uso de algum auxílio, dentre outras informações. isso ocorre devido a escassez de análises descritivas, visualizações e interpretações que mostrem para a sociedade aspectos de como esses gastos são realizados e distribuídos. desta forma, este trabalho tem como objetivo gerar análises descritivas sobre dados de remuneração do sistema judiciário e desenvolver técnicas e resultados que possibilitem uma melhor compreensão de suas características e particularidades, promovendo um maior controle social sobre essas despesas.']"
23,8,23_complexo_aplicativo_ufcg_campus,"['complexo', 'aplicativo', 'ufcg', 'campus', 'centro', 'usurio', 'cidades', 'campina grande', 'campina', 'cidade']","['o complexo esportivo da universidade federal de campina grande (ufcg) atualmente oferece opções de agendamento consideradas mais tradicionais. os usuários têm a possibilidade de fazer reservas pessoalmente na secretaria do complexo localizada no campus, ou por meio de um aplicativo de mensagens. no entanto, esses métodos são considerados pouco eficientes. para aprimorar o processo tanto para as comunidades envolvidas quanto para os administradores dos espaços, torna-se necessário o desenvolvimento de uma aplicação online. uma versão inicial dessa aplicação foi criada no início deste ano, porém ainda não estava disponível para uso e já se fez necessário adicionar novas funcionalidades. este trabalho documenta o desenvolvimento de uma progressive web app (pwa), com ênfase nas funcionalidades essenciais que serão implementadas, destacando-se: recuperação de senha do usuário, controle de reserva, atualização dos perfis, notificações por e-mail e bloqueio de usuários. essas funcionalidades são consideradas prioritárias e fundamentais para o pleno funcionamento e experiência do usuário no sistema.', 'estimular a inteligência coletiva e criativa das cidades é o grande desafio da rede mundial de cidades criativas da unesco. nesse contexto, campina grande, cidade membro da uccn na categoria artes midiáticas, desenvolveu, na unidade acadêmica de sistemas e computação da ufcg, uma versão inicial do aplicativo cidade singular, com o objetivo de ajudar a população a divulgar e fomentar as atividades, bens e serviços culturais das cidades criativas. para além do desafio de ofertar à população um meio digital de divulgar seus bens culturais, resta o desafio de engajar esta população na participação coletiva e voluntária de uma política pública de economia criativa. este trabalho tem o objetivo de criar uma experiência na qual o usuário se sinta motivado a participar ativamente no aplicativo, compartilhando e alimentando o sistema com sua própria vivência. para isso foi concebida, desenvolvida e avaliada, uma nova versão do app cidade singular usando técnicas de gamificação para aumentar a participação e o engajamento do usuário na missão de valorizar as singularidades de uma cidade criativa. uma análise qualitativa dos resultados, mostrou que, na percepção dos usuários, o impacto da gamificação no aumento do engajamento da nova versão do aplicativo foi positivo.', 'a cidade de campina grande abriga uma unidade do cer-iv, um centro especializado em tratamentos de reabilitação, é responsável por atender diversos municípios, incluindo campina grande. isto acarreta em uma enorme quantidade de pacientes visitando o centro por ano, gerando uma enorme carga de trabalho para todos os profissionais alocados no centro, como fisioterapeutas, enfermeiras, fonoaudiólogos, entre outros. após algumas visitas e entrevistas realizadas com profissionais do centro, foi proposta a criação de uma plataforma web, que funcionaria como um apoio para os profissionais cadastrarem diversas informações relacionadas ao tratamento dos pacientes, além de prover formas de enviar dúvidas ao centro por meio da plataforma. entre as informações, algumas podem ser citadas como: descrição de atividades que são realizadas com os pacientes e podem ser feitas em casa; utensílios que auxiliam a realizar o tratamento como brinquedos e móveis adaptados, por exemplo. a escolha de implementar a plataforma para web é pelo fácil acesso atualmente que as pessoas têm aos navegadores comuns em seus celulares, facilitando o acesso de todos à informações para o tratamento adequado dos pacientes. ao final, o aplicativo foi validado por meio de uma entrevista a três fisioterapeutas do cer para que fosse inicializada a utilização do aplicativo no centro de campina grande.']"
24,7,24_animais_animal_gua_pets,"['animais', 'animal', 'gua', 'pets', 'domsticos', 'abandonados', 'resgate', 'nvel gua', 'animais domsticos', 'lar']","['atualmente, é muito comum que donos de animais domésticos (pets) por vezes esqueçam de repor a água dos recipientes dos seus animais logo que esta acaba. com isto, é comum que seus pets fiquem muito tempo sem beber água, correndo o risco de até mesmo sofrer desidratação. assim, o trabalho ora descrito consiste no desenvolvimento de um sistema para monitoramento remoto do nível de água em recipientes de animais domésticos. o sistema pode ser utilizado para animais de pequeno porte (cães pequenos e gatos), até animais maiores e, de outros contextos, como por exemplo, cavalos e bois.para tanto, foi desenvolvida uma solução hardware e software, com sensor de nível de água e dispositivo da família esp32, a partir da qual, no momento em que o nível da água atinge certo ponto, uma notificação é enviada para dispositivos móveis, como smartphones. espera-se, que este sistema auxilie os donos de pets a manter o recipiente de água de seus animais sempre abastecido, evitando assim que seus pets fiquem sem água por tempo prolongado.', 'diversas indústrias brasileiras consomem diariamente recursos naturais. algumas delas possuem suas próprias regiões de reservas ambientais. nesse contexto, essas indústrias objetivam continuamente prover proteção para os seus recursos. contudo, um complicador presente na maioria desses espaços, são os animais. seus hábitos por vezes acarretam na danificação dessas áreas. nesse sentido, este artigo tem como proposta, o monitoramento da presença dos animais nos ambientes de reservas naturais de ambientes industriais. para tanto, foram utilizadas técnicas de processamento digital de imagens para identificação da presença de animais nesses ambientes. em virtude da impossibilidade momentânea de aquisição de imagens em um ambiente industrial, o trabalho apresenta uma prova de conceito, com aquisição dos dados a partir de uma base de dados pública. para identificação das imagens de animais foram utilizadas redes de aprendizagem profunda. a abordagem apresentada possibilitou a identificação de animais de diferentes portes e espécies, com acurácia relevante em comparação com outras pesquisas similares. esses resultados permitem validar a metodologia proposta, possibilitando a sua instalação em um ambiente industrial, de forma a realizar o monitoramento da presença de animais .', 'um grande problema das cidades brasileiras é o alto índice de animais abandonados, que segundo uma pesquisa da oms existem mais de milhões de cães sem lares no brasil [ ]. além desse número muito alto, há um outro problema na adoção que é adotar um animal, não se adaptar ao seu comportamento, temperamento daquele animal, então algumas pessoas acabam abandonando esses animais, novamente. existem ongs e pessoas que gostariam de fazer a diferença nesse meio. nessa plataforma, auxiliaremos de algumas formas, como: ajudar na avaliação do adotante e do pet, mostrar os animais disponíveis para adoção podendo classificar e/ou filtrar, além disso abrir um canal de comunicação entre as organizações e os adotantes. para o desenvolvimento desta plataforma, será utilizada uma biblioteca de javascript bem conhecida no mercado, o react-native [ ]. espera-se que o aplicativo sirva para disseminar a ideia da adoção de animais, além de ajudar a evitar a reintrodução desse animal ao abandono e assim aumentar o número de animais adotados.']"
25,7,25_projetos_geis_software_metodologias,"['projetos', 'geis', 'software', 'metodologias', 'desenvolvimento', 'fatores', 'metodologias geis', 'desenvolvimento software', 'projetos software', 'codex']","['a importância do atomic design no desenvolvimento de software é pouco discutida no meio acadêmico, mas é importante destacar o valor de um sistema de design que estabelece diretrizes para criar interfaces de usuário e facilita o desenvolvimento de aplicações web. o atomic design permite construir um sistema de componentes que seguem uma hierarquia bem deinida, resultando na construção de interfaces lexíveis. porém, essa lexibilidade e escalabilidade podem não ser tão simples de serem implementadas e possuem algumas limitações. diante disto, com o intuito de observar de que maneira o atomic design pode ser utilizado como uma metodologia que facilita a forma de pensamento e organização dos projetos, este trabalho relata a experiência de adaptar as diretrizes do atomic de- sign em uma aplicação conidencial de um e-commerce; trata-se de um projeto interno de gerenciamento e integração de apis de uma empresa privada. além disso, são propostas alguns critérios gerais de avaliação para realização da análise da estratégia. o objetivo geral é destacar fatores de sucesso, diiculdades e pontos negativos do modelo diante da observação de sua aplicação na prática.', '""um estudo sobre metodologias ágeis e sua correlação com o êxito em projetos de software"" investiga a eficácia das metodologias ágeis em projetos de desenvolvimento de software e sua relação com o sucesso. o estudo aprofunda a compreensão das práticas e princípios fundamentais das metodologias ágeis, como o manifesto ágil e os princípios ágeis. além disso, a pesquisa analisa como diversos fatores, como o apoio executivo, a maturidade emocional das equipes, o envolvimento dos usuários, a otimização de recursos, a qualificação dos membros da equipe e outros elementos, impactam a taxa de sucesso de projetos ágeis. também são investigados os obstáculos e desafios comuns que as organizações enfrentam ao implementar essas metodologias, incluindo a resistência à mudança e conflitos culturais. o trabalho enfatiza a relevância das metodologias ágeis na melhoria do desempenho em projetos de software e fornece diretrizes valiosas para organizações que buscam adotar ou aprimorar suas práticas ágeis. o estudo oferece uma visão abrangente do universo ágil, abordando suas vantagens e desafios, e apresenta recomendações práticas para aprimorar a implementação de abordagens ágeis nas empresas de desenvolvimento de software.', 'ao tomar decisões, gerentes de projetos de software precisam considerar diversos fatores, tornando essa uma atividade muito complexa. para auxiliar os gerentes com essas decisões, ferramentas têm sido utilizadas para simular o impacto desses fatores nos resultados do projeto. modelos em dinâmica de sistemas (sd) têm se mostrado uma boa opção para tais simulações por possuírem características dinâmicas e utilizarem sistemas de ""feedback"", aspectos próprios do desenvolvimento de projetos de software. o objetivo deste trabalho é identificar quais os fatores (antecedentes) que influenciam os projetos de software e o que eles influenciam (consequentes) nos projetos que já foram modelados em sd. para isso foi realizado um mapeamento por meio da base de indexação web of science (wos). os artigos foram avaliados por ordem de publicação partindo do mais recente e considerando os critérios de inclusão e exclusão propostos. alguns dos fatores mapeados foram: influência do cliente, promoção da equipe, pressão de cronograma, horas extras (antecedentes); produtividade do time, custo do projeto e duração do projeto (consequentes). a contribuição esperada é auxiliar pesquisadores que pretendem construir modelos sd a encontrarem fatores ainda não modelados ou reaproveitar os fatores já modelados por outros pesquisadores evitando re-trabalho.']"
26,7,26_tweets_sociais_sentimentos_comentrios,"['tweets', 'sociais', 'sentimentos', 'comentrios', 'aspectos', 'twitter', 'dessas', 'anlise', 'guerra', 'dessas plataformas']","['para a comunidade lgbti+ é muito importante ter conhecimento sobre o quanto um local é receptivo e aberto à diversidade, principalmente, devido ao fato de que o preconceito, ainda hoje, é bastante recorrente. diante desse contexto, são constantes os casos noticiados de indivíduos, que vivenciam situações constrangedoras ou que são violentados - física e moralmente - em determinados estabelecimentos. sendo assim, este trabalho visa desenvolver uma aplicação web que possibilita um ambiente virtual para avaliação de locais, de acordo com a receptividade ao público lgbti+ e de modo que permita que os colaboradores os julguem de forma objetiva (estrelas) e subjetiva (comentários). este artigo apresenta uma visão geral da arquitetura do aplicativo e, também, uma análise da interação dos seus usuários com as funcionalidades disponibilizadas. os dados coletados das avaliações subjetivas foram organizados em grupos (fora do foco do aplicativo; positivas; e negativas) e analisados, o que resultou em um total de avaliações disponíveis. nesse sentido, observou-se alguns comportamentos semelhantes nessas interações, a exemplo de: avaliações fora do contexto, contraditórias com a nota atribuída, o tipo dos locais avaliados (bar, restaurante, escola etc.) e lugares que começaram com uma avaliação positiva, mas após um certo período tiveram uma queda na média. o aplicativo inhaí está disponível desde de maio de e congrega usuários que interagem socialmente e colaboram nesse mapeamento, o que revela um impacto promissor nas trocas de experiências vivenciadas nesses lugares e demonstra potencial para expansão e ampliação de suas funcionalidades.', 'a internet tem possibilitado uma maior interação entre as pessoas ao redor do mundo, e o twitter tem desempenhado um papel significativo nisso. como uma das redes sociais mais utilizadas do planeta, o twitter permite que os usuários façam postagens expressando diversos aspectos do seu dia a dia. com a aproximação de grandes eventos de reconhecimento mundial(e.g., eventos esportivos), é natural que os usuários também expressem seus comentários sobre os diversos aspectos dessas competições. no entanto, devido à enorme quantidade de comentários gerados diariamente, é possível observar uma variedade de interpretações que refletem diferentes aspectos do evento, como discussões sobre escolha dos técnicos, desempenho das equipes e até mesmo comentários pessoais sobre os participantes. este artigo apresenta uma abordagem para analisar as discussões em torno de um evento, utilizando a copa do mundo como exemplo para validar o método. para isso, foram empregados dados coletados do twitter, os quais foram usados como entrada em técnicas de agrupamento, a fim de identificar potenciais conjuntos de comentários relacionados à competição. como resultado, foram obtidos grupos diversos, cada um com características únicas, abrangendo desde avaliações individuais, até críticas ao país-sede qatar, rivalidades entre seleções, entre outros aspectos. esses resultados indicam a existência de padrões nos comentários sobre um tema específico, sugerindo que os usuários buscam comentar temas do momento e com grande engajamento.', 'o presente artigo trata-se de uma análise de sentimentos e emoções expressos em tweets relacionados à guerra da ucrânia, mediante análise dos tópicos discutidos pelos usuários da plataforma twitter. este estudo visa compreender como os usuários reagem ao evento em curso, quais aspectos da guerra as pessoas estão discutindo na plataforma, e como se sentem a respeito deste acontecimento. além disso, visa identificar correlações entre as variáveis presentes nos tweets, como localização, informações de perfil do usuário autor da postagem, e a natureza de suas opiniões. tais análises foram conduzidas através de tarefas de processamento de linguagem natural como análises exploratórias dos dados e a aplicação de classificadores de sentimentos de tweets utilizando modelos de dados pré-treinados. os dados analisados contém tweets coletados desde o início do conflito, que se deu em fevereiro de até outubro de , e foram coletados a partir de hashtags relacionadas à guerra. para a realização das análises de sentimento e emoção foram utilizados a variante roberta. os tweets foram classificados em sentimentos como positivos, negativos ou neutros, e em emoções como alegria, raiva, medo, nojo, otimismo, pessimismo, surpresa e amor. os resultados mostraram que a maioria dos tweets em inglês expressam raiva e antecipação como emoções predominantes, e sentimentos negativos e neutros com maior predominância, atingindo mais de % do da amostragem analisada. algumas das frases mais recorrentes na análise fazem alusão ao apoio à ucrânia e pedindo o fim da guerra. da mesma forma, frases de preocupação com a crise, armas e fatalidades são recorrentes. na maioria das postagens, pessoas demonstram preocupação com o conflito armado e apoio à ucrânia. trabalhos futuros poderiam utilizar mais tweets para abranger a análise e visualizar a correlação de mais atributos relacionados às postagens como os engajamentos e curtidas.']"
27,7,27_recomendao_saps_feies_sistemas recomendao,"['recomendao', 'saps', 'feies', 'sistemas recomendao', 'nfs', 'vendas', 'dados', 'feies geogrficas', 'indicador', 'performance']","['no mundo dos negócios, a análise de indicadores para tomada de decisões é muito importante . um indicador comumente avaliado é o de volume de vendas, o qual pode ser decomposto por mercado, linha de produto, meios de distribuição etc. com base nesse indicador, pode-se compreender como as vendas da empresa se comportam e, então, decidir que ações deverão ser tomadas para aumentar os lucros ou reduzir prejuízos de uma empresa. desta forma, este estudo teve como objetivo analisar o comportamento do indicador volume de vendas de algumas empresas vendedoras de café, para identificar possíveis correlações e tendências entre mercados e estimar o volume de vendas para o ano seguinte. para isto, foram aplicadas algumas técnicas de extração e análise de dados, de correlação e de auto regressão, para predição deste indicador de vendas. na realização do trabalho, foi utilizado uma base de dados de quatro empresas vendedoras de café, clientes de uma empresa de consultoria em análise de dados.', 'a recomendação de pontos de interesse (pois) ganha destaque no contexto de sistemas de recomendação (srs), especialmente com o crescimento de redes sociais baseadas em localização, como foursquare, gowalla e yelp. a qualidade dessas recomendações é essencial para enriquecer a experiência do usuário nessas plataformas, facilitando a sociabilidade e promovendo o turismo, além de levantar uma série de desafios para a comunidade. no entanto, os sistemas tradicionais de recomendação de pois frequentemente se limitam a considerar informações como avaliações de locais, fotos, horários de acesso e check-ins, negligenciando dados geográficos relevantes, como feições geográficas que incluem rios, edifícios, ruas e lagos no contexto de um poi. essas feições podem influenciar significativamente as preferências dos usuários, uma vez que eles podem visitar um poi por gostar das feições geográficas presentes no ambiente. como exemplo, algumas pessoas preferem cafeterias próximas a lagos e áreas arborizadas em vez de rodovias movimentadas. neste estudo, propomos e avaliamos a utilização de embeddingsgegráficos que incorporam feições geográficas para aprimorar srs de pois. os resultados indicaram que o uso dos embeddings que consideram as feições aumentou a acurácia e o mrr em até . % na tarefa de recomendação do próximo poi no conjunto de dados utilizado, em comparação ao baseline, confirmando a importância das feições geográficas para melhorar srs de pois.', 'saps é um sistema que executa algoritmos para aquisição, pré-processamento e processamento de imagem de satélite. durante a execução dos algoritmos, arquivos são compartilhados entre os processos que compõem cada um dos estágios (aquisição, pré-processamento e processamento). atualmente, o saps utiliza um sistema de arquivos nfs centralizado como solução para compartilhamento de dados. o nfs é uma solução conhecida por apresentar alguns problemas, tais como implicar em ponto único de falha e não ser escalável. existem outras abordagens mais recentes que resolvem os problemas do nfs. uma dessas, o objectivefs é um sistema de arquivos em cloud que permite a escalabilidade do storage possuindo uma performance similar ao armazenamento via nfs. neste trabalho projetarei e implementarei mudanças no projeto do saps permitindo o uso de uma nova abordagem que possibilite resolver os problemas do nfs. a solução escolhida lida com o uso de uma sdk desenvolvida para lidar com a nuvem do openstack e um dos seus serviços de armazenamento, o object storage (swift). este trabalho gerou uma melhoria no design do saps, porém seu desempenho não atendeu as expectativas de ser superior, existindo um trade-off entre ambas as versões.']"
28,7,28_jogadores_jogo_basquete_nba,"['jogadores', 'jogo', 'basquete', 'nba', 'anlise', 'preditiva', 'resultados', 'caixa', 'jogos', 'dados']","['o futebol, assim como diversas outras atividades do nosso cotidiano, recebeu e vem recebendo grandes mudanças com o advento da tecnologia. a grande evolução desse esporte no século xxi foi guiada, em certa parte, pelo avanço tecnológico implementado. a análise estatística é um grande expoente desses avanços, a maioria dos grandes clubes de futebol de hoje possuem um setor que alia essa análise a parte tática do jogo, essas atividades norteiam dirigentes e treinadores em como fazer as contratações corretas para o time e quais jogadores estão com bom desempenho em campo. diante do exposto, utilizando dados extraídos de uma base chamada statsbomb, disponível no site fbref.com, este trabalho busca fazer um estudo do impacto dos jogadores a partir da análise e visualização de dados. a partir da extração desses dados, o objetivo é observar, a partir do cálculo de uma métrica, os fundamentos e estilos de jogo presentes e observados hoje no esporte moderno, como construção de jogo, criação ofensiva, finalização, capacidade defensiva, etc. a partir dessa métrica, é possível ainda fazer uma análise, levando em consideração a posição dos jogadores, o que mostra onde cada uma das características está mais concentrada em campo. a partir disso, é possível observar, por exemplo, quantos defensores possuem tal característica ofensiva ou atacantes que realizam ações defensivas. essa análise pode ser feita para várias posições e funções do futebol e ajuda a entender e demonstrar o impacto que cada um dos jogadores tem dentro de campo.', 'este artigo oferece um estudo sobre a evolução do basquete brasileiro, empregando índices estatísticos de desempenho (ied) como métrica para avaliar o desempenho das equipes no contexto das três temporadas mais recentes do novo basquete brasil (nbb), que abrangem o período de a . o objetivo central desta pesquisa é explorar e analisar o desempenho das equipes de maior destaque no cenário do basquete brasileiro. os dados utilizados nesta análise foram coletados a partir da plataforma da liga nacional de basquete (lnb), a entidade responsável pela organização e gestão do nbb. foram analisados jogos ao longo dessas três temporadas, abrangendo diversos índices estatísticos, que incluem a média e o desvio padrão de diversos parâmetros, tais como pontuação, arremessos (tentados e convertidos), aproveitamento de arremessos de dois pontos, três pontos e lances livres, assistências, rebotes, roubadas de bola, erros, bloqueios (tocos), eficiência e outros índices avançados associados ao esporte do basquete. os dados foram comparados utilizando a análise de variância (anova) e os valores de referência foram calculados a partir de percentis, permitindo uma análise comparativa e a identificação de tendências. nesse contexto, a temporada de emergiu como aquela que registrou os índices mais baixos, especialmente em termos de aproveitamento de arremessos, eficiência, média de pontos e assistências. por outro lado, as outras duas temporadas analisadas apresentaram números similares na maioria dos índices estudados, o que pode sugerir uma certa estabilidade nesses aspectos ao longo desse período. outro ponto observado no estudo é que as equipes que competem no nbb precisam atingir um alto nível de desempenho, conforme evidenciado pelos percentis utilizados como referência. além disso, o estudo revela uma clara tendência ao aumento do uso de arremessos de três pontos nos últimos anos do nbb, alinhando-se com as mudanças observadas no basquete em nível global, onde a bola de três pontos se tornou um elemento fundamental nas estratégias ofensivas.', 'modelos preditivos em aprendizado de máquina e processos de descoberta de conhecimento em bases de dados, particularmente em domínios como o basquete, são inestimáveis para obter insights sobre o desempenho dos jogadores. este estudo compara abordagens de aprendizado de máquina supervisionado (modelos de caixa preta e caixa branca, incluindo métodos de conjunto) para analisar dados estatísticos de jogadores de basquete universitário (ncaa). nosso objetivo é identificar jogadores da ncaa com alto potencial para sucesso na nba, determinar quais características dos jogadores mais influenciam as decisões de seleção e como esses modelos chegam a tais conclusões para comparar seus desempenhos e a explicabilidade associada. esta tarefa é desafiadora devido a fatores além das estatísticas, como o contexto do jogador e as considerações do elenco da equipe durante a seleção. o objetivo principal é fornecer aos tomadores de decisão insights cruciais para a seleção de jogadores, ajudar na melhor avaliação de jogadores e desenvolver jovens talentos enfatizando aspectos-chave do jogo. comparamos os resultados de modelos de predição interpretáveis com níveis satisfatórios de precisão. equilibrando interpretabilidade e precisão preditiva, empregamos métodos de classificação de caixa branca, caixa preta e de conjunto, como árvores de decisão, regressão logística, máquina de vetores de suporte, perceptron multicamadas, floresta aleatória e xgboost. além disso, algoritmos genéticos foram usados para reduzir o conjunto de características de cada modelo, retendo apenas as características mais impactantes. comparado aos procedimentos padrão sem seleção de características, todos os modelos mostraram desempenho melhorado. encontramos diferenças mínimas na precisão preditiva entre os melhores modelos de caixa branca e caixa preta. a combinação de algoritmos genéticos e regressão logística superou a precisão preditiva de outros modelos, reduzindo significativamente as características e melhorando a interpretabilidade dos resultados. a análise também destaca as características mais influentes no modelo e como os modelos chegaram a tais conclusões.']"
29,7,29_segurana_acesso_ambientes_integridade,"['segurana', 'acesso', 'ambientes', 'integridade', 'energia', 'containers', 'implantao', 'contineres', 'arquitetura', 'ambiente']","['a liteme é uma empresa de inteligência energética em ascensão que desagrega dados de consumo de energia de seus clientes via modelo nialm, distinguindo o consumo de cada um dos aparelhos registrados, e os processa para oferecer seus serviços. isso faz da desagregação um dos alicerces do negócio, e conforme a liteme se expande, dados de mais clientes precisam ser desagregados, o que leva à necessidade de replicar o desagregador nialm. atualmente o desagregador faz parte de uma arquitetura monolítica fortemente acoplada com o backend robusto da empresa, chamado de núcleo. ele realiza as operações mais custosas da plataforma, que elevam o requisito de hardware para executá-la, e atua como servidor sempre disponível. esse acoplamento prejudica a escalabilidade do nialm, que não pode ser replicado sozinho. uma arquitetura distribuída de microsserviço que permita separar o nialm do núcleo, mantendo comunicação entre os dois, pode fornecer ao desagregador melhor escalabilidade, desacoplamento, menores requisitos de hardware e de disponibilidade. este trabalho propõe uma arquitetura de desagregação distribuída para substituir a monolítica, de forma a executar em nuvem pública com uso de instâncias oportunistas (e.g. ""spots"" na aws). a arquitetura foi validada com apoio da empresa e testada em ambiente simulado. após análise, foi possível alcançar os objetivos e reduzir custos de hospedagem em nuvem em até , % em comparação à arquitetura monolítica.', 'no contexto do projeto do smartcampus, da universidade federal de campina grande (ufcg), a disponibilização de um kafka contendo informações de consumo de energia apresenta desafios complexos relacionados à segurança e controle de acesso. diversas entidades, incluindo desenvolvedores, usuários finais e operadores dos sistemas de produção, necessitam interagir com esse kafka a partir de ambientes variados, criando a necessidade de diferenciar e controlar o acesso de maneira eficaz. este trabalho explora estratégias fundamentadas no modelo zero trust, que preconiza a autenticação contínua e autorização granular, garantindo que cada acesso seja verificado e autenticado. adotando o spire em conjunto com uma série de outros microsserviços, busca-se assegurar a autenticação segura por meio de identidades spiffe, bem como configurar um serviço de autorização personalizado de acordo com o perfil do usuário. o objetivo é prevenir acesso inadequado, vazamento de dados e manipulações indevidas, visando obter um ambiente seguro e confiável para a implantação do projeto. por fim, também é desejado obter métricas para monitoramento das ferramentas utilizadas, a fim de identificar anomalias e falhas rapidamente.', 'no contexto do projeto do smartcampus, da universidade federal de campina grande (ufcg), a implementação de apis que disponibilizam acesso a informações de eficiência energética apresenta desafios relacionados à segurança e ao controle de acesso. as partes interessadas, incluindo desenvolvedores, usuários finais e operadores de produção, necessitam interagir com essas apis em ambientes variados, criando a necessidade de diferenciar e controlar o acesso de maneira eficaz. este trabalho explora estratégias fundamentadas no modelo zero trust, que preconiza a autenticação contínua e autorização granular, garantindo que cada acesso seja verificado e autenticado. adotando o spire (spiffe runtime environment) em conjunto com serviços de proxy, busca-se assegurar a autenticação segura por meio de identidades criptográficas fornecidas pelo spire, bem como implementar um serviço de autorização personalizado conforme o perfil do usuário. o objetivo é prevenir acesso inadequado, vazamento de dados e manipulações indevidas, garantindo um ambiente seguro e confiável para a implantação do projeto.']"
30,6,30_divulgao_sociedade_eventos agropecurios_alimentos,"['divulgao', 'sociedade', 'eventos agropecurios', 'alimentos', 'agropecurios', 'cidades', 'impulsionar', 'pedidos', 'eventos', 'meio']","['o mercado de restaurantes e fast foods tem experimentado um crescimento constante ao longo dos anos, representando uma parcela significativa das vendas de alimentos anuais. os serviços de distribuição de alimentos têm um impacto significativo na vida cotidiana da sociedade, proporcionando acesso rápido e eficiente à comida. com isso, foi desenvolvido um aplicativo móvel que ajuda os funcionários dos estabelecimentos a capturar os pedidos dos clientes de maneira mais atualizada e segura, gerando uma comanda digital altamente flexível, fácil de gerenciar e ler. isso contribui para aumentar a segurança, otimizar os tempos de atendimento e fornecer um serviço mais atual e inclusivo tecnologicamente para os funcionários que utilizam a ferramenta, com uma interface intuitiva e minimalista. além disso, essa ferramenta também contribui para a preservação do meio ambiente ao eliminar a necessidade de usar papel, substituindo-o por um método completamente digital, evitando o descarte excessivo de material orgânico.', 'um dos aliados do desenvolvimento sustentável nas cidades é a economia criativa, uma forma de unir tecnologia, inovação, cultura, criatividade e sustentabilidade para impulsionar a economia. a unesco criou a rede de cidades criativas justamente para incentivar esse tipo de iniciativa. as cidades que aderem à rede se comprometem a compartilhar suas melhores práticas e desenvolvem parcerias para impulsionar o setor cultural. uma das responsabilidades é a divulgação e difusão de atividades, bens e serviços culturais. diante disso, este trabalho se propõe a construir um sistema para divulgação de atrações culturais das cidades criativas. o aplicativo cidade singular foi finalizado com as funcionalidades planejadas, como a funcionalidade de mapa interativo para navegar entre os principais pontos turísticos. além do aplicativo para os visitantes, foi desenvolvido o aplicativo de gestão onde são adicionados curadores especializados e os mesmos cadastram e atualizam a base de dados das atrações turísticas.', 'este trabalho tem como objetivo promover uma solução para pequenas e médias empresas no ramo gastronômico de delivery (entregas em domicílio), com ênfase nos estabelecimentos que, por muitas vezes, contam com uma equipe reduzida e pessoas não familiarizadas com o uso de computadores. com a pandemia da covid- , o número de entregas de alimentos aumentou consideravelmente, e a maioria dessas demandas são concentradas em poucos aplicativos comerciais. no entanto, esses aplicativos apresentam alta complexidade de uso. o que prejudica os restaurantes de menor porte que não tem familiaridade com uso de computador e nem condições para compra de equipamentos necessários como impressora térmica serial que tem um custo elevado. para solucionar esse problema, foi desenvolvido uma aplicação android e backend que gerencia os pedidos, desde o fluxo do cardápio até a entrega. a aplicação possui funcionalidades essenciais, como a impressão do pedido por meio de impressora térmica bluetooth de baixo custo. dessa forma, é possível atingir um público maior que depende de pedidos impressos para a comunicação entre a cozinha e os entregadores, melhorando a organização do estabelecimento com redução do tempo de entrega e prevenção de erros humanos.']"
31,5,31_modelos_gpt_chatgpt_linguagem,"['modelos', 'gpt', 'chatgpt', 'linguagem', 'llm', 'chat', 'problemas', 'baseadas llm', 'cache', 'ensino']","['a geração de programas a partir de linguagem natural visa transformar frases ou comandos em linguagem natural em código de programação. o chatgpt é um chatbot de propósito geral baseado no modelo de linguagem gpt- desenvolvido para gerar texto como humano, e treinado em uma forma de conversação usando aprendizagem por reforço com feedback humano. nesse cenário, questionamentos a respeito da confiabilidade das respostas geradas pelo chatgpt . foram levantados. assim, neste estudo, foi realizada uma avaliação do desempenho do modelo na resolução de problemas de programação selecionados aleatoriamente de plataformas populares como leetcode e beecrowd. os problemas selecionados estão distribuídos entre os graus de complexidade fácil, intermediário e difícil. do total de problemas submetidos, o modelo de linguagem conseguiu responder corretamente problemas ao longo de tentativas, sendo deles da plataforma leetcode e da plataforma beecrowd. sendo assim, é possível concluir que o chatgpt pode ser usado para resolver uma gama de problemas, porém seu uso requer muita atenção, uma vez que nem sempre o resultado gerado estará correto.', 'estamos vivendo uma fase de inserção tecnológica de um novo sistema de recuperação de informação de forma fácil e bastante eficiente: o chatgpt. diante do surgimento dessa promissora ferramenta, há diversos casos em que o ensino/aprendizagem vem se moldando considerando as possibilidades que ela nos oferece. no ensino superior especificamente, durante os primeiros semestres, o processo de adaptação ao ensino na graduação se torna um grande desafio para muitos e é possível que o uso dessa tecnologia possa deixar esse processo cada vez mais ameno. como exemplo, temos os sistemas de tutoria inteligentes que usam algoritmos de ia para avaliar o desempenho dos alunos e fornecer feedback personalizado e adaptado em tempo real. nesta perspectiva, este trabalho tem por objetivo realizar uma revisão narrativa da literatura para identificar possibilidades e desafios no uso do chatgpt no processo de ensino e aprendizagem de áreas e disciplinas diversas durante o curso de ciência da computação e temas voltados para a programação, no geral. para isso serão definidas palavras chaves relacionadas tanto em português quanto inglês, perguntas de pesquisa que permitam catalogar os trabalhos encontrados de forma sistemática e a realização de busca por trabalhos em bases de dados bem conhecidos como google acadêmico e acm. o foco do estudo é a programação, mas, resultados mais gerais também serão considerados e relacionados para o contexto desejado. por fim, espera-se que em primeiro âmbito essa pesquisa contribua positivamente no ensino e na aprendizagem na área de programação, promovendo o uso responsável e ético da ferramenta.', 'modelos de linguagem de grande escala (llms), como o chatgpt, claude e llama , revolucionaram o processamento de linguagem natural, criando novos casos de uso para aplicações que utilizam esses modelos em seus fluxos de trabalho. no entanto, os altos custos computacionais desses modelos acarretam problemas de custo e latência, impedindo a escalabilidade de funcionalidades baseadas em llm para muitos serviços e produtos, especialmente quando dependem de modelos com melhores capacidades de raciocínio, como o gpt- ou o claude opus. além disso, muitas consultas a esses modelos são duplicadas. o cache tradicional é uma solução natural para esse problema, mas sua incapacidade de determinar se duas consultas são semanticamente equivalentes leva a baixas taxas de cache hit. neste trabalho, propomos explorar o uso de cache semântico, que considera o significado das consultas em vez de sua formulação exata, para melhorar a eficiência de aplicações baseadas em llm. realizamos um experimento usando um conjunto de dados real da alura, uma empresa brasileira de educação, em um cenário onde um aluno responde a uma pergunta e o gpt- corrige a resposta. os resultados mostraram que , % das solicitações feitas ao llm poderiam ter sido atendidas a partir do cache usando um limiar de similaridade de . , com uma melhoria de - vezes na latência. esses resultados demonstram o potencial do cache semântico para melhorar a eficiência de funcionalidades baseadas em llm, reduzindo custos e latência enquanto mantém os benefícios de modelos avançados de linguagem como o gpt- . essa abordagem poderia possibilitar a escalabilidade de funcionalidades baseadas em llm para uma gama mais ampla de aplicações, avançando na adoção desses modelos poderosos em diversos domínios.']"
32,5,32_nuvem_servio_ambiente_tempo resposta,"['nuvem', 'servio', 'ambiente', 'tempo resposta', 'desenvolvimento', 'ambiente desenvolvimento', 'resposta', 'custos', 'tempo', 'provedores']","['o uso de computação oportunista na nuvem é amplamente adotado devido a sua capacidade de hospedar aplicações de processamento em lote a um custo reduzido. no entanto, esse modelo apresenta a desvantagem de não garantir disponibilidade, o que pode afetar a qualidade de serviço das aplicações. além disso, devido à alta oscilação de preço, alcançar uma disponibilidade ideal com esse tipo de serviço requer despriorizar o potencial econômico oferecido. este trabalho propõe uma solução na forma de um escalonador oportunista modular para kubernetes, que busca equilibrar os custos dos nós oportunistas e os atrasos no processamento, garantindo assim a qualidade de serviço. com base nos preços de da aws, a solução demonstrou uma economia significativa, de até % em determinados dias, quando comparado aos custos com instâncias sob demanda, possibilitando o processamento em tempo quase real.', 'no processo de desenvolvimento de software, a aquisição e manutenção de hardware adequado para as necessidades de programação podem resultar em altos custos de investimento de capital. a alternativa de uso de recursos em nuvem oferece flexibilidade, porém o gerenciamento desses recursos pode ser complexo e oneroso, requerendo conhecimentos especializados em operações em nuvem. o problema consiste em gerenciar um ambiente de desenvolvimento na nuvem de forma eficiente, evitando altos custos de aquisição e manutenção de hardware próprio, além de simplificar o gerenciamento de recursos ao alugar máquinas na nuvem, buscando minimizar despesas e eliminar a necessidade de expertise complexa em operações em nuvem. propomos o desenvolvimento de uma ferramenta de linha de comando, destinada a simplificar o gerenciamento do ambiente de desenvolvimento de software. essa ferramenta terá a capacidade de criar, configurar e gerenciar recursos na nuvem de forma automatizada e eficiente. uma característica diferencial é a utilização de instâncias preemptivas oferecidas por provedores de nuvem, permitindo aproveitar recursos ociosos a custos ainda mais baixos, sem comprometer a qualidade do ambiente de desenvolvimento. espera-se que o usuário seja capaz de criar ambientes de desenvolvimento utilizando a ferramenta proposta integrando-a com outras soluções já existentes para desenvolvimento de código. ao oferecer uma solução intuitiva, nossa abordagem visa otimizar o ambiente de desenvolvimento, maximizando a economia e eliminando a necessidade de conhecimentos avançados em operações em nuvem por parte da equipe de desenvolvimento. ao final deste trabalho, a usabilidade da ferramenta foi validada e demonstrou ser eficaz na simplificação do gerenciamento dos ambientes. a maioria dos participantes conseguiu gerenciar ambientes com sucesso, destacando a facilidade de uso e a utilidade da documentação fornecida.', 'a hospedagem de uma aplicação em um ambiente centralizado pode oferecer vantagens em termos de simplificação de arquitetura, baixo tempo de resposta em condições normais, controle da infraestrutura, mitigação de complexidades operacionais e gerenciamento, porém quando se trata de um ambiente de produção, tal solução pode não ser viável a médio e longo prazo por ser um ambiente limitado, instável, com poucas janelas de otimização e expansão do serviço. uma das soluções possíveis de evolução em termos de infraestrutura é a distribuição do sistema em cloud pública, no caso desse estudo em nós aws, com microsserviços orquestrados com kubernetes, de forma a aproveitar as garantias operacionais de ambas as plataformas para mitigar boa parte do leque de falhas que um ambiente centralizado oferece, a custo de relativa perda de desempenho em relação a tempo de resposta. esse estudo de caso tem como objetivo elencar os principais problemas que existem hoje na implementação do sênior saúde móvel, uma plataforma de prontuário digital arquitetada em microsserviços e hospedado de forma centralizada no data center do laboratório nutes-uepb. ao final pretende-se adaptar e implementar a mesma aplicação em infraestrutura distribuída, estudar o tradeoff relativo ao tempo de resposta, e por fim sumarizar soluções, melhorias e trabalhos futuros referentes à avaliação dos resultados observados. podemos concluir que apesar da diferença de desempenho de tempo de resposta, as garantias providas pela solução distribuída, o potencial de expansão, manutenção do sistema, escalabilidade e o nível aceitável de tempo de resposta com prospectos de otimização justificam positivamente a adoção da adaptação para um futuro ambiente de produção.']"
33,5,33_logs_observabilidade_aplicaes_opensearch,"['logs', 'observabilidade', 'aplicaes', 'opensearch', 'recursos', 'depurao', 'replayer logs', 'mariadb', 'replayer', 'influxdb']","['vem se tornando cada vez mais comum a utilização de técnicas de observação de aplicações. observar uma aplicação gera dados importantes sobre o seu funcionamento e da infraestrutura onde ela está inserida. analisar o comportamento de aplicações é um elemento chave que permite entender e provisionar recursos computacionais, otimizando o uso da infraestrutura em que se executam as aplicações. embora haja o reconhecimento comportamental das aplicações em relação ao uso de recursos computacionais a partir de decisões humanas, a detecção de comportamentos de alto e baixo consumo de memória, por exemplo, através de modelos preditivos ainda não é muito comum, o que abre oportunidades de estudos nesta área. o presente trabalho se propõe a detectar comportamentos de uma aplicação a partir de diferentes algoritmos de agrupamento. os resultados mostram que é possível detectar cada comportamento para facilitar a compreensão e alocação eficiente de recursos de computação.', 'os benchmarks são essenciais à investigação científica, uma vez que proporcionam uma forma fiável de comparar abordagens inovadoras com o padrão académico. especificamente, benchmarks são amplamente utilizados em java para avaliar novas versões da jvm e dos coletores de lixo (cl). à medida que novas cargas de teste e cls chegam à indústria, é fundamental expandir a nossa compreensão da gestão dinâmica de memória, estudando como funcionam essas novas estratégias. este trabalho estuda o desempenho dos coletores de lixo modernos e estabelecidos na indústria utilizando hyperalloc, uma carga de trabalho do heapothesys benchmark da amazon que prevê com precisão o comportamento de alocação de memória e facilita as comparações entre algoritmos de cl. a análise fornecida neste documento serve como guia sobre a adequação da heapothesys para avaliar os cls modernos e fornece informações sobre os seus trade-offs de desempenho.', 'a observabilidade desempenha um papel importante no desenvolvimento e na manutenção de software. podemos dizer que um sistema é observável quando pode-se entender e explicar qualquer estado em que o mesmo possa entrar, podendo ele ser corriqueiro ou algo totalmente novo. juntamente com métricas e traces, os logs representam um dos pilares da observabilidade, desempenhando um papel vital na depuração dos estados de um sistema. isso ressalta sua importância como fonte de dados e a necessidade de seu tratamento e armazenamento. nesse contexto, o opentelemetry emerge como um framework e conjunto de ferramentas que se propõe a facilitar a coleta e a gestão de dados de observabilidade em sistemas. sendo independente de fornecedores e ferramentas, e adotando um modelo de código aberto, o opentelemetry se revela um software altamente versátil, adaptável às necessidades individuais de seus usuários, tornando-se uma escolha ideal na implementação de observabilidade em sistemas. o foco deste trabalho está no aprimoramento de um módulo utilizado em um coletor opentelemetry, cuja função principal é receber logs em uma plataforma de comércio eletrônico. esse módulo compreende dois componentes: o wal, responsável por detectar falhas no envio de logs ao opensearch e armazenar logs não enviados em um serviço de armazenamento de objetos; e um replayer de logs, que tenta reenviar os logs armazenados posteriormente ao opensearch. entretanto, o replayer de logs enfrenta desafios relacionados a disponibilidade de recursos de hardware, instabilidade em ambientes variáveis e limitações na configuração, o que impacta negativamente em sua eficácia no envio de logs ao opensearch. além disso, a ausência de dados sobre a saúde e o desempenho do wal pode dificultar a manutenção e depuração deste componente, devido à falta de informações relevantes. diante desse cenário, este trabalho tem como objetivo aprimorar o replayer de logs, visando melhorar a disponibilidade e a utilização dos recursos de hardware, aumentar sua confiabilidade no envio de logs ao opensearch e torná-lo mais flexível em termos de configuração. também, pretende-se adicionar capacidades de observabilidade ao mecanismo wal com o objetivo de garantir maior visibilidade do funcionamento do mecanismo e facilitar a depuração do mesmo.']"
34,5,34_matrculas_disciplinas_acadmico_perodo,"['matrculas', 'disciplinas', 'acadmico', 'perodo', 'graduao', 'regulamento', 'horrios', 'sistema', 'universidade', 'alunos']","['o regulamento de ensino de graduação é um dispositivo legal da universidade federal de campina grande que congrega as principais regras que devem ser observadas pela comunidade universitária, no âmbito da graduação. esse regulamento dispõe sobre temas como: a organização de cursos de graduação e seus componentes curriculares; estágio; formas de ingresso e seleção de estudantes; os procedimentos acadêmicos referentes ao vínculo dos discentes; a mobilidade acadêmica; o aproveitamento de estudos; o quadro de horários; as avaliações de aprendizagem; os documentos acadêmicos, etc. no entanto, nem todos os alunos conhecem esse regulamento, ou alguns apresentam dificuldade para encontrar um tópico específico, por se tratar de um documento extenso. além disso, alunos portadores de deficiência, a exemplo de deficiência visual, também encontram dificuldades no uso do regulamento. pensando nessa problemática, o trabalho consistiu em desenvolver um chatbot, denominado regbot, que possibilitará ao estudante o acesso às informações sobre o regulamento de ensino de graduação da ufcg, de forma mais fácil, ágil e dinâmica, a partir do uso de uma linguagem mais acessível. em seguida, o chatbot foi avaliado por uma amostra da comunidade acadêmica, a partir de um questionário de avaliação, de forma a concretizar o objetivo do trabalho.', 'os dias que antecedem a semana de matrículas da universidade federal de campina grande é o período em que os alunos dedicam parte do seu tempo organizando seus horários e planejando quais disciplinas pretendem cursar no próximo semestre. nesse período, as coordenações disponibilizam listas, através do sistema de “controle acadêmico” da universidade, com os dados de professores e horários para as disciplinas daquele semestre. esse trabalho tem o intuito de auxiliar o planejamento dos horários para o período de matrículas dos cursos de graduação na universidade federal de campina grande, com o desenvolvimento de um sistema web que permite o aluno organizar e planejar suas disciplinas através de uma interface agradável e que proporcione uma melhor experiência para a matrícula. para verificar a satisfação dos usuários quanto a usabilidade do sistema foi realizado um levantamento, utilizando o computer system usability questionnaire, que analisando as médias das respostas, foi obtido, na maioria dos itens, valores entre e , sendo o valor máximo, que apontam bons indicadores e alto nível de satisfação.', 'ao longo de todo o regime acadêmico é necessário que os discentes façam diversas matrículas, a fim de efetuar a inscrição em disciplinas do período acadêmico vigente. nesse contexto, principalmente com o grande volume de matrículas realizadas, se faz necessário o melhoramento da organização, do gerenciamento, do controle e do acompanhamento das matrículas acadêmicas. atualmente, mesmo diante de uma crescente modernização sistemática, o sistema de matrículas universitárias da universidade federal de campina grande é muito dependente do coordenador acadêmico, que deve manualmente: inscrever turmas, modificar quantidades de vagas e efetuar todo o planejamento de turmas manualmente. além disso, o sistema não avisa aos estudantes sobre turmas ideais para suas matrículas e nem sobre o horário inicial da abertura do período de inscrição das disciplinas. nesse viés, o eureca dashboard, surge como uma ferramenta facilitadora, na qual o coordenador acadêmico pode visualizar a oferta de vagas ideal para as disciplinas, modificar a disponibilização das disciplinas e acompanhar a inscrição dos discentes, tudo isso de forma unificada e centralizada, com um acesso facilitado e seguro. com isso, o trabalho desses gerenciadores será facilitado, e através de uma entrevista, com um usuário alvo, será possível medir o real impacto da plataforma e se ela foi fundamental no gerenciamento de tempo desses profissionais.']"
35,5,35_voz_treinamento_autenticao_locutor,"['voz', 'treinamento', 'autenticao', 'locutor', 'vocal', 'vozes', 'tcnicas', 'caractersticas', 'dados', 'identidade vocal']","['a área de aprendizagem de máquina é uma grande aliada para garantir privacidade e segurança, pois promove avanços nos métodos empregados para controle de acesso. o uso de técnicas para reconhecimento automático da identidade vocal de locutores, para fins de autenticação, representa um desses avanços. diante do exposto, este artigo objetiva apresentar um sistema para verificação automática da identidade vocal de locutores, buscando aplicá-lo para autenticação e liberação de acesso a ambiente restrito. o sistema baseia-se numa tarefa de reconhecimento de padrões, dividida em duas etapas: treinamento e verificação. no treinamento, foram aplicadas técnicas para pré-processamento do sinal (pré-ênfase, divisão em frames e janelamento), extração de características (mel-frequency cepstral coefficients - mfcc) e construção de um padrão representativo da identidade vocal de cada locutor (clusterização). na verificação, ocorreram o pré-processamento do sinal, extração de características e autenticação, esta última a partir da comparação entre as características de teste e o padrão previamente armazenado do locutor. na lógica de decisão, foram utilizados limiares para autenticação de um locutor (aceitação, rejeição e indeterminação). os resultados obtidos demonstram uma autenticação correta do locutor em % dos casos e uma taxa de , % de rejeição de impostores, comprovando a eficiência da abordagem proposta.', 'a escuta da voz é uma forma de avaliação da saúde vocal, em que um profissional julga a voz do paciente como patológica ou não após ouvi-la. o problema desse método é o seu caráter subjetivo, devido à possibilidade do resultado variar conforme examinador. para uma análise mais precisa, técnicas laboratoriais podem ser aplicadas; contudo, são frequentemente evitadas pelos pacientes devido ao caráter invasivo e oneroso. assim, pesquisadores têm desenvolvido técnicas para auxiliar na discriminação de vozes patológicas usando análise acústica, por ser uma forma de processamento digital de sinais não invasiva e automática. esse método consiste em utilizar técnicas de processamento digital de sinais e reconhecimento de padrões, para determinar se o sinal de voz é patológico ou não. diante disso, este artigo objetiva analisar o uso de uma rede neural artificial (rna) como classificador e características obtidas por meio de coeficientes mel cepstrais (mfcc), que auxiliarão na detecção de patologias da voz. para o treinamento e validação da rna, foi utilizada a base de dados alemã saarbruecken voice database (svd). os resultados demonstraram, com validação cruzada k-fold para treinamento e teste, que a solução atingiu níveis de acurácia acima de % na distinção entre vozes saudáveis e patológicas.', 'para o treinamento de modelos transcritores que produzam resultados robustos, são necessários dados rotulados em grande quantidade e diversificados. encontrar tais dados com as características necessárias é uma tarefa difícil, principalmente em idiomas menos populares do que o inglês. além disso, produzir tais dados requer bastante esforço, tempo e, quase sempre, dinheiro. logo, uma estratégia para mitigar esse problema é a utilização de técnicas de aumento de dados. nesse trabalho, foi investigada a utilização de deepfake audio para o aumento de dados, utilizando um clonador de voz capaz de gerar novos áudios mantendo características da voz do falante original, como, por exemplo, o sotaque. para tanto, foi selecionado um pequeno conjunto de dados produzido por indianos no idioma inglês, garantindo a presença de apenas um sotaque no conjunto. para a realização das investigações, experimentos foram conduzidos utilizando o clonador para o aumento de dados. em seguida, os dados aumentados foram utilizados no treinamento dos transcritores, em diversos cenários. surpreendentemente, a estratégia não teve um impacto positivo após a realização dos treinamentos, tendo como possível causa a qualidade dos áudios gerados pelos clonadores atuais.']"
36,5,36_domsticas_pomodoro_automao_assistentes,"['domsticas', 'pomodoro', 'automao', 'assistentes', 'procedimentos', 'implementao', 'produtividade', 'modelos', 'objetos', 'pomosync']","['nos últimos anos, a automação do trabalho e de atividades domésticas se tornou algo indispensável para os seres humanos, que buscam formas de otimizar tempo e se livrar de tarefas repetitivas e indesejadas. as assistentes pessoais chegaram para facilitar a vida desses usuários, sendo dirigidas por comandos de voz, elas podem juntamente com outros dispositivos, transformar uma casa comum em uma casa inteligente. entretanto, as assistentes virtuais ainda possuem falhas de segurança e privacidade que podem tornar o seu uso perigoso, colocando em risco os dados dos consumidores. este trabalho explora as principais vulnerabilidades presentes nas assistentes virtuais e como os dados dos usuários que utilizam essa tecnologia podem ser expostos, assim como investiga qual a percepção das pessoas quanto ao vazamento de suas informações.', 'dispositivos eletrônicos que auxiliam em tarefas domésticas vem ganhando popularidade nos últimos anos e ajudam as pessoas a ganhar tempo nas suas rotinas. indivíduos com limitações físicas necessitam ainda mais de suporte nas suas atividades cotidianas. contudo, não existem muitas soluções na atualidade que consigam desempenhar tarefas normalmente executadas por humanos, devido a limitações de hardware, por exemplo. uma habilidade importante é o reconhecimento de objetos em uma cena visual. dessa forma, esta pesquisa tem por objetivo avaliar o desempenho de alguns modelos baseados em perceptron multicamada (mlp) em classificar imagens de objetos comumente encontrados em ambientes domésticos, para verificar a eficácia dessas soluções nesse contexto de aplicação. foram conduzidos experimentos de classificação com os modelos, observando as métricas obtidas, como acurácia, tempos de treinamento e teste, para qualificar o desempenho. a análise dos modelos confirmou a capacidade de classificar os objetos com uma boa taxa de acerto. os resultados obtidos indicam que é possível aplicar mlps em soluções de auxílio a atividades domésticas, reduzindo o custo computacional de implementação em relação a modelos mais complexos.', 'em ambientes de trabalho e estudo cada vez mais digitais e colaborativos, manter a produtividade individual e de equipe pode ser um desafio significativo. a técnica pomodoro, desenvolvida nos anos , é uma metodologia de gerenciamento de tempo amplamente usada para aumentar a produtividade, mas sua implementação em um contexto colaborativo pode ser difícil. portanto, a necessidade de uma solução digital eficaz para facilitar a implementação colaborativa da técnica pomodoro se torna clara. o pomosync surge como uma aplicação web que visa abordar esse problema, permitindo aos usuários organizar e participar de sessões pomodoro de forma colaborativa. o objetivo deste trabalho é detalhar o processo de desenvolvimento desta aplicação, analisando seus aspectos técnicos, metodológicos e seu impacto no aumento da produtividade em ambientes colaborativos. os resultados obtidos sugerem um bom grau de satisfação do usuário com a usabilidade e eficácia do pomosync, validando a necessidade de tal solução e, a partir das críticas feitas ao sistema, são sugeridas futuras melhorias para a aplicação.']"
37,5,37_jogos_jogo_wfc_pcg,"['jogos', 'jogo', 'wfc', 'pcg', 'entretenimento', 'gerao', 'tcnica', 'opengl', 'animaes', 'consumo memria']","['desenhos animados estão entre as mídias de entretenimento mais consumidas no mundo. um dos principais motivos é o fato desse formato não ter tantas limitações físicas quanto o cinema tradicional, o que permite uma maior liberdade nos temas abordados e na construção das cenas. uma das animações mais populares dos últimos tempos, adventure time (exibido originalmente pelo canal cartoon network), é um exemplo perfeito disso. ao longo de seus quase trezentos episódios, o desenho infantil aborda uma variedade de temas e adota vários estilos narrativos. no entanto, ao se analisar o enredo como um todo, essa quantidade grande de episódios pode tornar árdua a tarefa de acompanhar a complexidade emocional dos numerosos personagens, o que pode ser um empecilho ao se tentar compreender a profundidade da trama. com o auxílio de modelos transformers e da plataforma hugging face, este trabalho investiga uma abordagem de detecção de emoções nas transcrições de falas de personagens da obra. os resultados demonstram que a abordagem é viável para este tipo de análise e identificam a vasta gama emocional neste desenho animado.', 'o algoritmo wave function collapse (wfc) desempenha um papel de destaque na indústria de jogos, sendo comumente utilizado para a geração automática de texturas e mapas. tal utilização faz parte da área de geração procedural de conteúdo (procedural content generation - pcg), a qual tem como motivações aprimorar a rejogabilidade e aliviar a carga de trabalho dos desenvolvedores na criação manual de conteúdo. o uso de pcg tem experimentado um notável aumento nos últimos anos, impulsionado pelo constante crescimento no tamanho e na complexidade dos jogos produzidos. embora diversas técnicas de pcg já tenham sido amplamente documentadas e testadas, o wfc é uma abordagem relativamente recente que carece de avaliações abrangentes quanto à sua eficácia em comparação com outras técnicas. este trabalho de conclusão de curso tem como objetivo preencher essa lacuna, realizando uma análise da complexidade do algoritmo wfc em termos de tempo e recursos de hardware, comparando-o com alternativas no contexto da geração de mapas de jogos. foram executados algoritmos de pcg em mapas de diversos tamanhos, variando de 100px a 1000px. durante as execuções, foram registrados os dados de consumo de memória e tempo de execução. os resultados demonstraram que, entre os algoritmos avaliados, o wfc se destaca no quesito consumo de memória, superando os demais algoritmos nesse aspecto. já o binary space partitioning room placement (bsprp) demonstrou ser o mais eficiente em termos de tempo, superando significativamente o desempenho do wfc. por fim, o random room placement (rrp) se mostrou o menos eficiente, tanto em consumo de memória quanto em tempo de execução.', 'a experiência do jogador é um importante fator no desenvolvimento de jogos e precisa ser instigada sempre. cria atividades diversificadas no jogo que reforcem esse fator é fundamental para agregar valor e coerência ao jogo, assim como fomentar que jogadores voltem a experimentá-lo. no entanto, a geração manual deste tipo de conteúdo é custosa e pode resultar em divergência da experiência projetada. dessa forma, utilizar eventos dinâmicos se torna uma estratégia interessante para permitir uma diversidade coerente, pois são atividades em constante alteração pela interação com os sistemas do jogo. neste contexto, a técnica de design sistêmico aplicada a jogos se mostra uma abordagem promissora, pois utiliza a interação entre os próprios sistemas do jogo para criar a experiência do jogador. assim, este trabalho propõe um modelo para a geração de eventos dinâmicos utilizando o design sistêmico como fundamento. para isso, a abstração de um jogo digital foi criada para demonstrar tal modelo. a abstração é dividida em sistemas simples, que utilizam a interação de suas propriedades e atributos para criar atividades variadas e coerentes. o modelo foi testado e resultados mostram que quanto mais eventos são gerados, menor é a taxa de dissimilaridade entre eles.']"
38,5,38_carreira_software_desenvolvimento software_equipes,"['carreira', 'software', 'desenvolvimento software', 'equipes', 'desenvolvimento', 'community smells', 'turnover', 'community', 'empresas', 'abandono']","['o senso de pertencimento em equipes de engenharia de software ainda é um conceito pouco estudado quando se trata de aspectos humanos relacionados à área. porém, algumas pesquisas recentes já identificam esse aspecto no que diz respeito à eficácia do trabalho em equipe, liderança e produtividade. nelas, o sentimento de pertencer a uma equipe ou projeto, e até mesmo à organização em que trabalham, lhes permitem ter bons resultados em seus produtos ou serviços desenvolvidos. diante disso, este trabalho realizou um estudo de caso com uma equipe de desenvolvimento de software que trabalha de forma remota, com o objetivo de identificar qual a perspectiva do líder e dos desenvolvedores (não-líderes) de uma empresa privada. por meio de entrevistas e observações, os dados foram analisados qualitativamente através de análise temática. os resultados evidenciam que, para os desenvolvedores, a liderança tem um papel fundamental na contribuição desse sentimento. para o líder, à medida que esse valoriza e incentiva o trabalho em equipe, e mantém ações de melhorias, o senso de pertencimento é fortalecido, trazendo produtividade, engajamento e melhores resultados em seus serviços.', 'fenômenos como abandono (turnaway) e transição de carreira (turnover) estão cada vez mais frequentes em empresas de desenvolvimento de software, um dos principais motivos pode estar relacionado com a falta de identificação com a área. investigar esses fenômenos no âmbito comportamental por meio das âncoras de carreira do indivíduo pode ser primordial para que as empresas melhorem seus planos de carreira, política empresarial e política de retenção. este estudo visa investigar a relação entre o abandono de carreira (turnaway) com as âncoras de carreira de ex-desenvolvedores. para isso foi aplicado o teste âncora de carreira entre ex-desenvolvedores de diferentes regiões do brasil por meio de survey a fim de identificar dados sobre o processo de desenvolvimento de software a partir de entrevistas semi-estruturadas realizadas, transcritas e codificadas por outros pesquisadores da área. com base nos dados obtidos, esperamos que esses resultados contribuam para conceber estratégias eficazes para as empresas reterem seus colaboradores, além disso, minimizar o custo social de abandono de carreira daqueles desenvolvedores e incentivar pesquisas futuras.', 'community smells são sintomas de problemas organizacionais e sociais em equipes de software que frequentemente aumentam os custos do projeto e afetam a qualidade do software. pesquisas recentes identificaram vários community smells e os definiram como padrões abaixo do ideal relacionados à estrutura social organizacional em equipes de desenvolvimento de software, como falta de comunicação, coordenação e colaboração. por esse motivo, esse estudo tem como objetivo realizar um mapeamento sistemático a partir de artigos científicos com a finalidade de compreender como o tema está sendo tratado. em vista disso, foram selecionados artigos que abordam a temática em diversas bases de dados existentes e elaborado critérios de classificação com o objetivo de responder às indagações feitas e avaliando os resultados, este artigo concluiu que estudar os community smells no nível estrutural e organizacional pode ser vital para livrar as equipes de software para evitar falhas organizacionais críticas que podem no futuro acarretar em custos adicionais consideráveis.']"
39,5,39_software_desenvolvimento_modelos_tdd,"['software', 'desenvolvimento', 'modelos', 'tdd', 'modelos grficos', 'profissionais', 'prtica', 'gamificao', 'grficos', 'estudos']","['os modelos gráficos das decisões arquitetônicas, de processo e de código na indústria do desenvolvimento de software se tornaram ferramentas relevantes ao longo do tempo. ultimamente é sentida uma queda na utilização desses modelos tradicionais como uml ou modelo c4. neste artigo queremos entender como se dá a utilização de tais modelos gráficos, quem são as pessoas que realmente utilizam os modelos, a motivação por trás da utilização, além de mostrar os benefícios, desvantagem, e relevância. iremos avaliar dados coletados com pessoas da comunidade do desenvolvimento de software para entender como se dá todo esse fenômeno envolto dos modelos gráficos. espera-se que ao final deste artigo sejamos capazes de ter um pequeno panorama atual do uso desses modelos na indústria, e avaliar se realmente esses modelos tradicionais estão declinando.', 'este estudo investiga o impacto de ferramentas baseadas em ia generativa no mercado de trabalho para desenvolvedores de software, a partir de um questionário distribuído entre profissionais do setor, com diferentes níveis de experiência. recentemente, tem-se observado o surgimento de modelos de linguagem de larga escala (llm) cada vez mais poderosos, habilitando o desenvolvimento de ferramentas com grande capacidade de geração e análise automática de código em diferentes linguagens de programação. com o rápido desenvolvimento da área, cresce também a preocupação dos profissionais acerca das implicações decorrentes da automação das tarefas que realizam, assim como da conseqüente redução dos postos de trabalho no setor de desenvolvimento de software. embora a literatura apresente uma discussão substancial sobre a temática, observa-se uma escassez de estudos baseados na opinião dos profissionais do setor. as percepções obtidas a partir do nosso estudo sugerem que essas tecnologias tendem a ser mais eficazes em tarefas simples e repetitivas, especialmente para profissionais iniciantes. por outro lado, sua efetividade parece diminuir em trabalhos mais subjetivos e complexos, especialmente à medida que a senioridade profissional aumenta. os resultados observados visam subsidiar uma reflexão mais aprofundada sobre o impacto dessas ferramentas no mercado de trabalho, especialmente em relação à oferta de vagas para profissionais iniciantes.', 'o test driven development (tdd) é uma prática considerada por muitos de grande importância no desenvolvimento de um software, deixando claro as regras de negócio antes mesmo de se iniciar o desenvolvimento e ajudando na construção de um código limpo e de melhor qualidade. porém, mesmo com todos os seus benefícios, muitos desenvolvedores preferem não utilizar esta metodologia nos seus projetos. neste estudo irei analisar o atual cenário da utilização do tdd nos projetos de software e quais são os principais motivos que levam os desenvolvedores a não utilizarem a prática nos seus projetos. o estudo foi realizado por meio de um questionário com programadores de empresas, que utilizam ou já utilizaram o tdd na prática ou que possuem pelo menos conhecimento teórico sobre o assunto. os resultados mostraram uma baixa aceitação do tdd por parte dos participantes e foi observado que o acréscimo de tempo no desenvolvimento é o principal motivador que leva os desenvolvedores a desistirem de empregar a prática.']"
40,5,40_monitoramento_microsservios_frontends_micro frontends,"['monitoramento', 'microsservios', 'frontends', 'micro frontends', 'micro', 'shm', 'arquitetura microsservios', 'arquitetura', 'soluo', 'distribuda']","['visto que houve uma crescente popularização na arquitetura de microsserviços, que são majoritariamente implementados no backend, observou-se a possibilidade da sua implementação no frontend, concebendo-se o termo micro frontends. apesar de micro frontends ser um termo conhecido, sua proposta ainda está sendo difundida entre a comunidade e o mercado. sendo assim, ainda não existem ferramentas de fácil utilização que auxiliem os desenvolvedores a implementarem a infraestrutura de micro frontends de forma otimizada. para solucionar esse problema foi criado uma ferramenta de cli que busca auxiliar os desenvolvedores a criarem e manterem seus micro frontends de forma interativa em seus terminais, criando componentes compartilhados de forma automática, e permitindo que as configurações sejam criadas sem que haja uma pesquisa aprofundada ou um conhecimento técnico específico sobre o tema.', 'o monitoramento estrutural (structural health monitoring - shm) é feito com o objetivo de manter a confiabilidade de segurança da construção durante sua utilização, e identificar antecipadamente possíveis problemas que danifiquem as estruturas. de maneira geral, um shm dinâmico convencional é composto por acelerômetros, sistemas de aquisição de sinais e softwares de visualização. entretanto, essas soluções possuem um alto custo, sobretudo no que diz respeito ao hardware que compõe um shm, o que dificulta a popularização desse tipo de monitoramento. neste trabalho, foi desenvolvida uma solução hardware-software para monitoramento estrutural, cujos dados de aceleração são coletados a partir de um acelerômetro, processados em uma placa nodemcu, transmitidos via wireless, para um servidor que exibe em um dashboard os dados do monitoramento. a solução foi avaliada a partir do funcionamento adequado dos módulos de aquisição, processamento, transmissão e exibição. o sistema foi apresentado a professores de engenharia civil da universidade federal de campina grande, os quais relataram o alto potencial da solução para utilização em projetos com edificações reais.', 'a utilização de contêineres tem sido amplamente adotada na indústria de tecnologia devido à sua flexibilidade e escalabilidade. no entanto, o monitoramento dos contêineres é uma tarefa crucial para garantir a disponibilidade e o desempenho do sistema. por isso, há uma ampla variedade de ferramentas de monitoramento disponíveis, para os mais variados casos de uso e escopos. neste estudo, serão comparadas duas dessas ferramentas, o prometheus e o netdata, ambas sendo apontadas pela cloud native computing foundation (cncf) como sendo dois dos projetos com mais contribuidores de [ ]. os critérios de avaliação serão cinco: monitoramento em tempo real, indicador base de performance, monitoramento de perfomance de rede, visualização de dados e alerta. ao final, concluiu-se que o netdata obtém leve vantagem sobre o prometheus por este ser mais limitado em aspectos como monitoramento de performance de rede e visualização de dados.']"
41,5,41_scrum_empresas_desenvolvimento_qualidade,"['scrum', 'empresas', 'desenvolvimento', 'qualidade', 'sociedade', 'negcio', 'empreendedor', 'soluo', 'projetos', 'requisitos']","['práticas relacionadas à qualidade de software são bem vistas, principalmente por grandes empresas, pois agregam valor ao produto final. entretanto, o cenário em pequenas empresas e startups tende a ser diferente. empresas menores ou recém criadas, em muitos casos, precisam lançar seus produtos rapidamente e em tempo hábil para se estabelecer no mercado. do outro lado da balança observamos que programas com níveis de qualidade superior conseguem mais espaço entre os consumidores, logo é necessário existir um equilíbrio entre agilidade e qualidade. neste trabalho conduzirei estudo de caso sobre a implantação dessas práticas em uma dessas empresas de pequeno porte, inserindo padrões comuns da área de qualidade ao processo de desenvolvimento da empresa buscando aumentar a confiabilidade da solução desenvolvida e diminuir as taxas de incidentes que são reportadas pelos clientes da mesma.', 'softwares de gestão são meios altamente eficazes de administrar um ambiente. na sociedade moderna, em que as coisas ocorrem em alta velocidade e frequência, não é incomum deixar de anotar algo importante por falta de conhecimento da situação e, se tratando de cidades, mão de obra fiscalizadora. em uma de suas falas ao público, o prefeito de campina grande, bruno cunha lima, mencionou não haver registro na prefeitura sobre ruas calçadas, asfaltadas ou qualquer registro relacionado a isso. este trabalho de conclusão de curso propõe um software para web que irá facilitar a gestão pública, registrando, de forma colaborativa, problemas encontrados na cidade. ao final deste trabalho, esperamos possuir um ecossistema composto pela sociedade, sistema e poder público, capaz de garantir melhores condições de administração para o governo local e tornar-se replicável para outras municipalidades, beneficiando a sociedade através da transparência e agilidade aplicada aos governantes com o software.', 'diante das mudanças ocorridas na sociedade, em que as incertezas e dinamismo causam mudanças contínuas nos requisitos dos stakeholders, ferramentas de projetos em que há um escopo fixo não conseguem acompanhar algumas das demandas sociais. assim, as necessidades do mercado acompanharam essa inconstância, necessitando-se de abordagens metodológicas de desenvolvimento de projetos mais ágeis, que se tornassem um canal de adaptação. em face das alterações de requisitos no decorrer do projeto, o scrum surgiu como uma alternativa tornando-se uma solução utilizada por muitas empresas, principalmente de tecnologia da informação. porém, a falta de uma estrutura adequada de implementação de metodologias ágeis em ambientes de pesquisa e desenvolvimento provocou uma necessidade por customizações nas práticas do scrum. o presente estudo analisou se as alterações aplicadas estão de acordo com os valores da metodologia scrum em projetos de pesquisa e desenvolvimento de computação na universidade federal de campina grande, por meio da obtenção de dados descritivos, quantitativos e qualitativos através de contato direto com a situação alvo, questionários com os colaboradores e voc (voz do consumidor). grande parte dos projetos possuem uma estrutura hierárquica e não realizavam todas as cerimônias do scrum, gerando dificuldades de autogestão e retrabalho.']"
