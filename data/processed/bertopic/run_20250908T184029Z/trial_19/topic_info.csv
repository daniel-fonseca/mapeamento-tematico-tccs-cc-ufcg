Topic,Count,Name,Representation,Representative_Docs
-1,166,-1_dados_so_sistema_forma,"['dados', 'so', 'sistema', 'forma', 'objetivo', 'projetos', 'anlise', 'tempo', 'uso', 'ferramenta']","['criar projetos de programação que necessitam de mais de um programador é uma prática comum na comunidade de desenvolvimento de software. isso ocorre porque muitos projetos exigem uma equipe de programadores com diferentes habilidades e conhecimentos para serem concluídos de maneira eficiente e eficaz. é comum ver programadores buscando outros profissionais para colaborar em seus projetos e, assim, alcançar um resultado de alta qualidade e escalabilidade. infelizmente, muitas vezes esse projeto não é levado adiante devido à falta de pessoas capacitadas e dispostas a trabalhar neles. o problema é a dificuldade em encontrar programadores qualificados e interessados em trabalhar em um projeto específico, o que pode levar a uma perda de tempo e recursos para aqueles que estão procurando por colaboradores. nesse contexto, o presente projeto propõe uma solução: uma aplicação web que organiza e conecta programadores para realizar projetos em conjunto. o usuário que se cadastrar no site irá disponibilizar informações sobre suas habilidades e conhecimentos na área de programação, permitindo que seja conectado a projetos disponíveis de acordo com seu perfil. além disso, o usuário também poderá criar seu próprio projeto, informando as tecnologias que serão utilizadas. dessa forma, espera-se que, ao final deste projeto, os programadores que desejam criar algum projeto ou empreender consigam encontrar facilmente outros programadores capacitados e dispostos a trabalhar com eles, superando a dificuldade encontrada atualmente na procura por colaboradores.', ""as redes adversárias generativas (gan's) têm aplicações amplas, desde a criação de imagens e vídeos até a geração de texto e design de produtos. no contexto deste estudo, serão avaliadas imagens de faces sintéticas geradas por gan's. há benefícios neste uso de gan's como pesquisas voltadas a entender a complexidade e nuances de imagens de faces e formação de bases de dados anônimas para treinamento de redes neurais com imagens de faces. entretanto, faces sintéticas podem ser usadas para criar identidades falsas, podendo levar a crimes como fraude de identidade e phishing, onde faces sintéticas são usadas para enganar sistemas de segurança baseados em reconhecimento facial. além disso, também podem ser usadas para criar vídeos e imagens falsos com intenções maliciosas, como difamação, desinformação ou propaganda política. neste trabalho, foi treinada uma rede neural convolucional profunda baseada na arquitetura efficientvit utilizando um conjunto de dados composto por bases de dados disponíveis publicamente e imagens sintéticas geradas pela rede stylegan3. os resultados obtidos indicam uma taxa de acurácia de %, semelhante a outros métodos na literatura, porém as bases de dados utilizadas para treinamento e avaliação diferem além da quantidade de imagens utilizadas na avaliação. ademais, houve uma procura de bases de dados diversificadas a fim de mitigar viés e justiça do modelo em relação à idade/etnia, porém uma análise à parte seria necessária para avaliar o impacto dessa escolha das bases de dados em comparação com outros modelos já treinados na literatura."", 'o github é a plataforma de hospedagem de código e controle de versão mais utilizada atualmente. diariamente, inúmeros projetos são criados, estendidos e modificados por diferentes usuários. entretanto, muitos projetos que possivelmente seriam do interesse de determinados usuários, acabam por passar despercebidos diante da grande quantidade de projetos disponíveis. neste contexto, surge a necessidade de algum mecanismo que possa auxiliar o usuário a encontrar projetos que podem ser de seu interesse. já existe na literatura trabalhos que buscam analisar fatores de interesse com o objetivo de recomendar projetos, entretanto ainda há margem para utilização de outros fatores e critérios na tentativa de obter resultados melhores. para tanto, o presente trabalho busca utilizar features, algumas já propostas na literatura e outras ainda não utilizadas nesse contexto, disponíveis em projetos do github, com o auxílio de algoritmos de learning to rank, para encontrar relações de interesse em projetos e assim recomendá-los para o usuário. verificamos a efetividade de learning to rank para recomendação de projetos usando os algoritmos lambdamart, random forest e coordinate ascent, utilizando como base repositórios e usuários do github. os resultados mostram que a abordagem de learning to rank para recomendação de projetos é promissora e efetiva, ao mesmo tempo que oferece muito espaço para aprimoramento.']"
0,32,0_alunos_estudantes_universidade_ufcg,"['alunos', 'estudantes', 'universidade', 'ufcg', 'campina', 'grande', 'campina grande', 'ensino', 'disciplinas', 'universidade federal']","['durante o período da pandemia promovida pelo covid- , a universidade federal de campina grande (ufcg) realizou quatro períodos letivos de forma remota para continuar as atividades de ensino. este trabalho analisa o impacto dessa modalidade de ensino e sua influência no desempenho e no nível de aprendizado dos alunos do curso de ciência da computação. a pesquisa utilizou dados anonimizados de registro de matrículas de a , concentrando-se nas disciplinas obrigatórias do curso. inicialmente, o estudo analisa a evolução das médias das notas dos alunos ao longo dos períodos acadêmicos, destacando um aumento nas médias durante os períodos remotos. além disso, a análise mostra uma diminuição no número de reprovações durante a pandemia em comparação com os períodos presenciais. o estudo também investiga, no regime remoto, as relações entre as disciplinas e o aproveitamento do aprendizado dos alunos. utilizando métodos estatísticos, foram identificadas as disciplinas mais afetadas pelo ensino remoto, tanto positivamente quanto negativamente. disciplinas como teoria da computação e estatística aplicada mostraram uma queda no desempenho, enquanto teoria dos grafos apresentou uma melhoria. em conclusão, o estudo aponta para um impacto negativo no desempenho dos alunos após o período de ensino remoto, sugerindo que fatores como eventuais irregularidades nas avaliações remotas podem ter contribuído para esse resultado.', 'os dias que antecedem a semana de matrículas da universidade federal de campina grande é o período em que os alunos dedicam parte do seu tempo organizando seus horários e planejando quais disciplinas pretendem cursar no próximo semestre. nesse período, as coordenações disponibilizam listas, através do sistema de “controle acadêmico” da universidade, com os dados de professores e horários para as disciplinas daquele semestre. esse trabalho tem o intuito de auxiliar o planejamento dos horários para o período de matrículas dos cursos de graduação na universidade federal de campina grande, com o desenvolvimento de um sistema web que permite o aluno organizar e planejar suas disciplinas através de uma interface agradável e que proporcione uma melhor experiência para a matrícula. para verificar a satisfação dos usuários quanto a usabilidade do sistema foi realizado um levantamento, utilizando o computer system usability questionnaire, que analisando as médias das respostas, foi obtido, na maioria dos itens, valores entre e , sendo o valor máximo, que apontam bons indicadores e alto nível de satisfação.', 'nos últimos anos, o uso de plataformas de ensino ganhou mais espaço nas instituições de educação, seja de modo complementar ao ensino presencial, ou para cursos totalmente remotos. dessa forma, é de grande importância o acesso às informações e funcionalidades nesses serviços à usuários com algum tipo de deficiência. no entanto, grande parte dos sistemas ainda não são projetados priorizando requisitos e padrões de usabilidade e acessibilidade para pessoas com deficiência, onde se pode destacar o grupo dos deficientes visuais, que segundo a organização mundial da saúde, no mundo, são milhões de pessoas. na universidade federal de campina grande (ufcg), uma das plataformas usadas para o ensino remoto é o moodle. neste contexto, este artigo apresenta um estudo para avaliar o nível de acessibilidade visual da plataforma moodle usando ferramentas automáticas e testes de usabilidade com participantes quem tinham algum grau de deficiência visual, com o intuito de encontrar possíveis problemas de interação. os resultados mostram que a plataforma não pode ser considerada totalmente acessível a todo e qualquer aluno com deficiência visual para usar plenamente todos os serviços disponibilizados nele, devido às violações das diretrizes utilizadas nas avaliações automáticas e de usabilidade.']"
1,27,1_nuvem_recursos_arquitetura_ambiente,"['nuvem', 'recursos', 'arquitetura', 'ambiente', 'infraestrutura', 'servio', 'gerenciamento', 'tempo', 'sistema', 'implantao']","['os contêineres são uma solução amplamente adotada na indústria de tecnologia, graças à sua flexibilidade e escalabilidade. nesse contexto, a gestão eficiente dos contêineres é essencial para garantir a disponibilidade e o desempenho do sistema. por esse motivo, há uma grande variedade de ferramentas de gerenciamento disponíveis, cada uma adequada para diferentes casos de uso. o objetivo deste estudo é analisar e comparar o desempenho de duas das principais ferramentas de orquestração de containers, o docker swarm e o kubernetes, em ambientes de nuvem. para realizar esta análise comparativa de desempenho, serão realizados testes de carga em ambas as plataformas e os resultados serão comparados. os testes serão executados em um ambiente de nuvem pública, com diferentes tamanhos de cluster e cargas de trabalho. os critérios de avaliação serão seis: complexidade de configuração, tempo de implantação de containers, escalabilidade, uso de recursos de cpu e memória e disponibilidade. os resultados dessa análise comparativa de desempenho permitirão contribuir para a identificação de qual ferramenta é mais adequada para diferentes cenários e cargas de trabalho.', 'o laboratório de sistemas distribuídos (lsd) é responsável por gerenciar a nuvem privada do uasc/ufcg. para manter essa nuvem privada, o lsd enfrentou desaios no gerenciamento dos recursos da nuvem, especialmente na visualização de dados e na geração de novos relatórios para os responsáveis pelos projetos que usam esses recursos. anteriormente, a ferramenta utilizada era o shylock, mas ela apresentava diiculdades na geração de relatórios sobre a quantidade e o consumo de recursos da nuvem, pois gerava apenas um relatório por dia. além disso, a criação de relatórios era um processo complexo que exigia a criação de um novo modelo jinja e a codiicação de variáveis. dado os déicits encontrados no sistema atual, o redash foi a plataforma selecionada para supri-los. o redash resolve a diiculdade de visualização dos dados analisados através da criação de dashboards para a visualização e monitoramento dos recursos da nuvem em tempo real. ao incluir as informações dos dashboards criados no redash no luxo de trabalho de gerenciamento e monitoramento da nuvem, a facilidade de tomar decisões com base em dados reais e atualizados permitiu uma melhor tomada de decisões sobre o gerenciamento dos recursos da nuvem.', 'no processo de desenvolvimento de software, a aquisição e manutenção de hardware adequado para as necessidades de programação podem resultar em altos custos de investimento de capital. a alternativa de uso de recursos em nuvem oferece flexibilidade, porém o gerenciamento desses recursos pode ser complexo e oneroso, requerendo conhecimentos especializados em operações em nuvem. o problema consiste em gerenciar um ambiente de desenvolvimento na nuvem de forma eficiente, evitando altos custos de aquisição e manutenção de hardware próprio, além de simplificar o gerenciamento de recursos ao alugar máquinas na nuvem, buscando minimizar despesas e eliminar a necessidade de expertise complexa em operações em nuvem. propomos o desenvolvimento de uma ferramenta de linha de comando, destinada a simplificar o gerenciamento do ambiente de desenvolvimento de software. essa ferramenta terá a capacidade de criar, configurar e gerenciar recursos na nuvem de forma automatizada e eficiente. uma característica diferencial é a utilização de instâncias preemptivas oferecidas por provedores de nuvem, permitindo aproveitar recursos ociosos a custos ainda mais baixos, sem comprometer a qualidade do ambiente de desenvolvimento. espera-se que o usuário seja capaz de criar ambientes de desenvolvimento utilizando a ferramenta proposta integrando-a com outras soluções já existentes para desenvolvimento de código. ao oferecer uma solução intuitiva, nossa abordagem visa otimizar o ambiente de desenvolvimento, maximizando a economia e eliminando a necessidade de conhecimentos avançados em operações em nuvem por parte da equipe de desenvolvimento. ao final deste trabalho, a usabilidade da ferramenta foi validada e demonstrou ser eficaz na simplificação do gerenciamento dos ambientes. a maioria dos participantes conseguiu gerenciar ambientes com sucesso, destacando a facilidade de uso e a utilidade da documentação fornecida.']"
2,22,2_modelos_linguagem_natural_linguagem natural,"['modelos', 'linguagem', 'natural', 'linguagem natural', 'prprocessamento', 'ocr', 'llms', 'processamento linguagem', 'respostas', 'arestas']","['este trabalho de conclusão de curso aborda o desenvolvimento de uma ferramenta web para o projeto de anotação de dados do laboratório de computação inteligente aplicada(lacina). essa ferramenta tem como foco anotações de áudio baseadas em diálogos, com o objetivo de facilitar a criação de conjuntos de dados obtidos para o treinamento de sistemas de diálogo baseados em machine learning. a ferramenta foi projetada para fornecer uma plataforma eficiente e intuitiva, permitindo a identificação de falantes e a marcação de intenções e entidades nos diálogos em áudio. o projeto destaca a importância das anotações de áudio baseadas em diálogos para o avanço dos sistemas de diálogo, e discute as vantagens de se ter conjuntos de dados anotados, bem como as considerações éticas e de privacidade associadas a esse processo. a sessão de desenvolvimento abordou os aspectos técnicos da ferramenta, incluindo a escolha das tecnologias e frameworks utilizados, os desafios enfrentados durante o andamento e as soluções adotadas para superá-los. foram destacadas as funcionalidades desejáveis, como uma interface de usuário amigável, recursos avançados de reprodução e visualização sincronizada de áudio e texto anotados. em seguida, foram discutidas as aplicações práticas da ferramenta no contexto de projetos de machine learning e processamento de linguagem natural. os conjuntos de dados anotados criados com a ferramenta podem ser usados para treinar modelos de reconhecimento de fala e compreensão de linguagem, contribuindo para o desenvolvimento de sistemas de diálogo conversacional. este trabalho visa fornecer uma visão abrangente desta área em constante evolução, destacando seu impacto e seu potencial para impulsionar a pesquisa e o desenvolvimento de aplicações de machine learning baseadas em áudio. a ferramenta web de anotações de áudio baseadas em diálogos representa uma contribuição significativa para o campo do processamento de linguagem natural e sistemas de diálogo, facilitando a criação de conjuntos de dados anotados de alta qualidade e impulsionando o avanço e o desenvolvimento de sistemas de diálogo mais eficientes, naturais e precisos.', 'avanços recentes em modelos de linguagem de grande escala (llms) expandiram significativamente as capacidades da inteligência artificial (ia) em tarefas de processamento de linguagem natural. no entanto, seu desempenho em domínios especializados, como a ciência da computação, permanece relativamente pouco explorado. este estudo investiga se os llms podem igualar ou superar o desempenho humano no poscomp, um exame brasileiro prestigiado usado para admissões de pós-graduação em ciência da computação. quatro llms-chatgpt- , gemini . advanced, claude sonnet e le chat mistral large-foram avaliados nos exames poscomp de e . a avaliação consistiu em duas análises: uma envolvendo interpretação de imagens e outra somente de texto, para determinar a proficiência dos modelos em lidar com questões complexas típicas do exame. os resultados indicaram que os llms tiveram um desempenho significativamente melhor nas questões baseadas em texto, com a interpretação de imagens representando um grande desafio. por exemplo, na avaliação baseada em imagens, o chatgpt- respondeu corretamente de perguntas, enquanto o gemini . advanced conseguiu apenas respostas corretas. na avaliação baseada em texto de , o chatgpt- liderou com respostas corretas, seguido por gemini . advanced ( ), le chat mistral ( ) e claude sonnet ( ). o exame de mostrou tendências semelhantes.', 'modelos de linguagem de grande escala (llms), como o chatgpt, claude e llama , revolucionaram o processamento de linguagem natural, criando novos casos de uso para aplicações que utilizam esses modelos em seus fluxos de trabalho. no entanto, os altos custos computacionais desses modelos acarretam problemas de custo e latência, impedindo a escalabilidade de funcionalidades baseadas em llm para muitos serviços e produtos, especialmente quando dependem de modelos com melhores capacidades de raciocínio, como o gpt- ou o claude opus. além disso, muitas consultas a esses modelos são duplicadas. o cache tradicional é uma solução natural para esse problema, mas sua incapacidade de determinar se duas consultas são semanticamente equivalentes leva a baixas taxas de cache hit. neste trabalho, propomos explorar o uso de cache semântico, que considera o significado das consultas em vez de sua formulação exata, para melhorar a eficiência de aplicações baseadas em llm. realizamos um experimento usando um conjunto de dados real da alura, uma empresa brasileira de educação, em um cenário onde um aluno responde a uma pergunta e o gpt- corrige a resposta. os resultados mostraram que , % das solicitações feitas ao llm poderiam ter sido atendidas a partir do cache usando um limiar de similaridade de . , com uma melhoria de - vezes na latência. esses resultados demonstram o potencial do cache semântico para melhorar a eficiência de funcionalidades baseadas em llm, reduzindo custos e latência enquanto mantém os benefícios de modelos avançados de linguagem como o gpt- . essa abordagem poderia possibilitar a escalabilidade de funcionalidades baseadas em llm para uma gama mais ampla de aplicações, avançando na adoção desses modelos poderosos em diversos domínios.']"
3,19,3_software_empresas_requisitos_crm,"['software', 'empresas', 'requisitos', 'crm', 'carreira', 'processo', 'scrum', 'objetivo', 'qualidade', 'equipes']","['fenômenos como abandono (turnaway) e transição de carreira (turnover) estão cada vez mais frequentes em empresas de desenvolvimento de software, um dos principais motivos pode estar relacionado com a falta de identificação com a área. investigar esses fenômenos no âmbito comportamental por meio das âncoras de carreira do indivíduo pode ser primordial para que as empresas melhorem seus planos de carreira, política empresarial e política de retenção. este estudo visa investigar a relação entre o abandono de carreira (turnaway) com as âncoras de carreira de ex-desenvolvedores. para isso foi aplicado o teste âncora de carreira entre ex-desenvolvedores de diferentes regiões do brasil por meio de survey a fim de identificar dados sobre o processo de desenvolvimento de software a partir de entrevistas semi-estruturadas realizadas, transcritas e codificadas por outros pesquisadores da área. com base nos dados obtidos, esperamos que esses resultados contribuam para conceber estratégias eficazes para as empresas reterem seus colaboradores, além disso, minimizar o custo social de abandono de carreira daqueles desenvolvedores e incentivar pesquisas futuras.', 'o desenvolvimento de aplicações android é uma tarefa complexa. a variedade de dispositivos, e suas diferentes configurações, são um exemplo das razões que dificultam esse desenvolvimento. por causa disso, um processo de desenvolvimento bem planejado é importante para garantir a qualidade do software. a falta desse processo pode aumentar a incidência de bugs e as chances de requisitos do sistema não serem satisfeitos. esse trabalho tem como objetivo implantar um processo de qualidade de software no contexto do desenvolvimento de uma aplicação android. essa aplicação tem como propósito ser utilizada por agentes de saúde que a alimentarão com dados que serão processados e modelados a fim de emitir alertas antecipados sobre a incidência de populações de mosquitos transmissores de doenças. o processo de qualidade será definido a partir de uma pesquisa na literatura sobre boas práticas em desenvolvimento android, um estudo sobre o domínio do problema e sobre regras de negócio da aplicação. a partir dessa pesquisa, será discutida uma estratégia para a criação de um processo que engloba o desenvolvimento de testes, versionamento de código, escolha dos requisitos não funcionais e procedimentos para a implantação da aplicação. ao final do trabalho, será aplicado um questionário aos supervisores para que eles possam avaliar a eficácia do processo implantado. espera-se que, depois de adotado o processo de qualidade, exista uma maior clareza e fluidez nas entregas da aplicação, e que seja minimizada a quantidade de problemas.', 'o software crm (customer relationship management) realiza a gestão de diversos tipos de processos, tais como: edição gráfica, pós-venda, atendimento e prospecção de clientes, entre outros. entretanto, tem-se notado que agências de marketing também atuam em projetos de software. idealmente, projetos de software devem ser gerenciados de forma ágil, através de metodologias consolidadas na engenharia de software, como o scrum e xp, utilizando ferramentas especializadas. todavia, lidar com duas ou mais ferramentas para atender demandas menores é laborioso e problemático para agências que, há anos, utilizam crms. diante disso, o objetivo central do trabalho é propor uma estratégia para adaptação do uso do crm para agregar os conceitos das metodologias ágeis em projetos de software e avaliar os ganhos obtidos. nesse sentido, foi realizado um levantamento das principais ferramentas de crm do mercado; conduziu-se um estudo de caso em uma agência de marketing em um projeto real de implantação de e-commerce; e avaliou-se a estratégia proposta através de uma pesquisa de opinião com os participantes do estudo, constatando-se que o êxito da entrega do projeto teve influência direta dos ganhos obtidos com a adaptação do uso da ferramenta crm na fase de implantação.']"
4,17,4_digital_tweets_cidadania digital_cidadania,"['digital', 'tweets', 'cidadania digital', 'cidadania', 'sociais', 'sobre', 'notcias', 'redes', 'usurios', 'anlise']","['o mundo se aproxima a quase bilhões de usuários na internet, tornando-se a sua utilização um hábito imperceptível. estamos conectados e precisamos seguir um conjunto de regras para um relacionamento virtual agradável, para isso, surge a cidadania digital. cidadania digital pode ser entendido como um conjunto de normas que devemos seguir para utilizarmos o mundo digital, e as diferentes tecnologias que o constituem, com consciência, responsabilidade, ética e segurança. a temática abrange pessoas de todas as idades e gera especial atenção para as crianças e adolescentes. considerando esse público, existem iniciativas relevantes de grandes empresas como a google™, através da plataforma interland[ ], e de ongs especializadas no uso seguro da internet, tais como safernet[ ], nic.br e cetic.br[ ]. apesar dessas ações, ainda é um desafio a introdução de trilhas de conhecimento em cidadania digital para crianças e adolescentes na escola. o objetivo dessa pesquisa é desenvolver uma trilha de conhecimento para educação em cidadania digital voltada para o público do ensino fundamental (anos iniciais) que seja lúdica e participativa, com produção de material didático para alunos e professores, desenvolverem conhecimento e prática em cidadania digital.', 'a cidadania digital envolve o desenvolvimento de competências eficazes e éticas no uso das tecnologias, sempre respeitando princípios fundamentais, como privacidade, segurança e responsabilidade. a educação digital desempenha um papel crucial como parte integrante da cidadania digital, indo além do mero uso das tecnologias e sendo a base para a formação de cidadãos digitalmente conscientes. a educação digital é a chave para preparar as gerações presentes e futuras, capacitando-as a navegar com sucesso no mundo digital, compreender e aplicar as tecnologias de forma ética e eficaz. a crescente demanda no mercado de trabalho por profissionais com habilidades digitais destaca a importância da educação digital em todos os níveis, desde programas técnicos até a educação superior. a essência de uma aprendizagem eficaz reside na concepção de programas de capacitação envolventes, que não apenas desenvolvam habilidades técnicas, mas também promovam uma cidadania digital consciente. dentro desse contexto, este trabalho tem como objetivo contribuir para a promoção da educação digital, fornecendo um guia prático repleto de orientações para o desenvolvimento de cursos, com metodologias ativas e no contexto de extensão, que gerem um impacto significativo na comunidade da cidade de campina grande.', 'em um mundo onde as pessoas têm cada vez mais contato com as tecnologias, surgiram também questões sobre como utilizá-las adequadamente. muitas vezes, essas questões surgem em respostas a perigos percebidos ou comportamentos inapropriados como roubo de identidade, cyberbullying ou disseminação de notícias falsas, especialmente no mundo online. tais abusos podem ser ainda mais prejudiciais aos adolescentes que cada vez mais cedo são inseridos nesse contexto tecnológico. apesar de os usuários terem capacidade de avaliar uma conduta online adequada e identificar riscos, é necessário que eles desenvolvam um senso crítico para utilizar a internet de forma responsável, ou seja, usar o ambiente online sem prejudicar a si mesmos e aos outros. para isso, a cidadania digital surge como um conjunto de normas de comportamento que devem ser seguidas para o uso amigável, seguro e ético das tecnologias. logo, a presente pesquisa tem como objetivo analisar as principais tecnologias digitais utilizadas por adolescentes e, a partir delas, listar formas de uso responsável de acordo com os elementos da cidadania digital a fim de conduzir de forma positiva o comportamento de adolescentes na sociedade e constituir uma cultura de uso seguro das tecnologias digitais.']"
5,15,5_oratio_projeto_clean code_clean,"['oratio', 'projeto', 'clean code', 'clean', 'solid', 'code', 'escalabilidade', 'manuteno', 'oratio projetada', 'java oratio']","['', '', 'oratio é uma aplicação web criada para facilitar o trabalho dos professores responsáveis pela disciplina de trabalho e conclusão de curso (tcc) de ciência da computação da universidade federal de campina grande . ela permite que o professor aloque facilmente avaliadores e tenha o controle de todas as informações relacionadas a um projeto e a um aluno. desenvolvida em flutter e java, oratio foi projetada com a filosofia de clean code e solid, garantindo assim a qualidade, a eficiência e a escalabilidade do código. embora o projeto oratio tenha seguido boas práticas de programação, não foram utilizados todos os conceitos do clean code, mas sim uma modificação dos mesmos para melhorar o desenvolvimento e a manutenção do projeto. isso foi feito para garantir que as características do solid sejam atendidas, visando aumentar a manutenção e a flexibilidade do código e tornar o projeto mais robusto e de fácil escalabilidade em um curto tempo de desenvolvimento.']"
6,12,6_dados_financeira_mercado_financeiro,"['dados', 'financeira', 'mercado', 'financeiro', 'obras', 'fiscais', 'notas', 'meio', 'obras pblicas', 'elasticsearch']","['é notório o crescimento do ingresso de investidores individuais no mercado financeiro. muitos desses investidores não conseguem discernir corretamente onde devem investir o seu dinheiro para obter uma maior rentabilidade. neste contexto, este trabalho teve como objetivo desenvolver uma aplicação para auxiliar nas decisões de compra e venda de ativos por meio de uma rede neural treinada de forma supervisionada sobre dados extraídos das cotações dos ativos. a aplicação desenvolvida busca apresentar uma forma intuitiva para guiar as decisões diárias de investimento (day trading) no tocante às ações que devem permanecer, entrar ou sair da carteira de cada usuário. o princípio é prever a cotação do dia seguinte da ação, com recomendação de venda se o preço previsto for diminuir e de compra se o preço for aumentar. a aplicação informa também uma estimativa sobre quanto cada operação de compra ou venda produzirá de lucro. para isso, foram feitos experimentos com três ativos diferentes que atendiam as principais movimentações do mercado e foi possível notar um acréscimo no valor inicial investido pelo usuário de aproximadamente % a % no montante final. a ferramenta desenvolvida poderá auxiliar tanto usuários iniciantes no mercado de ações quanto usuários mais experientes.', 'o objetivo do tce-ac é fiscalizar as despesas e receitas dos municípios e do estado do acre. para tanto, nos últimos anos tem modernizado a sua forma de trabalho. em particular, o acesso rápido aos preços praticados é fundamental para a fiscalização e também para a população em geral. para isso, o banco de preços é utilizado, sendo alimentado por uma base de dados em constante crescimento e que, atualmente, conta com dezenas de milhões de registros de notas fiscais. diante desse cenário, por utilizar de banco de dados relacionais para a realização das consultas e devido a grande massa de dados existente, o sistema em questão acaba demorando para produzir resultados em diversas situações, além de retornar resultados pouco relevantes em algumas situações. para solucionar o problema, propõe-se a implantação do elasticsearch como o motor de busca do sistema. o elasticsearch utiliza técnicas de indexação e possui ferramentas que otimizam a execução e resultados das queries realizadas. além disso, serão implementadas estratégias para a carga contínua dos dados, além da documentação dos desafios enfrentados durante a implementação. para avaliar a solução proposta, foram realizadas medições de estatísticas referentes ao tempo de resposta e qualidade das consultas antes e depois da implantação do elasticsearch. a qualidade dos resultados foi verificada por meio de técnicas como ndcg (normalized discounted cumulative gain) e f1-score, a partir da definição dos documentos relevantes ou não para cada consulta. como resultado, foi possível notar uma diminuição em vezes do tempo de respostas das consultas realizadas no elasticsearch quando comparado com os resultados envolvendo o sql server. além disso, também foi possível observar uma melhora na relevância dos resultados retornados de cerca de %, chegando a um ndcg de , % em média, para consultas com resultados, utilizadas por padrão no sistema.', 'a gestão eficaz das finanças pessoais é fundamental para a saúde financeira e, consequentemente, para a qualidade de vida. para melhorar a gestão financeira, muitas pessoas recorrem a aplicativos de controle financeiro para smartphones. pesquisas demonstraram que esses aplicativos têm um impacto positivo na administração financeira dos usuários. no entanto, alguns usuários enfrentam dificuldades na utilização desses aplicativos visto que a maioria exige atualizações manuais dos dados. além disso, aplicativos com automatização muitas vezes carecem de detalhes sobre os gastos, deixando os usuários questionando como gastaram seu dinheiro. tendo isso em vista, o propósito do trabalho ora descrito é desenvolver um aplicativo android, a fim de facilitar o controle financeiro dos seus usuários com o registro de finanças, por meio de comando de voz. o uso da voz tornará mais fácil e prático o registro em qualquer momento do dia, incluindo o exato momento em que uma alteração em suas finanças é feita, desde o recebimento de um salário até um gasto com um alimento comprado no caminho para o trabalho.']"
7,11,7_dados_logs_aplicaes_sistemas,"['dados', 'logs', 'aplicaes', 'sistemas', 'banco', 'benchmarks', 'observabilidade', 'banco dados', 'vendas', 'sistema']","['no contexto de sistemas de banco de dados, tanto o desenvolvimento quanto a manutenção são processos que demandam alterações nos seus modelos. essas alterações causam consequências em todo o sistema. os modelos lógico, conceitual e físico possuem versões que variam à medida que o sistema evolui. o modo como a equipe de desenvolvimento ou profissionais de manutenção de banco de dados, os dbas, visualizam o sistema, mais especificamente as relações entre os objetos desse sistema, tem efeitos diretos na forma em que o sistema será gerenciado por eles. sendo assim, desenvolvemos uma aplicação web que permita a visualização dessas relações e exiba ao usuário os objetos que sofreriam algum efeito colateral caso algum outro objeto do sistema for alterado. para o escopo deste trabalho, consideraremos apenas as alterações causadas pela operação drop sobre algum objeto do sistema gerenciador de banco de dados oracle autonomous data warehouse, do serviço oracle. essa ferramenta se mostra fundamentalmente útil na manutenção de sistemas de banco de dados.', 'a observabilidade desempenha um papel importante no desenvolvimento e na manutenção de software. podemos dizer que um sistema é observável quando pode-se entender e explicar qualquer estado em que o mesmo possa entrar, podendo ele ser corriqueiro ou algo totalmente novo. juntamente com métricas e traces, os logs representam um dos pilares da observabilidade, desempenhando um papel vital na depuração dos estados de um sistema. isso ressalta sua importância como fonte de dados e a necessidade de seu tratamento e armazenamento. nesse contexto, o opentelemetry emerge como um framework e conjunto de ferramentas que se propõe a facilitar a coleta e a gestão de dados de observabilidade em sistemas. sendo independente de fornecedores e ferramentas, e adotando um modelo de código aberto, o opentelemetry se revela um software altamente versátil, adaptável às necessidades individuais de seus usuários, tornando-se uma escolha ideal na implementação de observabilidade em sistemas. o foco deste trabalho está no aprimoramento de um módulo utilizado em um coletor opentelemetry, cuja função principal é receber logs em uma plataforma de comércio eletrônico. esse módulo compreende dois componentes: o wal, responsável por detectar falhas no envio de logs ao opensearch e armazenar logs não enviados em um serviço de armazenamento de objetos; e um replayer de logs, que tenta reenviar os logs armazenados posteriormente ao opensearch. entretanto, o replayer de logs enfrenta desafios relacionados a disponibilidade de recursos de hardware, instabilidade em ambientes variáveis e limitações na configuração, o que impacta negativamente em sua eficácia no envio de logs ao opensearch. além disso, a ausência de dados sobre a saúde e o desempenho do wal pode dificultar a manutenção e depuração deste componente, devido à falta de informações relevantes. diante desse cenário, este trabalho tem como objetivo aprimorar o replayer de logs, visando melhorar a disponibilidade e a utilização dos recursos de hardware, aumentar sua confiabilidade no envio de logs ao opensearch e torná-lo mais flexível em termos de configuração. também, pretende-se adicionar capacidades de observabilidade ao mecanismo wal com o objetivo de garantir maior visibilidade do funcionamento do mecanismo e facilitar a depuração do mesmo.', 'um dos maiores problemas encontrados em aplicações que estão envolvidas no ecossistema de big data está relacionado à disponibilidade e qualidade de dados para modelos de ia e outras análises direcionadas. aplicações com esse foco necessitam de dados que disponham de alta qualidade, já que o resultado de seus serviços depende da integridade da informação usada no processo. quando pensamos em dados textuais, devemos saber que a informação fornecida para aplicações que envolvem processamento de texto, devem ser as melhores possíveis. desta forma, foi desenvolvido uma aplicação que trata da gerência da coleta e tratamento contínuo de dados textuais. o contexto da aplicação está fixo na coleta de dados textuais da rede social reddit. através da api fornecida pela rede, é feita a ingestão de dados de uma comunidade específica. com base nos dados coletados, a ferramenta trata de fazer todo o orquestramento de tarefas que gerenciam a coleta, tratamento e disponibilização desses dados. para teste da ferramenta, os dados disponíveis são passados para um modelo de pln, que usa lda para mapear tópicos com base nos textos extraídos do site. a aplicação se baseia nos conceitos de streaming de dados e processamento de texto, de forma contínua e automática, a fim de manter uma base de dados sólida e de qualidade para análises de texto.']"
8,10,8_busca_soluo_regio_placas,"['busca', 'soluo', 'regio', 'placas', 'regies', 'gis', 'nodegis', 'mapas', 'ferramenta', 'geogrficas']","['a representação de regiões geográficas tem sido alvo de pesquisas nos últimos tempos, pois é a peça chave para a realização de diversas tarefas, como a busca por regiões similares. tal representação, porém, não é tarefa trivial, uma vez que pode envolver inúmeras variáveis no processo. a tendência atual é que essas representações sejam feitas através de vetores de alta dimensão, conhecidos como embeddings. porém, operações de busca por estes costumam ser custosas para a máquina em termos de tempo de processamento e consumo de disco. neste artigo experimentou-se diferentes manipulações nesses vetores a fim de diminuir o consumo de recursos computacionais no momento da busca sem comprometer significativamente a relevância dos resultados produzidos por ela. técnicas de redução de dimensionalidade dos vetores e quantização de seus elementos foram executadas, além de comparações entre a busca exata por vizinhos mais próximos e a busca aproximada por estes. observou-se que a busca aproximada por vizinhos mais próximos reduz o tempo de busca em aproximadamente , %, mantendo uma boa aproximação com os resultados do baseline. a técnica de quantização dos embeddings apresentou a segunda maior interseção com o baseline e reduziu consideravelmente o consumo de disco pelos índices. técnicas como a redução de dimensionalidades não apresentaram grandes alterações no tempo de busca e tiveram interseções baixíssimas com o baseline da pesquisa.', 'diante da vasta presença da informação espacial nas aplicações atuais, surgiram várias ferramentas que fomentam o desenvolvimento de aplicações web de sistemas de informações geográficas (gis). apesar de haver um grande número de soluções, muitas são complexas, requerendo habilidades específicas dos desenvolvedores. portanto, há a necessidade de uma ferramenta que simplifique o processo de desenvolver e publicar uma aplicação web gis, com a vantagem de ser de código aberto. neste trabalho é apresentado o nodegis, uma ferramenta de código aberto que provê uma interface gráfica para o desenvolvimento de aplicações de web gis, sem escrita de código ou configuração complexa de servidores. o nodegis utiliza-se de uma arquitetura baseada em contêineres, fazendo uso de rest, e facilitando o deployment de uma aplicação web gis. a ferramenta apresentada permite ao usuário plotar mapas vetoriais, realizar operações de overlay e de personalização de camadas, zooming, panning, tooltip, consultas em atributos convencionais e espaciais. o nodegis também pode ser utilizado no ensino de gis, não necessitando instalação de software por parte dos alunos.', 'aplicações de recuperação de informação estão cada dia mais robustas, versáteis e atendendo aos mais diversos ins. na área de recuperação de informação geográica (gir), embora muitas abordagens tenham sido propostas para lidar com diversos problemas, alguns ainda permanecem insuicientemente explorados, a exemplo de abordagens que permitam buscar regiões que sejam similares a uma região de interesse de forma rápida e escalável. dada uma região espacial e uma região de consulta, uma pesquisa por região similar visa encontrar as k regiões mais semelhantes à região de consulta na região espacial. neste trabalho é apresentada uma abordagem de busca por similaridade em mapas utilizando um algoritmo de similaridade textual com dados armazenados e indexados. para isso, desenvolveu-se um algoritmo baseado no uso de índices no elasticsearch[ ] para armazenamento dos dados geográicos. as consultas são textuais, utilizando o conceito de índice invertido junto com o algoritmo de similaridade bm25[ ]. nesse algoritmo, as informações de points of interest (poi) presente nas regiões dos mapas são convertidas em dados textuais que são a base para realização de consultas. a abordagem proposta é baseada em um estudo de caso utilizando dados de grandes cidades obtidos através openstreetmaps1.']"
9,9,9_programao_objetos_conceitos_programao orientada,"['programao', 'objetos', 'conceitos', 'programao orientada', 'orientada', 'competncias', 'paradigma', 'orientada objetos', 'pensamento computacional', 'questes']","['o pensamento computacional fundamenta-se nas competências adquiridas por meio da ciência da computação. com base nisso, surgiu o compensar, uma ferramenta web voltada para o desenvolvimento do pensamento computacional em conjunto com a matemática, direcionada à educação básica. sua estrutura baseia-se na classificação das questões matemáticas conforme as competências do pensamento computacional que podem ser estimuladas durante o processo de resolução. no entanto, diversos fatores contribuíram para sua transformação em um sistema legado e inoperante. a ausência de contribuições ao longo de mais de anos resultou na obsolescência das tecnologias empregadas tanto no frontend quanto no backend da ferramenta. além disso, a inatividade do serviço de hospedagem do compensar, devido à ausência de requisições por parte dos usuários, culminou na perda do banco de dados devido à falta de consultas. a identificação desses problemas ressaltou a necessidade de uma reengenharia de software em conjunto com a engenharia reversa, visando reconstruir o sistema com tecnologias mais recentes. esse processo foi dividido em etapas que culminaram no desenvolvimento de uma nova versão do compensar, agora disponível para acesso público. em uma dessas etapas, foram realizadas avaliações para compreender o nível de satisfação do cliente com o software entregue, bem como análises de desempenho e indexação em motores de busca. os resultados obtidos foram positivos. na análise de desempenho, o compensar recebeu boas avaliações em categorias como acessibilidade, práticas recomendadas e seo (search engine optimization). esses resultados contribuíram para a visibilidade da ferramenta nos motores de busca, os quais utilizam dados dessas categorias, como o carregamento de conteúdo e a acessibilidade, na atribuição de pontuação do compensar nos motores de busca.', 'cursos introdutórios de programação costumam ser desafiadores, especialmente para aqueles sem experiência prévia em lógica de programação. ao ingressar em um curso de tecnologia, é necessário o aprendizado de terminologias relacionadas à ciência da computação e adaptar-se aos diferentes conceitos e paradigmas. se tratando de programação orientada a objetos (poo), utilizando java como linguagem de programação, a dificuldade é ainda maior. seu amplo conjunto de conceitos e sua sintaxe verbosa fazem parte de uma transição desafiadora, seguida de erros quanto aos conceitos aprendidos, refletindo muitas vezes a permanência do aluno no curso. sendo assim, com o objetivo de compreender e analisar estes erros, este trabalho se propõe a seguir os mesmos passos de um estudo anterior conduzido em uma turma da disciplina de laboratório de programação ii no curso de ciência da computação da universidade federal de campina grande. a dificuldade de assimilação de conceitos de java implica na observação de erros em atividades práticas e avaliativas, como foi constatado no estudo mencionado, que analisou principalmente o volume de código das submissões dos alunos. a intenção é fazer um comparativo entre os resultados, investigando padrões e tendências que possam variar conforme há mudança de amostra. uma vez identificando tais erros, bem como a sua frequência e impacto na nota do aluno, este estudo pode contribuir para potenciais adaptações nos métodos de ensino para que haja uma melhor aproximação na solução destes problemas, fortalecendo a compreensão dos conceitos fundamentais de programação orientada a objetos, uma vez que é recorrente em outras disciplinas da grade curricular.', 'ensinar o paradigma de programação orientada a objetos costuma ser um desafio para os professores. a principal dificuldade é muitas vezes atribuída à mentalidade que o paradigma exige. essa mentalidade envolve raciocinar sobre elementos da realidade em termos de classes, objetos, atributos, polimorfismo etc. em suma, é uma mentalidade que requer boas habilidades de abstração. várias metodologias, abordagens e ferramentas já foram propostas para ajudar os alunos a alcançar a mentalidade necessária para aplicar esse paradigma, mas o aprendizado continua difícil. diante disso, uma ferramenta que até então nunca havia sido considerada para o ensino de programação é o trivium medieval. o trivium consiste nas três artes liberais de gramática, lógica e retórica. o syllabus e a estrutura das aulas do trivium podem ser um modelo interessante para ser aplicado em cursos de programação orientada a objetos, pois abordam de forma bastante didática conceitos fundamentais idênticos ao do paradigma orientado a objetos. a demonstração da correlação entre os dois assuntos é um dos objetivos deste trabalho. além disso, conjecturamos que ensinar os conceitos fundamentais da gramática antes ou paralelamente ao ensino do paradigma orientado a objetos parece ser mais eficiente do que começar logo pela prática de programação, como costuma ser feito em cursos de programação. este artigo propõe duas abordagens para o ensino do paradigma orientado a objetos. eles consistem na estruturação do curso de programação orientada a objetos com base na filosofia e metodologia educacional clássica, a fim de facilitar a compreensão do paradigma.']"
10,8,10_tags_ecommerce_web_gtm,"['tags', 'ecommerce', 'web', 'gtm', 'performance', 'pesquisa', 'pwa', 'usurio', 'tags gtm', 'utilizao tags']","['a capacidade de fornecer resultados de pesquisa personalizados e relevantes em um ambiente de e-commerce altamente competitivo é crucial para a satisfação do cliente e o sucesso das lojas online. neste trabalho, exploramos um método para melhorar a experiência de pesquisa no e-commerce usando modelos de aprendizado profundo para personalizar as consultas do usuário e melhorar a relevância dos itens retornados. o modelo de aprendizado de máquina apresentado foi projetado como uma prova de conceito para avaliar sua capacidade de entender o contexto e a intenção por trás das consultas de pesquisa do usuário e adaptá-las de forma inteligente antes de serem submetidas ao mecanismo de pesquisa. o modelo reescreve a consulta original para priorizar os produtos de interesse do cliente, descobrindo a intenção subjacente do usuário e o contexto da pesquisa. além disso, também propomos um modelo classificador que é responsável por selecionar consultas passíveis de serem reescritas antes de usar o modelo de reescrita. esta abordagem permite melhorar os resultados da pesquisa para destacar produtos de interesse, melhorando significativamente a relevância e a eficácia da pesquisa.', 'o presente trabalho busca compreender se a utilização de tags do google tag manager (gtm) interfere na performance de um e-commerce perfeitamente otimizado. inicialmente, será analisado o conceito de performance, de tags, de gtm, bem como sua importância no contexto de um e-commerce. em seguida, para alcançar os objetivos propostos, foram conduzidas análises de performance em um e-commerce otimizado sob três cenários distintos: sem o uso de tags, com a utilização das tags do gtm e com utilização de tags robustas do gtm. em todos os cenários, foram excluídos os outliers na análise, com o objetivo de afastar possíveis inconsistências nos resultados. os resultados demonstram que houve uma tendência de queda na performance à medida que a complexidade das tags aumenta. constatou-se que a implementação de tags do gtm, embora útil para o rastreamento de dados, pode ter um impacto negativo na performance do site se não for gerenciada corretamente. por causa disso, atestou-se que no contexto de e-commerce esse processo envolve tomar decisões estratégicas sobre quais tags são realmente necessárias e como podem ser implementadas de forma a minimizar seu impacto na performance do site, garantindo, ao mesmo tempo, uma experiência positiva para o usuário.', 'com a ascensão de javascript como uma das principais linguagem de programação para web, diversos frameworks surgiram para auxiliar a criação de projetos complexos e escaláveis. uma tendência dos dias atuais é a utilização da web através de dispositivos móveis. neste contexto, ocorre um aumento na demanda por aplicações leves que substituam as aplicações nativas. essa demanda resultou em uma metodologia que combina recursos oferecidos pelos navegadores com as vantagens do uso de um celular. o progressive web app (pwa) é uma metodologia de desenvolvimento que busca trazer para aplicações web, acessadas por navegadores móveis, a mesma experiência (leve e responsiva) vivenciada nos aplicativos nativos. atualmente, existe uma enorme variedade de bibliotecas e frameworks, o que causa dúvidas na tomada de decisão para a construção de um pwa. o objetivo deste trabalho é realizar um estudo comparativo entre três tecnologias, para a implementação de um pwa. para isso, foi desenvolvido um mesmo sistema utilizando angular, vue e react para coletar métricas e realizar análises sobre suas vantagens e desvantagens. como as tecnologias seguem o mesmo paradigma, o estudo apresenta as características, de cada uma, associando essas a implementação de um pwa que consume uma api rest. por fim, foram estabelecidos guidelines para ajudar programadores web a escolher entre as várias tecnologias disponíveis.']"
11,8,11_estresse_ingressantes_atividades_curso,"['estresse', 'ingressantes', 'atividades', 'curso', 'computao', 'ufcg', 'alunos', 'desempenho', 'oportunidades', 'semestre']","['as instituições de ensino superior (ies) desempenham um papel fundamental na formação de profissionais qualificados, contribuindo significativamente para o desenvolvimento do país. no entanto, elas enfrentam desafios que afetam sua capacidade de cumprir esse papel de forma adequada. a gestão eficiente de seus recursos é uma das preocupações mais persistentes. muitos estudos avaliam a eficiência de vários aspectos da gestão através de uma avaliação do custo monetário associado a diferentes atividades. embora seja crucial ter uma visão clara desses aspectos, é igualmente importante considerar fatores específicos, como, por exemplo, a carga horária de docentes dedicada diretamente à formação dos discentes. apesar de existir um empenho significativo no sentido de otimizar esse custo, é notável a falta de estudos que detalhem como a trajetória acadêmica do discente afeta seu custo na ies. compreender como esse custo interfere na eficiência da instituição é crucial para tomar medidas direcionadas à melhoria da gestão e otimização dos recursos disponíveis. neste trabalho foram propostas métricas para medir a eficiência de cursos de graduação da universidade federal de campina grande (ufcg), durante um período de anos, no que diz respeito à alocação de docentes às atividades de ensino. como resultado da mensuração realizada, pode-se observar que cursos com um número de ingressantes baixo, assim como cursos com altas taxas de evasão e de retenção, têm um custo alto, enfatizando a necessidade de tratar cuidadosamente cada um dos aspectos relacionados ao cálculo da eficiência dos cursos analisados.', 'o estresse é uma resposta normal do corpo a situações desafiadoras ou exigentes, mas quando os níveis de estresse são muito altos ou duram por um longo período, podem levar a problemas de saúde mental, como ansiedade e depressão. estudos mostram que estudantes universitários experimentam altos níveis de estresse relacionados a pressão acadêmica e mudanças na vida social e financeira, entre outros fatores. neste trabalho, conduzimos uma pesquisa com os concluintes do curso de ciência da computação na universidade federal de campina grande (ufcg) durante o semestre . . a pesquisa buscou entender a relação entre fatores de estresse (saúde, social e acadêmico), o nível de estresse percebido em diferentes momentos do semestre (início, meio e fim) e seu impacto no desempenho acadêmico. os resultados mostram que os estudantes apresentam altos níveis de estresse ao longo de todo o semestre, e o principal fator de estresse que afeta o desempenho acadêmico é a pressão da família para conseguir um bom emprego. além disso, uma correlação negativa significativa foi identificada entre o estresse percebido no final do semestre e o desempenho acadêmico. contudo, o estresse no início e meio do período não mostrou relação estatisticamente significativa com o desempenho dos alunos.', 'a evasão nas instituições de ensino superior no brasil é um problema que gera impactos de cunhos financeiro e social, para toda a população. no curso de computação da universidade federal de campina grande (ufcg) a evasão é motivo de preocupação de professores e gestores, sobretudo em relação aos alunos ingressantes do curso. o programa de educação tutorial (pet) em computação da ufcg desenvolveu uma abordagem complementar e colaborativa de formação de alunos veteranos e ingressantes, denominada gesto, com o intuito de diminuir os índices de evasão dos estudantes ingressantes nesta fase desafiadora de início da experiência universitária. a abordagem é baseada na prática do voluntariado e na construção social de sentidos do estudo de computação, através de atividades de desenvolvimento de habilidades técnicas (hardskills) e não-técnicas (softskills) escolhidas pelos alunos em um portfólio que lhes é oferecido semestralmente. este trabalho apresenta uma análise dos impactos desta abordagem a partir do estudo de três atividades escolhidas pelos ingressantes dentre aquelas disponíveis no portfólio oferecido no semestre letivo de . . resultados das avaliações feitas pelos alunos participantes, ingressantes e veteranos, indicam que tal abordagem pode, juntamente com outras ações, diminuir a evasão no curso de ciência da computação na ufcg.']"
12,8,12_dados_privacidade_informaes_sensveis,"['dados', 'privacidade', 'informaes', 'sensveis', 'privacidade dados', 'informaes sensveis', 'homomrfica', 'ataques', 'proteo', 'pessoais']","['atualmente, a grande gama de plataformas, aplicativos e operações online disponíveis para a resolução de diferentes problemas resulta em um tráfego de grande volume de dados de usuários, inclusive dados sensíveis e de identificação. para proteger a privacidade dos usuários, um direito assegurado por leis em todo o mundo (leis de proteção de dados), é necessária uma atenção maior a esses dados para não serem publicados. no entanto, identificar as informações sensíveis entre tantos outros tipos de dados, pode não ser uma tarefa trivial. estudos já existentes propõem a aplicação de técnicas de processamento de linguagem natural (pln) para identificação automática de informações pessoais identificáveis (personal identifiable information, pii) em documentos em português. o objetivo deste trabalho é propor, através de uma prova de conceito, uma abordagem complementar às utilizadas nos estudos relacionados, através da tarefa de extração de relação de pln. para tal, foi criado um componente que combina um modelo de linguagem especializado na língua portuguesa e camadas adicionais de extração de relação. para o treinamento e avaliação do componente, foi gerada uma base de dados sensíveis sintéticos com o auxílio de um large language model (llm). os resultados foram satisfatórios, com métricas de precisão, recall e f1-score acima de %, indicando que a abordagem pode ser uma boa proposta para detecção automática de informações sensíveis pessoais.', 'o progresso da tecnologia tem gerado uma evolução da popularidade de serviços que utilizam a troca de dados, consentindo que computadores desconhecidos ou que não sejam confiáveis mantenham uma grande quantidade de informações dos indivíduos a partir de seus dados fornecidos. como resultado, manter a privacidade dos usuários e ao mesmo tempo garantir a qualidade dos serviços é um dilema complexo que tem recebido atenção nos últimos anos. a integração dessas diversas bases de dados pode ocorrer por meio da resolução de entidades (re), entretanto em muitas situações esses dados são de caráter privado, algo não suportado pela re. surge então a resolução de entidade com garantia de privacidade (regp) com o mesmo propósito da re, mas com a adição de suporte à privacidade dos dados. uma das técnicas de proteção de dados utilizadas na regp é conhecida como a privacidade diferencial (pd) que consiste em usar mecanismos para adicionar ruído aos registros. este trabalho propõe avaliar a privacidade de dados através de mecanismos de pd aplicados à regp.', 'a divulgação de dados é um processo que ocorre com o objetivo de trazer mais transparência e possibilitar análises de dados em geral. visando garantir a privacidade dos dados, muitas divulgações são feitas anonimizando os registros (de banco de dados) a partir da remoção de informações que identifiquem os indivíduos envolvidos, como é o caso das divulgações dos dados públicos de vacinação contra a covid- . porém, existem ataques que podem ser facilmente realizados em dados anonimizados apenas associando registros, através de atributos comuns com outras divulgações de dados com identificadores que não possuem informações sensíveis. em razão disso, diversas técnicas de anonimização foram desenvolvidos como, por exemplo, a l-diversidade. este artigo tem como objetivo evidenciar o ganho de privacidade aplicando essa técnica sobre os dados de vacinação, em que foram realizados ataques de associação utilizando o perfil de beneficiários do prouni e dados públicos de agendamentos divulgados pela prefeitura municipal de fortaleza. como resultado, foi possível observar um aumento substancial na proteção de informações sensíveis.']"
13,8,13_jogo_jogadores_basquete_nba,"['jogo', 'jogadores', 'basquete', 'nba', 'jogos', 'anlise', 'resultados', 'jogador', 'preditiva', 'dados']","['este artigo apresenta um estudo sobre a predição de resultados em partidas do jogo eletrônico league of legends (lol) utilizando técnicas de aprendizado de máquina. com o objetivo de explorar a capacidade de prever resultados em tempo real, considerando diferentes variáveis e estágios da partida, destacamos o uso de dados inéditos como parte fundamental desse processo. com o aumento da popularidade do lol e a realização de torneios, surgiram também as apostas relacionadas ao jogo, tornando ainda mais relevante a investigação nessa área. diversos modelos foram avaliados e os resultados foram encorajadores. o modelo baseado em random forest obteve o melhor desempenho, alcançando uma acurácia média de , % em estágios intermediários da partida, quando a porcentagem de tempo decorrido estava entre % e %. por outro lado, os modelos de regressão logística e gradient boosting mostraram-se mais eficazes em estágios iniciais do jogo, com resultados promissores. esse estudo contribui para o campo de aprendizado de máquina aplicado a jogos eletrônicos, fornecendo insights valiosos sobre a predição em tempo real no league of legends. os resultados obtidos podem ser relevantes tanto para os jogadores que desejam aprimorar suas estratégias quanto para a indústria de apostas relacionada ao jogo.', 'este artigo oferece um estudo sobre a evolução do basquete brasileiro, empregando índices estatísticos de desempenho (ied) como métrica para avaliar o desempenho das equipes no contexto das três temporadas mais recentes do novo basquete brasil (nbb), que abrangem o período de a . o objetivo central desta pesquisa é explorar e analisar o desempenho das equipes de maior destaque no cenário do basquete brasileiro. os dados utilizados nesta análise foram coletados a partir da plataforma da liga nacional de basquete (lnb), a entidade responsável pela organização e gestão do nbb. foram analisados jogos ao longo dessas três temporadas, abrangendo diversos índices estatísticos, que incluem a média e o desvio padrão de diversos parâmetros, tais como pontuação, arremessos (tentados e convertidos), aproveitamento de arremessos de dois pontos, três pontos e lances livres, assistências, rebotes, roubadas de bola, erros, bloqueios (tocos), eficiência e outros índices avançados associados ao esporte do basquete. os dados foram comparados utilizando a análise de variância (anova) e os valores de referência foram calculados a partir de percentis, permitindo uma análise comparativa e a identificação de tendências. nesse contexto, a temporada de emergiu como aquela que registrou os índices mais baixos, especialmente em termos de aproveitamento de arremessos, eficiência, média de pontos e assistências. por outro lado, as outras duas temporadas analisadas apresentaram números similares na maioria dos índices estudados, o que pode sugerir uma certa estabilidade nesses aspectos ao longo desse período. outro ponto observado no estudo é que as equipes que competem no nbb precisam atingir um alto nível de desempenho, conforme evidenciado pelos percentis utilizados como referência. além disso, o estudo revela uma clara tendência ao aumento do uso de arremessos de três pontos nos últimos anos do nbb, alinhando-se com as mudanças observadas no basquete em nível global, onde a bola de três pontos se tornou um elemento fundamental nas estratégias ofensivas.', 'modelos preditivos em aprendizado de máquina e processos de descoberta de conhecimento em bases de dados, particularmente em domínios como o basquete, são inestimáveis para obter insights sobre o desempenho dos jogadores. este estudo compara abordagens de aprendizado de máquina supervisionado (modelos de caixa preta e caixa branca, incluindo métodos de conjunto) para analisar dados estatísticos de jogadores de basquete universitário (ncaa). nosso objetivo é identificar jogadores da ncaa com alto potencial para sucesso na nba, determinar quais características dos jogadores mais influenciam as decisões de seleção e como esses modelos chegam a tais conclusões para comparar seus desempenhos e a explicabilidade associada. esta tarefa é desafiadora devido a fatores além das estatísticas, como o contexto do jogador e as considerações do elenco da equipe durante a seleção. o objetivo principal é fornecer aos tomadores de decisão insights cruciais para a seleção de jogadores, ajudar na melhor avaliação de jogadores e desenvolver jovens talentos enfatizando aspectos-chave do jogo. comparamos os resultados de modelos de predição interpretáveis com níveis satisfatórios de precisão. equilibrando interpretabilidade e precisão preditiva, empregamos métodos de classificação de caixa branca, caixa preta e de conjunto, como árvores de decisão, regressão logística, máquina de vetores de suporte, perceptron multicamadas, floresta aleatória e xgboost. além disso, algoritmos genéticos foram usados para reduzir o conjunto de características de cada modelo, retendo apenas as características mais impactantes. comparado aos procedimentos padrão sem seleção de características, todos os modelos mostraram desempenho melhorado. encontramos diferenças mínimas na precisão preditiva entre os melhores modelos de caixa branca e caixa preta. a combinação de algoritmos genéticos e regressão logística superou a precisão preditiva de outros modelos, reduzindo significativamente as características e melhorando a interpretabilidade dos resultados. a análise também destaca as características mais influentes no modelo e como os modelos chegaram a tais conclusões.']"
14,8,14_bugs_relatrios_bug_relatrio,"['bugs', 'relatrios', 'bug', 'relatrio', 'relatrios bugs', 'software', 'brs', 'informaes', 'usurios', 'campos']","['mantis bug tracker é uma ferramenta web cujo foco é gerenciar defeitos (bugs) no software durante o seu processo de desenvolvimento e evolução. com ele, tem-se um banco de dados dos defeitos relatados pelos usuários, equipes de desenvolvimento e de qualidade. a motivação para o uso de uma ferramenta como a mantis é que a alta quantidade de bugs em um software gera desgaste para o cliente, desvaloriza o produto, traz custos com retrabalho e riscos de prejuízos financeiros e de negócio. em , por exemplo, um programador da google adicionou uma barra invertida nas urls e o site foi sinalizado como malware por cerca de uma hora, gerando um prejuízo total de quase us$ milhões. neste trabalho, tem-se como objetivo apresentar uma abordagem para suporte à análise de riscos no mantis bug tracker, levantando os pontos críticos de um sistema através da análise dos bugs relatados pelos clientes. com base no estudo desses bugs entender a sua causa e assim criar ações para eliminá-los ou ter uma resposta mais rápida caso eles apareçam. isso trará benefícios para a empresa que o desenvolve em relação a tempo, custo, curva de aprendizado e maior satisfação do cliente.', 'por dia, novos aplicativos são lançados na playstore e com eles inúmeros bugs, definidos como um erro ou falha em um software ou sistema, que causa um resultado inesperado ou incorreto, ou se comporta de maneira não intencional. para controlar esses bugs surgiram os bugs reports, relatórios feitos por usuários ou profissionais contratados para testar uma aplicação. esses relatórios ajudam tanto aos desenvolvedores a dar manutenção no sistema como a organizar a ordem de prioridade e como cada bug será atacado. esses relatórios são muitas vezes criados com base em feedbacks e avaliações, que apontam alguma falha ou problema. esses feedbacks são lidos e interpretados por pessoas de cunho mais técnico, para aí sim se tornarem bug reports, o que gera uma camada a mais entre o cliente final e o desenvolvedor. neste trabalho, propomos uma ferramenta de integração pensada em conectar o aplicativo com uma ferramenta de bug tracking, aproximando mais a relação entre o cliente e o desenvolvedor. o objetivo é oferecer autonomia para o usuário inal do aplicativo criar um bug report, já preenchendo os dados mais comuns e que não demandam um conhecimento técnico, e após isso já gerar um relatório, dando facilidade ao luxo de criação de relatórios de bugs por usuários de uma aplicação e consequentemente a sua investigação e correção pelos desenvolvedores responsáveis.', 'dentre as atividades típicas de um processo de software, podemos destacar as tarefas de testar, analisar, reportar e corrigir bugs. a realização dessas tarefas é importante para identificar erros comuns ou complexos durante todas as etapas do desenvolvimento, evitando retrabalho e entregando um software com mais qualidade e confiabilidade [ ]. em um relatório de bug, geralmente, seu autor oferece detalhes da anormalidade que vem ocorrendo. tipicamente, um relatório de bug é aberto, o bug é corrigido e o relatório é fechado. contudo, por vezes, é verificado que a correção do bug não foi eficaz, seja por falta de descrição mais objetiva no relatório, seja por dificuldade de entendimento por parte do desenvolvedor. assim, faz-se necessária a reabertura do bug, adicionando tempo no processo de desenvolvimento, tornando o software mais custoso. por isso, é importante investigar o que pode ser feito para mitigar tais problemas. neste trabalho, investigamos as características que levam um bug a ser reaberto. os resultados deste trabalho podem ajudar aos usuários finais e desenvolvedores a melhor escrever relatórios de bugs, bem como aos desenvolvedores a melhor entendê-los e tratá-los. o estudo utilizou um dataset extraído da ferramenta bugzilla.']"
15,8,15_parlamentares_eleies_dados_posicionamento,"['parlamentares', 'eleies', 'dados', 'posicionamento', 'transparncia', 'reunies', 'sobre', 'sociedade', 'urnas', 'comisses']","['o poder legislativo no brasil é uma das três funções essenciais do estado. no entanto, há um desafio evidente em relação ao acompanhamento das discussões nos órgãos públicos por parte da população. isso se deve à extensão considerável e ao volume significativo dessas reuniões, tornando-as inacessíveis para muitos cidadãos. para enfrentar esse desafio, este estudo utilizou as notas taquigráficas do senado federal do ano de , que são transcrições dos debates parlamentares, com o objetivo de avaliar o potencial de grandes modelos de linguagem (do inglês, large language models-llms), de detectar tópicos relevantes discutidos pelos parlamentares e o posicionamento deles em relação a esses tópicos, classificando-os como a favor, neutro ou contra. foram realizados experimentos, ambos utilizando o modelo gpt- . -turbo, para as tarefas mencionadas. o primeiro experimento empregou uma técnica de compressão de dados antes de fornecer a entrada para o gpt e abrangeu reuniões de diferentes tamanhos. o segundo experimento não envolveu compressão e focou apenas em reuniões pequenas. os resultados indicam que o modelo teve um desempenho superior para reuniões pequenas. além disso, em um panorama geral para reuniões independentes de tamanho, o modelo teve um desempenho superior na tarefa de detecção de tópicos, com uma precisão média de aproximadamente %, enquanto na detecção de posicionamento teve um desempenho razoável com uma precisão média de aproximadamente %.', 'as redes sociais tornaram-se uma importante plataforma de posicionamento político dos parlamentares fora do congresso brasileiro. especialmente o twitter, que atualmente reúne perfis de % dos parlamentares. apesar dessa importância como difusor não oficial do discurso político, existem poucos estudos que analisam o posicionamento, a atuação e a influência dos parlamentares na plataforma (e.g. parlamentares e partidos mais ativos, engajamento dos parlamentares, proposições mais mencionadas, etc.). desta forma, este trabalho tem como principal objetivo analisar quantitativamente a influência e atuação dos parlamentares brasileiros no twitter, além de seu comportamento na rede. análises dessa natureza possibilitam que a população acompanhe e fiscalize o posicionamento e o comportamento de parlamentares fora do congresso sobre temas fundamentais à sociedade. além do mais, permite compreender como as redes sociais podem ser utilizadas como um elemento fundamental no processo de comunicação entre a sociedade civil e o setor político.', 'após os históricos e correntes eventos políticos envolvendo corrupção e uso inadequado do dinheiro público, a transparência na administração pública tornou-se um tema popular e ações, como vidinha de balada[ ] e brasil.io[ ], foram criadas para fiscalizar e compreender o uso do dinheiro no poder executivo e legislativo. porém, pouco tem sido feito para melhorar a transparência dos gastos no sistema judiciário brasileiro. as conhecidas lei de acesso à informação (brasil, ) e lei de responsabilidade fiscal (brasil, ) exigem que os órgãos públicos disponibilizem seus dados de gastos na internet, mas não especifica a forma como devem ser disponibilizados, levando a dados descentralizados e não estruturados. projetos recentes de recuperação e libertação de dados, como o dadosjusbr [ ], buscam disponibilizar dados estruturados sobre remunerações do sistema judiciário brasileiro. entretanto, mesmo com esse nível de organização, a compreensão da sociedade sobre os dados é limitada. por exemplo, não é trivial entender como é constituída a remuneração de um funcionário, quais benefícios são usados com mais frequência e por quais cargos, se em determinada época do ano é mais comum o uso de algum auxílio, dentre outras informações. isso ocorre devido a escassez de análises descritivas, visualizações e interpretações que mostrem para a sociedade aspectos de como esses gastos são realizados e distribuídos. desta forma, este trabalho tem como objetivo gerar análises descritivas sobre dados de remuneração do sistema judiciário e desenvolver técnicas e resultados que possibilitem uma melhor compreensão de suas características e particularidades, promovendo um maior controle social sobre essas despesas.']"
16,7,16_sangue_brasil_esto satisfeitos_trabalho presencialhbrido,"['sangue', 'brasil', 'esto satisfeitos', 'trabalho presencialhbrido', 'mpes', 'presencialhbrido', 'dados', 'desenvolvimento software', 'esto', 'digital']","['o departamento de informática do sistema único de saúde (datasus) disponibiliza dados fundamentais para análises da situação sanitária do brasil, embasando tomadas de decisão e o desenvolvimento de programas de intervenção em saúde. no entanto, esses dados são fornecidos em um formato não diretamente compatível com ferramentas populares, como excel ou google sheets, dificultando sua acessibilidade para pesquisadores, analistas e profissionais da área médica. além disso, enfrenta-se o desafio atrelado ao significativo volume dos dados, caracterizando-os a nível de big data. a escala massiva desses dados demanda abordagens eficientes de engenharia de dados para lidar com processamento e armazenamento, a fim de proporcionar uma estrutura robusta e escalável para a manipulação dos mesmos. diante dessa problemática, este trabalho propõe o desenvolvimento de um processo automatizado para extração de tabelas, disponíveis em arquivos no formato dbc e provenientes de bases de dados do datasus, o armazenamento em um data warehouse e a disponibilização dos dados por meio de uma api. espera-se que essa iniciativa facilite a análise de dados de saúde no brasil, oferecendo acesso simplificado e dados prontos para análise a pesquisadores, analistas e profissionais da área médica.', 'com o fim da pandemia do covid- , muitas organizações estão considerando a possibilidade de retornar ao trabalho presencial. nesse contexto, é importante avaliar as percepções das equipes de desenvolvimento de software no brasil sobre esse retorno. este estudo investigou como as equipes brasileiras de desenvolvimento de software lidaram com o retorno ao trabalho presencial/híbrido após a pandemia e como essa mudança de trabalho repercutiu no processo de desenvolvimento de software. aplicamos uma pesquisa com participantes de equipes de desenvolvimento de software e investigamos aspectos, como: rotina de trabalho, colaboração, comunicação, produtividade, bem-estar, auxílio oferecido pelas empresas e processo de desenvolvimento de software. realizamos uma análise quantitativa e qualitativa dos resultados da pesquisa e os comparamos com estudos anteriores. nossas principais conclusões sobre os participantes que retornaram ao trabalho presencial/híbrido são: (i) % dos participantes afirmaram que mantém uma rotina de trabalho estável relacionado ao horário padrão da empresa; (ii) , % dos participantes consideram sua equipe colaborativa; (iii) , % estão satisfeitos com a comunicação no regime de trabalho presencial/híbrido em comparação ao work from home (wfh) durante a pandemia de covid- ; (iv) % estão satisfeitos com o seu bem-estar; (v) % estão satisfeitos com sua produtividade; (vi) , % estão satisfeitos com as medidas de segurança adotadas pela empresa no pós pandemia e (vii) , % afirmaram ter mudado no processo de desenvolvimento de software devido ao retorno ao trabalho presencial/híbrido. as principais mudanças positivas no processo estão relacionadas às práticas de: divisão de tarefas, segurança e eficiência.', 'o avanço das tecnologias da informação e comunicação exigem transformações nas organizações em todo o mundo. em paralelo a isso, as micro e pequenas empresas (mpes) cresceram de forma acelerada e correspondem a % dos negócios brasileiros. a crise da covid- escancarou muitos desafios ao microempreendedor; se antes da pandemia ter uma presença digital era fundamental para maximizar as vendas, no período de isolamento social se tornou essencial à sobrevivência das micro e pequenas empresas. entretanto, ainda há muitos desafios a serem ultrapassados para digitalizar a economia e os setores produtivos, por exemplo, a escassez de funcionários com conhecimento e habilidades no ambiente digital, a falta de capital financeiro, dentre outros. este estudo explora os fatores que determinam a falta de democratização e maturidade digital das mpes, bem como indicadores e dados relacionados à questão que auxiliam na compreensão desses desafios. de acordo com a problemática, foi realizada uma pesquisa bibliográfica sobre o assunto, utilizando palavras-chave como diretrizes de busca. além disso, foram obtidos e analisados dados descritivos, qualitativos e quantitativos, obtidos junto aos micro e pequenos empreendedores, através de entrevistas e um questionário. a partir disso, no final do estudo, foi possível fazer um levantamento dos principais desafios da inclusão e maturidade digital das mpes e apresentar algumas das ferramentas motivacionais necessárias para mitigar esses obstáculos, por exemplo, a necessidade de uma educação digital complementar para os funcionários das mpes.']"
17,7,17_reviso_cdigo_comentrios_prs,"['reviso', 'cdigo', 'comentrios', 'prs', 'reviso cdigo', 'refatoramentos', 'comentrios reviso', 'compilao', 'erros compilao', 'code']","['os comentários feitos em pull requests (prs) de repositórios git, podem induzir a refatoramentos, que são melhorias feitas no código-fonte, preservando o seu comportamento. com base nos comentários de revisão, desenvolvedores são capazes de decidir por refatorar o código-fonte. este trabalho tem como objetivo analisar prs de repositórios java do projeto apache [ ] no github à luz dos comentários dos revisores e refatoramentos realizados em tempo de revisão de código (minerados previamente pelo refactoringminer - uma ferramenta estado-da-arte para a detecção de refatoramentos). para tanto, propõem-se uma análise manual de comentários de revisão em uma base de dados que contém prs. foram definidos pontos específicos a serem analisados em cada comentário, buscando atingir objetivos específicos definidos para ajudar na pesquisa. essa análise foi feita de forma manual, lendo, caracterizando e verificando o comportamento desses pontos levantados e fazendo anotações sobre cada pull request. ao final deste trabalho foram notados diversos padrões que se repetiam entre os prs, esses padrões foram compilados e serão apresentados, e com isso espera-se entender de que forma os comentários de revisão influenciam nos refatoramentos de código-fonte.', 'a compilação é um processo essencial no desenvolvimento de linhas de produto de software, como o linux. entretanto, identificar erros de compilação em linhas de produto de software (lps) não é trivial, já que os compiladores tradicionais não são conscientes de variação. abordagens anteriores foram propostas que identificam alguns desses erros de compilação usando técnicas avançadas que requerem um esforço dos programadores em usarem. este estudo avalia a eficácia de modelos de linguagem de grande porte (llms), especificamente o chatgpt e le chat mistral, na identificação de erros de compilação em lps. inicialmente foram testados produtos nas linguagens c++, java e c, e posteriormente lps em c, abrangendo tipos diferentes de erros de compilação. os dois llms foram avaliados com base na sua capacidade de reconhecer e diagnosticar corretamente os erros. o chatgpt conseguiu identificar % e % dos erros de compilação em produtos e lps, enquanto que o le chat mistral obteve % e %, respectivamente. a análise revelou que, embora os llms possam identificar uma gama de erros de compilação, desafios específicos permanecem, especialmente em ambientes de lps com alta variabilidade. o estudo sugere a necessidade de refinamentos contínuos nos modelos de llm para melhorar sua precisão e utilidade em cenários de desenvolvimento de software complexos.', 'com o passar dos anos, a revisão de código vem mudando; antes, era feita uma inspeção manual (rigorosa e síncrona), já nos dias atuais, é feita uma revisão mais moderna (assíncrona e menos rigorosa). atualmente, o git, através da plataforma github, é o sistema de controle de versões mais popular, favorecendo diversas discussões relacionadas a mudanças no código-fonte. com o auxílio de ferramentas como refactoringminer, que fornece a detecção de refatoramentos aplicados aos códigos-fonte e, utilizando-se de uma amostra de repositórios provenientes do projeto do apache no github, este trabalho, através de inspeções manuais de comentários de revisão, visa entender e caracterizar os comentários que induziram refatoramentos nos prs, com o intuito de entender as características próprias e diferenças dos comentários de revisão em prs com e sem refatoramentos. através das hipóteses levantadas, tentamos complementar o entendimento da parte qualitativa dos comentários de revisão, abordados anteriormente de forma similar em outra pesquisa, que analisava dados qualitativos e quantitativos de prs que induziram refatoramentos e de prs que não induziram refatoramentos, com a intenção de entender melhor as diferenças entre os dois tipos de prs, no nível de pull request.']"
18,6,18_animais_animal_gua_pets,"['animais', 'animal', 'gua', 'pets', 'domsticos', 'abandonados', 'nvel gua', 'lar', 'resgate', 'animais domsticos']","['o resgate e tratamento de animais abandonados é uma questão de suma importância do ponto de vista social, do bem estar animal e da saúde pública. em muitas situações o intervalo entre o resgate do animal e sua adoção pode ser bastante longo, o que dificulta o resgate devido a superlotação ou falta de espaço físico para abrigar os animais. uma alternativa nessas situações é buscar um lar temporário que possa cuidar do animal até que um lar permanente e responsável seja encontrado. para auxiliar nessa tarefa foi desenvolvido um website com o objetivo de facilitar a busca por lares temporários, utilizando o framework react para o frontend (a interface gráfica de usuário) e o baas (backend as a service) firebase. os usuários deste website são os projetos/pessoas comuns que prestam apoio a animais abandonados e voluntários que estão dispostos a oferecer um lar. ao final do desenvolvimento foi feita uma pesquisa de satisfação com potenciais usuários. no geral, o site foi bem aceito e a pesquisa também apontou áreas em que o site deve melhorar.', 'atualmente, é muito comum que donos de animais domésticos (pets) por vezes esqueçam de repor a água dos recipientes dos seus animais logo que esta acaba. com isto, é comum que seus pets fiquem muito tempo sem beber água, correndo o risco de até mesmo sofrer desidratação. assim, o trabalho ora descrito consiste no desenvolvimento de um sistema para monitoramento remoto do nível de água em recipientes de animais domésticos. o sistema pode ser utilizado para animais de pequeno porte (cães pequenos e gatos), até animais maiores e, de outros contextos, como por exemplo, cavalos e bois.para tanto, foi desenvolvida uma solução hardware e software, com sensor de nível de água e dispositivo da família esp32, a partir da qual, no momento em que o nível da água atinge certo ponto, uma notificação é enviada para dispositivos móveis, como smartphones. espera-se, que este sistema auxilie os donos de pets a manter o recipiente de água de seus animais sempre abastecido, evitando assim que seus pets fiquem sem água por tempo prolongado.', 'um grande problema das cidades brasileiras é o alto índice de animais abandonados, que segundo uma pesquisa da oms existem mais de milhões de cães sem lares no brasil [ ]. além desse número muito alto, há um outro problema na adoção que é adotar um animal, não se adaptar ao seu comportamento, temperamento daquele animal, então algumas pessoas acabam abandonando esses animais, novamente. existem ongs e pessoas que gostariam de fazer a diferença nesse meio. nessa plataforma, auxiliaremos de algumas formas, como: ajudar na avaliação do adotante e do pet, mostrar os animais disponíveis para adoção podendo classificar e/ou filtrar, além disso abrir um canal de comunicação entre as organizações e os adotantes. para o desenvolvimento desta plataforma, será utilizada uma biblioteca de javascript bem conhecida no mercado, o react-native [ ]. espera-se que o aplicativo sirva para disseminar a ideia da adoção de animais, além de ajudar a evitar a reintrodução desse animal ao abandono e assim aumentar o número de animais adotados.']"
19,5,19_threads_testes_angular_virtual threads,"['threads', 'testes', 'angular', 'virtual threads', 'virtual', 'componentes', 'implementao', 'biblioteca', 'java', 'reduo']","['testes automáticos em aplicações web são amplamente adotados devido à sua eficiência e relação custo-benefício. quando realizados pela gui, esses testes podem simular cenários de uso, com o objetivo de expor falhas visíveis. o scriptless testing, uma abordagem que gera e executa casos de teste automaticamente, pode adotar uma exploração sistemática da gui. entretanto, essa abordagem pode demandar um tempo significativo em aplicações web, principalmente numa execução com repetição em visitas de estados. neste estudo, apresentamos o algoritmo iterative deepening url-based search (idubs), que se associado ao uso de testes de gui, resulta em uma exploração otimizada, utilizada para a redução da redundância em visita de estados. avaliamos a eficácia do algoritmo em um estudo empírico com quatro sistemas web de código aberto, analisando sua contribuição para otimização do tempo de execução e redução da redundância em visita de estados. o idubs reduziu o tempo de execução em , % e a redundância de casos de teste em , %. em suma, o algoritmo idubs apresenta uma solução promissora para otimizar a execução de testes em aplicações web, oferecendo benefícios significativos em termos de eficiência e redução de redundâncias.', 'green thread é um modelo de thread no qual o escalonamento é realizado por runtimes ao invés de ser realizado pelo sistema operacional. essa abordagem necessita de menos recursos, em termos de memória e ciclos de cpu, do que projetos tradicionais de threads. recentemente, a linguagem java, em sua versão , introduziu uma implementação de green threads, chamada de virtual threads. por ser ainda pouco utilizada, conhecemos pouco da eficiência desta implementação. neste trabalho, avaliamos o desempenho da implementação de green threads de java. comparamos os resultados desta implementação com a implementação padrão já disponível na linguagem. ao término deste trabalho, foi evidenciado que as virtual threads apresentam uma significativa melhoria de desempenho em comparação com a abordagem convencional de threads em java, sendo superior em todos os testes realizados. os resultados deste estudo destacaram a disparidade de desempenho entre diferentes abordagens de manipulação de threads, com as virtual threads mostrando um desempenho excepcional em comparação com as threads convencionais em java. ficou claro que sistemas que possuem um grande número de threads, podem obter melhorias significativas de desempenho ao implementar as virtual threads em sua estrutura.', 'objetos sintéticos são utilizados no desenvolvimento de software para simular o comportamento de objetos reais de forma controlada. durante o desenvolvimento de servidores back-end, existe uma constante necessidade de modelar testes com objetos simulados para garantir o funcionamento adequado de funcionalidades dos mesmos. todavia, essa tarefa acaba por se tornar repetitiva e complicada conforme a aplicação cresce. ademais, testes que envolvam endpoints de aplicações se mostram mais complexos por ter necessidade de modelar requisições completas do tipo http. a biblioteca mocktests tem por finalidades principais promover uma elaboração de testes sem a necessidade de inserção repetitiva de alguns componentes a cada caso de teste, simplificando assim diversos aspectos e solucionando empecilhos que podem surgir durante o desenvolvimento. este presente trabalho tem por finalidade relatar as etapas do desenvolvimento da biblioteca mocktests, concebida para solucionar percalços durante a criação de testes envolvendo endpoints de aplicações. os resultados obtidos demonstraram como a ferramenta se adequou ao ser usada em testes de uma aplicação backend real, e como a mesma solucionou diversos problemas encontrados na logística de construção dos respectivos testes.']"
20,5,20_violncia_sade_pandemia_cuidadores,"['violncia', 'sade', 'pandemia', 'cuidadores', 'vrus', 'zika', 'contra mulher', 'agentes vigilncia', 'mulher', 'vigilncia']","['a violência contra a mulher é um problema de saúde pública, com índices alarmantes no brasil. no contexto da pandemia de coronavírus, espera-se que tal problemática pode se agravar tendo em vista que as medidas de isolamento social, em que homens e mulheres estão convivendo por mais tempo em suas casas, podem tornar os lares com histórico de violência doméstica em um verdadeiro pesadelo para essas mulheres. dados da onu indicam, que durante este período de quarentena, os índices de violência doméstica aumentaram. por este motivo, a partir da coleta de dados públicos, técnicas de análise e visualização de dados foram aplicadas para estudar os indicadores de violência contra a mulher antes e durante o período de isolamento social, com o objetivo de entender os impactos da pandemia de covid- nos casos de violência contra a mulher no brasil.', 'os agentes de vigilância ambiental em saúde exercem um papel fundamental no controle da proliferação dos mosquitos transmissores de doenças como a dengue, a zika e a chikungunya. esses profissionais atuam no combate das arboviroses através de ações de campo, visitas domiciliares e comunitárias. essas atividades têm como principal função a identificação e o tratamento de focos de reprodução dos vetores. atualmente, o registro dos dados das inspeções realizadas pelos agentes de vigilância da cidade de campina grande-pb, é feito utilizando um formulário de papel. porém, esse formato de anotação pode apresentar diversos problemas na organização, na visualização e na segurança das informações. sendo assim, visando auxiliar o serviço desses profissionais, este trabalho tem como objetivo desenvolver a primeira versão de uma plataforma web responsiva, para o registro e acompanhamento das pesquisas entomológicas realizadas pelo agente, o mosquito zero. a primeira versão da aplicação foi testada com alguns agentes de vigilância da cidade de cabedelo no estado da paraíba, a fim de obter a avaliação desses usuários referente à usabilidade do sistema.', 'em outubro de , a secretaria de saúde de pernambuco emitiu uma declaração de estado de emergência para uma epidemia de microcefalia. em novembro do mesmo ano foi identificado o vírus da zika (zikv) como responsável pela microcefalia ocorrida em bebês. nesse contexto, o cer (centro especializado em reabilitação) da cidade de campina grande-pb, atua na reabilitação de crianças com microcefalia por zika vírus (msc-zikv). parte do tratamento é conduzido por meio de procedimentos de estimulação motora a serem continuados em ambiente domiciliar. após entrevistas e visitas realizadas ao cer, descobriu-se que esses cuidados não vêm sendo feitos de forma adequada, havendo uma diferença entre as tarefas prescritas e as que são realizadas de fato pelos cuidadores. tais diferenças podem comprometer a eficácia do tratamento e prejudicar a evolução do paciente. este trabalho tem como objetivo o desenvolvimento de uma aplicação capaz de auxiliar os cuidadores no processo de estimulação motora de crianças com msc-zikv. a aplicação foi desenvolvida em duas plataformas distintas, sendo elas web e mobile. a aplicação web tem como objetivo possibilitar aos profissionais de saúde do cer o cadastro de atividades e prescrevêlas aos seus pacientes. já a parte mobile é usada pelos cuidadores, a fim de obter melhores instruções quanto às atividades a serem executadas em ambiente domiciliar e o registro dessas atividades.']"
21,5,21_geis_software_projetos_atomic,"['geis', 'software', 'projetos', 'atomic', 'metodologias', 'receitas', 'modelos', 'metodologias geis', 'componentes', 'desenvolvimento software']","['""um estudo sobre metodologias ágeis e sua correlação com o êxito em projetos de software"" investiga a eficácia das metodologias ágeis em projetos de desenvolvimento de software e sua relação com o sucesso. o estudo aprofunda a compreensão das práticas e princípios fundamentais das metodologias ágeis, como o manifesto ágil e os princípios ágeis. além disso, a pesquisa analisa como diversos fatores, como o apoio executivo, a maturidade emocional das equipes, o envolvimento dos usuários, a otimização de recursos, a qualificação dos membros da equipe e outros elementos, impactam a taxa de sucesso de projetos ágeis. também são investigados os obstáculos e desafios comuns que as organizações enfrentam ao implementar essas metodologias, incluindo a resistência à mudança e conflitos culturais. o trabalho enfatiza a relevância das metodologias ágeis na melhoria do desempenho em projetos de software e fornece diretrizes valiosas para organizações que buscam adotar ou aprimorar suas práticas ágeis. o estudo oferece uma visão abrangente do universo ágil, abordando suas vantagens e desafios, e apresenta recomendações práticas para aprimorar a implementação de abordagens ágeis nas empresas de desenvolvimento de software.', 'a importância do atomic design no desenvolvimento de software é pouco discutida no meio acadêmico, mas é importante destacar o valor de um sistema de design que estabelece diretrizes para criar interfaces de usuário e facilita o desenvolvimento de aplicações web. o atomic design permite construir um sistema de componentes que seguem uma hierarquia bem deinida, resultando na construção de interfaces lexíveis. porém, essa lexibilidade e escalabilidade podem não ser tão simples de serem implementadas e possuem algumas limitações. diante disto, com o intuito de observar de que maneira o atomic design pode ser utilizado como uma metodologia que facilita a forma de pensamento e organização dos projetos, este trabalho relata a experiência de adaptar as diretrizes do atomic de- sign em uma aplicação conidencial de um e-commerce; trata-se de um projeto interno de gerenciamento e integração de apis de uma empresa privada. além disso, são propostas alguns critérios gerais de avaliação para realização da análise da estratégia. o objetivo geral é destacar fatores de sucesso, diiculdades e pontos negativos do modelo diante da observação de sua aplicação na prática.', 'recipemaster é um sistema de gerenciamento voltado para chefs de cozinha, facilitando o controle de receitas, ingredientes e cardápios. ele permite aos usuários manter um registro digital detalhado de suas receitas, bem como dos ingredientes utilizados. além disso, possui uma funcionalidade especial de criação de cardápios, onde cada item e custo adicional podem ser especificados, gerando uma estimativa de custo baseada nos ingredientes. desenvolvido usando o framework django para o back-end e angular para o front-end, o recipemaster combina a segurança e facilidade de uso do django com a capacidade do angular de criar interfaces de usuário interativas. a principal contribuição deste trabalho é oferecer uma solução eficiente para o gerenciamento de receitas, otimizando a organização na cozinha, reduzindo o desperdício e melhorando a qualidade dos pratos. futuros aprimoramentos podem incluir recursos de aprendizado de máquina para previsão de demanda e sugestão de receitas.']"
