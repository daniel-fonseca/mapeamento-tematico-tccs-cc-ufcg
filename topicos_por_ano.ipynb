{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c78835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e paths\n",
    "from pathlib import Path\n",
    "import json, ast, re, unicodedata\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Este notebook deve estar na RAIZ do projeto\n",
    "ROOT = Path.cwd()\n",
    "DATA = ROOT / \"data\"\n",
    "PROCESSED = DATA / \"processed\"\n",
    "INTERIM = DATA / \"interim\"\n",
    "RESULTS = ROOT / \"results\"\n",
    "EXPORT = DATA / \"exports\" / \"dashboard\"\n",
    "\n",
    "EXPORT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitários\n",
    "\n",
    "def _latest_compare_csv(results_root: Path) -> Path | None:\n",
    "    comp = results_root / \"comparison\"\n",
    "    if not comp.exists(): return None\n",
    "    runs = sorted(comp.glob(\"run_*\"))\n",
    "    if not runs: return None\n",
    "    csv = runs[-1] / \"all_trials_gr_rz.csv\"\n",
    "    return csv if csv.exists() else None\n",
    "\n",
    "def _pick_best_bertopic(csv: Path, max_outliers=0.30, k_min=5):\n",
    "    df = pd.read_csv(csv)\n",
    "    cand = df[(df[\"method\"]==\"bertopic\") & (df[\"K\"]>=k_min)].copy()\n",
    "    if \"outliers_pct\" in cand.columns:\n",
    "        cand = cand[cand[\"outliers_pct\"] <= max_outliers]\n",
    "    if cand.empty:\n",
    "        cand = df[df[\"method\"]==\"bertopic\"].copy()\n",
    "    cand = cand.sort_values([\"RZ_index\",\"GR_index\"], ascending=False)\n",
    "    assert not cand.empty, \"Nenhum trial BERTopic encontrado no CSV.\"\n",
    "    return cand.iloc[0].to_dict()\n",
    "\n",
    "def _trial_dir_bertopic(bt_root: Path, run: str, trial: str) -> Path:\n",
    "    if isinstance(run, str) and run.startswith(\"run_\"):\n",
    "        return bt_root / run / trial\n",
    "    return bt_root / trial\n",
    "\n",
    "def _parse_representation(val):\n",
    "    if isinstance(val, (list, tuple)): return [str(x) for x in val]\n",
    "    try:\n",
    "        lst = ast.literal_eval(str(val))\n",
    "        if isinstance(lst, (list, tuple)):\n",
    "            return [str(x) for x in lst]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [t for t in str(val).split() if t]\n",
    "\n",
    "def _load_topic_labels(tdir: Path, topn_terms=5):\n",
    "    labels = {}\n",
    "    tip = tdir / \"topic_info.csv\"\n",
    "    if tip.exists():\n",
    "        df = pd.read_csv(tip)\n",
    "        if \"Topic\" in df.columns:\n",
    "            for _, row in df.iterrows():\n",
    "                tid = int(row[\"Topic\"])\n",
    "                if tid == -1: \n",
    "                    continue\n",
    "                name = str(row.get(\"Name\", \"\")).strip()\n",
    "                if name:\n",
    "                    labels[tid] = name\n",
    "                else:\n",
    "                    reps = _parse_representation(row.get(\"Representation\", \"\"))\n",
    "                    if reps:\n",
    "                        labels[tid] = \", \".join(reps[:topn_terms])\n",
    "    # fallback com c_tf_idf + vocab\n",
    "    missing = [t for t in range(max(labels.keys())+1 if labels else 0) if t not in labels]\n",
    "    ctf_p, voc_p = tdir/\"c_tf_idf.npy\", tdir/\"vocab.txt\"\n",
    "    if missing and ctf_p.exists() and voc_p.exists():\n",
    "        ctf = np.load(ctf_p)\n",
    "        vocab = [line.strip() for line in open(voc_p, encoding=\"utf-8\")]\n",
    "        for tid in range(ctf.shape[0]):\n",
    "            if tid in labels: continue\n",
    "            row = ctf[tid]\n",
    "            idx = np.argsort(-row)[:topn_terms]\n",
    "            labels[tid] = \", \".join(vocab[i] for i in idx)\n",
    "    return labels\n",
    "\n",
    "def _keyify_name(x: str) -> str:\n",
    "    x = str(x).strip()\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = re.sub(r\"\\s+([,.;:])\", r\"\\1\", x).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "    x = unicodedata.normalize(\"NFKD\", x)\n",
    "    x = \"\".join(c for c in x if not unicodedata.combining(c))\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"[^a-z0-9 ,.\\-]\", \"\", x)\n",
    "    return re.sub(r\"\\s+\", \" \", x).strip()\n",
    "\n",
    "def _detect_year_column(df: pd.DataFrame) -> str:\n",
    "    # tenta achar 'ano' ou 'year' em qualquer caixa/variação\n",
    "    for c in df.columns:\n",
    "        cl = c.lower().strip()\n",
    "        if cl in {\"ano\",\"year\",\"ano_defesa\",\"ano_defesa_tcc\",\"ano_publicacao\"}: \n",
    "            return c\n",
    "        if \"ano\" in cl or \"year\" in cl:\n",
    "            return c\n",
    "    raise AssertionError(\"Coluna de ano não encontrada no prep.csv. Esperado algo contendo 'ano' ou 'year'.\")\n",
    "\n",
    "def _year_to_int(s):\n",
    "    try:\n",
    "        return int(str(s)[:4])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _soft_prob(counts: pd.Series | np.ndarray, alpha=0.5):\n",
    "    arr = np.asarray(counts, dtype=float)\n",
    "    arr = arr + alpha\n",
    "    s = arr.sum()\n",
    "    return arr / s if s > 0 else arr * np.nan\n",
    "\n",
    "def _jsd(p, q, eps=1e-12):\n",
    "    p = np.asarray(p, dtype=float); q = np.asarray(q, dtype=float)\n",
    "    p = p / max(p.sum(), eps); q = q / max(q.sum(), eps)\n",
    "    m = 0.5*(p+q)\n",
    "    def _kl(a, b):\n",
    "        a = np.where(a<=0, eps, a)\n",
    "        b = np.where(b<=0, eps, b)\n",
    "        return np.sum(a * np.log(a/b))\n",
    "    return float(np.sqrt(0.5*_kl(p,m) + 0.5*_kl(q,m)))\n",
    "\n",
    "def _ols_slope(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    x = np.asarray(x).reshape(-1,1)\n",
    "    y = np.asarray(y).reshape(-1,1)\n",
    "    if len(x) < 2: return np.nan\n",
    "    model = LinearRegression().fit(x, y)\n",
    "    return float(model.coef_[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleção automática do melhor BERTopic\n",
    "compare_csv = _latest_compare_csv(RESULTS)\n",
    "assert compare_csv and compare_csv.exists(), \"Rode compare_models antes (CSV de comparação não encontrado).\"\n",
    "\n",
    "best = _pick_best_bertopic(compare_csv, max_outliers=0.30, k_min=5)\n",
    "best_run, best_trial, K = best[\"run\"], best[\"trial\"], int(best[\"K\"])\n",
    "print(\"Selecionado:\", best_run, best_trial, \"| K =\", K)\n",
    "\n",
    "BT_ROOT = PROCESSED / \"bertopic\"\n",
    "TRIAL_DIR = _trial_dir_bertopic(BT_ROOT, best_run, best_trial)\n",
    "assert TRIAL_DIR.exists(), f\"Pasta do trial não encontrada: {TRIAL_DIR}\"\n",
    "print(\"TRIAL_DIR:\", TRIAL_DIR)\n",
    "\n",
    "# diretório de export desta análise temporal\n",
    "OUT_DIR = EXPORT / \"bertopic_time\" / str(best_run) / str(best_trial)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95886323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega meta e doc→tópico\n",
    "prep_csv = INTERIM / \"bertopic\" / \"prep.csv\"\n",
    "assert prep_csv.exists(), f\"Meta não encontrada: {prep_csv}\"\n",
    "\n",
    "meta = pd.read_csv(prep_csv, encoding=\"utf-8\")\n",
    "assert \"DOC_ID\" in meta.columns, \"prep.csv precisa conter coluna DOC_ID\"\n",
    "\n",
    "# detecta ano e orientador\n",
    "col_year = _detect_year_column(meta)\n",
    "cand_orient_cols = [c for c in meta.columns if \"orient\" in c.lower()]\n",
    "assert cand_orient_cols, \"Coluna de orientador não encontrada (esperado algo contendo 'orient').\"\n",
    "col_orient = cand_orient_cols[0]\n",
    "\n",
    "meta = meta[[\"DOC_ID\", col_year, col_orient]].rename(columns={col_year:\"ANO\", col_orient:\"orientador\"})\n",
    "meta[\"DOC_ID\"] = meta[\"DOC_ID\"].astype(int)\n",
    "meta[\"ANO\"] = meta[\"ANO\"].map(_year_to_int).astype(\"Int64\")\n",
    "meta[\"orientador\"] = meta[\"orientador\"].astype(str).fillna(\"NA\").str.strip()\n",
    "\n",
    "# normaliza orientadores (consolida variações)\n",
    "meta[\"_key\"] = meta[\"orientador\"].map(_keyify_name)\n",
    "name_map = meta.groupby(\"_key\")[\"orientador\"].agg(lambda s: s.mode().iat[0] if not s.mode().empty else s.iloc[0]).to_dict()\n",
    "meta[\"orientador\"] = meta[\"_key\"].map(name_map)\n",
    "meta = meta.drop(columns=[\"_key\"]).copy()\n",
    "\n",
    "# Doc→tópico (ignora outliers -1)\n",
    "labels = pd.read_csv(TRIAL_DIR / \"doc_topics.csv\", encoding=\"utf-8\")\n",
    "assert {\"DOC_ID\",\"topic\"}.issubset(labels.columns), \"doc_topics.csv precisa conter DOC_ID e topic\"\n",
    "labels[\"DOC_ID\"] = labels[\"DOC_ID\"].astype(int)\n",
    "labels = labels[labels[\"topic\"] != -1].copy()\n",
    "\n",
    "# junção\n",
    "df = labels.merge(meta, on=\"DOC_ID\", how=\"inner\")\n",
    "assert not df.empty, \"Junção vazia; verifique DOC_ID/ANO entre prep.csv e doc_topics.csv\"\n",
    "\n",
    "topic_labels = _load_topic_labels(TRIAL_DIR, topn_terms=5)\n",
    "print(\"Intervalo de anos:\", int(df[\"ANO\"].min()), \"→\", int(df[\"ANO\"].max()))\n",
    "print(\"Docs com tópico:\", len(df), \"| K:\", K, \"| Orientadores:\", df[\"orientador\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contagens por ano/tópico e totais por ano\n",
    "ct_ty = df.groupby([\"ANO\",\"topic\"]).size().rename(\"n\").reset_index()\n",
    "ct_y  = df.groupby(\"ANO\").size().rename(\"n_docs_assigned\")  # só docs com tópico (≠ -1)\n",
    "\n",
    "# total de docs por ano (base completa)\n",
    "year_tot = meta.groupby(\"ANO\").size().rename(\"n_docs_total\")\n",
    "\n",
    "# matriz P(t|ano) com suavização\n",
    "pivot_ty = ct_ty.pivot(index=\"ANO\", columns=\"topic\", values=\"n\").fillna(0.0).sort_index()\n",
    "p_t_given_year = pivot_ty.apply(lambda row: _soft_prob(row.values, alpha=0.5), axis=1, result_type=\"broadcast\")\n",
    "p_t_given_year.index = pivot_ty.index\n",
    "p_t_given_year.columns = pivot_ty.columns\n",
    "\n",
    "# tabela longa\n",
    "long_topic_year = pivot_ty.stack().rename(\"n_topic\").reset_index()\n",
    "long_topic_year[\"p_topic_given_year\"] = [\n",
    "    float(p_t_given_year.loc[y, t]) for y, t in zip(long_topic_year[\"ANO\"], long_topic_year[\"topic\"])\n",
    "]\n",
    "long_topic_year[\"topic_label\"] = long_topic_year[\"topic\"].map(lambda t: topic_labels.get(int(t), f\"topic_{t}\"))\n",
    "\n",
    "# cobertura por ano\n",
    "coverage_year = pd.concat([year_tot, ct_y], axis=1).fillna(0)\n",
    "coverage_year[\"coverage\"] = coverage_year[\"n_docs_assigned\"] / coverage_year[\"n_docs_total\"].replace(0, np.nan)\n",
    "coverage_year = coverage_year.reset_index().rename(columns={\"index\":\"ANO\"})\n",
    "\n",
    "display(long_topic_year.head(10))\n",
    "display(coverage_year.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ce15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tendências por tópico: slope (OLS) de p(t|ano), delta, \"CAGR\" aprox., Spearman rho\n",
    "\n",
    "years = np.array(sorted(long_topic_year[\"ANO\"].dropna().unique()), dtype=float)\n",
    "trend_rows = []\n",
    "for t in sorted(p_t_given_year.columns):\n",
    "    series = p_t_given_year[t].dropna()\n",
    "    yy = series.index.values.astype(float)\n",
    "    pp = series.values.astype(float)\n",
    "\n",
    "    if len(pp) < 2:\n",
    "        slope = rho = np.nan\n",
    "        delta = np.nan\n",
    "        cagr = np.nan\n",
    "    else:\n",
    "        slope = _ols_slope(yy, pp)                     # variação de proporção por ano\n",
    "        delta = float(pp[-1] - pp[0])                  # variação absoluta no período\n",
    "        # CAGR aproximado sobre proporções, com epsilon para zeros\n",
    "        eps = 1e-6\n",
    "        years_span = max(int(yy[-1]-yy[0]), 1)\n",
    "        cagr = float(((pp[-1]+eps)/(pp[0]+eps))**(1/years_span) - 1)\n",
    "        rho, _ = spearmanr(yy, pp, nan_policy=\"omit\")\n",
    "\n",
    "    trend_rows.append({\n",
    "        \"topic\": int(t),\n",
    "        \"topic_label\": topic_labels.get(int(t), f\"topic_{t}\"),\n",
    "        \"years_min\": int(yy[0]) if len(series)>0 else np.nan,\n",
    "        \"years_max\": int(yy[-1]) if len(series)>0 else np.nan,\n",
    "        \"p_first\": float(pp[0]) if len(series)>0 else np.nan,\n",
    "        \"p_last\": float(pp[-1]) if len(series)>0 else np.nan,\n",
    "        \"delta\": delta,\n",
    "        \"slope_per_year\": slope,\n",
    "        \"cagr_approx\": cagr,\n",
    "        \"spearman_rho\": float(rho) if 'rho' in locals() else np.nan\n",
    "    })\n",
    "\n",
    "topic_trends = pd.DataFrame(trend_rows).sort_values([\"slope_per_year\",\"delta\"], ascending=False)\n",
    "display(topic_trends.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(tópico|orientador, ano). Exigimos um mínimo de docs/ano para estabilidade.\n",
    "\n",
    "MIN_DOCS_ORIENTADOR_ANO = 3\n",
    "alpha = 0.5  # suavização das probabilidades\n",
    "\n",
    "# contagens por orientador/ano/tópico\n",
    "ct_oyt = df.groupby([\"orientador\",\"ANO\",\"topic\"]).size().rename(\"n\").reset_index()\n",
    "ct_oy  = df.groupby([\"orientador\",\"ANO\"]).size().rename(\"n_oy\").reset_index()\n",
    "\n",
    "# somente cels com pelo menos MIN_DOCS_ORIENTADOR_ANO\n",
    "valid_pairs = set(\n",
    "    ct_oy[ct_oy[\"n_oy\"] >= MIN_DOCS_ORIENTADOR_ANO][[\"orientador\",\"ANO\"]] \\\n",
    "        .itertuples(index=False, name=None)\n",
    ")\n",
    "\n",
    "rows_p = []\n",
    "for (o, y), block in ct_oyt.groupby([\"orientador\",\"ANO\"]):\n",
    "    if (o, y) not in valid_pairs:\n",
    "        continue\n",
    "    # vetor em todos os tópicos (0..K-1)\n",
    "    counts = np.zeros(K, dtype=float)\n",
    "    for _, r in block.iterrows():\n",
    "        counts[int(r[\"topic\"])] = r[\"n\"]\n",
    "    probs = _soft_prob(counts, alpha=alpha)\n",
    "    for t in range(K):\n",
    "        rows_p.append({\n",
    "            \"orientador\": o,\n",
    "            \"ANO\": int(y),\n",
    "            \"topic\": int(t),\n",
    "            \"topic_label\": topic_labels.get(int(t), f\"topic_{t}\"),\n",
    "            \"P_topic_given_orientador_year\": float(probs[t]),\n",
    "            \"n_docs_orientador_year\": int(ct_oy[(ct_oy[\"orientador\"]==o)&(ct_oy[\"ANO\"]==y)][\"n_oy\"].iloc[0])\n",
    "        })\n",
    "\n",
    "orientador_topic_year = pd.DataFrame(rows_p).sort_values([\"orientador\",\"ANO\",\"P_topic_given_orientador_year\"], ascending=[True, True, False])\n",
    "display(orientador_topic_year.head(10))\n",
    "\n",
    "# Top topic por orientador/ano\n",
    "top_by_year = (\n",
    "    orientador_topic_year\n",
    "    .sort_values([\"orientador\",\"ANO\",\"P_topic_given_orientador_year\"], ascending=[True, True, False])\n",
    "    .groupby([\"orientador\",\"ANO\"], as_index=False)\n",
    "    .first()[[\"orientador\",\"ANO\",\"topic\",\"topic_label\",\"P_topic_given_orientador_year\",\"n_docs_orientador_year\"]]\n",
    ")\n",
    "display(top_by_year.head(10))\n",
    "\n",
    "# Métrica de mudança de preferência: nº de trocas do top topic e JSD médio entre anos consecutivos\n",
    "shift_rows = []\n",
    "for o, g in orientador_topic_year.groupby(\"orientador\"):\n",
    "    years_sorted = sorted(g[\"ANO\"].unique())\n",
    "    if len(years_sorted) < 2: \n",
    "        continue\n",
    "\n",
    "    # seq de top topics\n",
    "    tops = top_by_year[top_by_year[\"orientador\"]==o].sort_values(\"ANO\")\n",
    "    seq = tops[\"topic\"].tolist()\n",
    "    switches = int(np.sum(np.array(seq[1:]) != np.array(seq[:-1])))\n",
    "\n",
    "    # JSD médio e máximo entre distribuições consecutivas P(t|o,ano)\n",
    "    jsds = []\n",
    "    for y1, y2 in zip(years_sorted[:-1], years_sorted[1:]):\n",
    "        p1 = orientador_topic_year[(orientador_topic_year[\"orientador\"]==o)&(orientador_topic_year[\"ANO\"]==y1)] \\\n",
    "                .sort_values(\"topic\")[\"P_topic_given_orientador_year\"].values\n",
    "        p2 = orientador_topic_year[(orientador_topic_year[\"orientador\"]==o)&(orientador_topic_year[\"ANO\"]==y2)] \\\n",
    "                .sort_values(\"topic\")[\"P_topic_given_orientador_year\"].values\n",
    "        if len(p1)==K and len(p2)==K:\n",
    "            jsds.append(_jsd(p1, p2))\n",
    "    mean_jsd = float(np.mean(jsds)) if jsds else np.nan\n",
    "    max_jsd  = float(np.max(jsds))  if jsds else np.nan\n",
    "\n",
    "    shift_rows.append({\n",
    "        \"orientador\": o,\n",
    "        \"years_covered\": len(years_sorted),\n",
    "        \"first_year\": int(years_sorted[0]),\n",
    "        \"last_year\": int(years_sorted[-1]),\n",
    "        \"top_switches\": switches,\n",
    "        \"mean_jsd_consecutive\": mean_jsd,\n",
    "        \"max_jsd_consecutive\": max_jsd,\n",
    "        \"last_top_topic\": int(tops.iloc[-1][\"topic\"]),\n",
    "        \"last_top_label\": str(tops.iloc[-1][\"topic_label\"])\n",
    "    })\n",
    "\n",
    "orientador_shift_summary = pd.DataFrame(shift_rows).sort_values(\n",
    "    [\"top_switches\",\"mean_jsd_consecutive\"], ascending=[False, False]\n",
    ")\n",
    "display(orientador_shift_summary.head(10))\n",
    "\n",
    "# Slopes por orientador-tópico (tendência de preferência)\n",
    "slope_rows = []\n",
    "for (o, t), g in orientador_topic_year.groupby([\"orientador\",\"topic\"]):\n",
    "    g = g.sort_values(\"ANO\")\n",
    "    if len(g) < 2: \n",
    "        continue\n",
    "    slope = _ols_slope(g[\"ANO\"].values.astype(float), g[\"P_topic_given_orientador_year\"].values.astype(float))\n",
    "    slope_rows.append({\n",
    "        \"orientador\": o,\n",
    "        \"topic\": int(t),\n",
    "        \"topic_label\": topic_labels.get(int(t), f\"topic_{t}\"),\n",
    "        \"years\": len(g),\n",
    "        \"slope_per_year\": float(slope),\n",
    "        \"p_first\": float(g.iloc[0][\"P_topic_given_orientador_year\"]),\n",
    "        \"p_last\": float(g.iloc[-1][\"P_topic_given_orientador_year\"])\n",
    "    })\n",
    "\n",
    "orientador_topic_slopes = pd.DataFrame(slope_rows).sort_values(\n",
    "    [\"slope_per_year\",\"years\"], ascending=[False, False]\n",
    ")\n",
    "display(orientador_topic_slopes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43652bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports\n",
    "(long_topic_year\n",
    " .sort_values([\"ANO\",\"topic\"])\n",
    " .to_csv(OUT_DIR/\"topic_by_year.csv\", index=False, encoding=\"utf-8\"))\n",
    "\n",
    "(topic_trends\n",
    " .to_csv(OUT_DIR/\"topic_trend_summary.csv\", index=False, encoding=\"utf-8\"))\n",
    "\n",
    "(coverage_year\n",
    " .to_csv(OUT_DIR/\"coverage_by_year.csv\", index=False, encoding=\"utf-8\"))\n",
    "\n",
    "(orientador_topic_year\n",
    " .to_csv(OUT_DIR/\"orientador_topic_year.csv\", index=False, encoding=\"utf-8\"))\n",
    "\n",
    "(top_by_year\n",
    " .to_csv(OUT_DIR/\"orientador_top_topic_by_year.csv\", index=False, encoding=\"utf-8\"))\n",
    "\n",
    "(orientador_shift_summary\n",
    " .to_csv(OUT_DIR/\"orientador_shift_summary.csv\", index=False, encoding=\"utf-8\"))\n",
    "\n",
    "(orientador_topic_slopes\n",
    " .to_csv(OUT_DIR/\"orientador_topic_slopes.csv\", index=False, encoding=\"utf-8\"))\n",
    "\n",
    "# Metadados\n",
    "with open(OUT_DIR/\"selection.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"selected_run\": best_run,\n",
    "        \"selected_trial\": best_trial,\n",
    "        \"K\": int(K),\n",
    "        \"source_compare_csv\": str(compare_csv),\n",
    "        \"timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Arquivos salvos em:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap 1: P(t|ano)\n",
    "plt.figure(figsize=(max(8, len(p_t_given_year.columns)*0.6), max(6, len(p_t_given_year.index)*0.5)))\n",
    "plt.imshow(p_t_given_year.values, aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label=\"P(tópico | ano)\")\n",
    "plt.yticks(range(len(p_t_given_year.index)), p_t_given_year.index)\n",
    "plt.xticks(range(len(p_t_given_year.columns)), \n",
    "           [topic_labels.get(int(t), t) for t in p_t_given_year.columns], rotation=90)\n",
    "plt.title(\"Distribuição de tópicos por ano (P(t|ano))\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"heatmap_topic_by_year.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap 2: para N orientadores com mais docs, agregando últimos N anos\n",
    "TOP_ORI = 20\n",
    "ori_order = df[\"orientador\"].value_counts().head(TOP_ORI).index.tolist()\n",
    "sub = orientador_topic_year[orientador_topic_year[\"orientador\"].isin(ori_order)]\n",
    "# usa último ano de cada orientador só para um snapshot comparável\n",
    "snap = sub.sort_values([\"orientador\",\"ANO\"]).groupby(\"orientador\").tail(1)\n",
    "mat = (snap.pivot(index=\"orientador\", columns=\"topic\", values=\"P_topic_given_orientador_year\")\n",
    "           .reindex(ori_order).fillna(0))\n",
    "\n",
    "plt.figure(figsize=(max(8, len(mat.columns)*0.6), max(6, len(mat.index)*0.35)))\n",
    "plt.imshow(mat.values, aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label=\"P(tópico | orientador, último ano)\")\n",
    "plt.yticks(range(len(mat.index)), mat.index)\n",
    "plt.xticks(range(len(mat.columns)), [topic_labels.get(int(t), t) for t in mat.columns], rotation=90)\n",
    "plt.title(\"Preferências por orientador (snapshot do último ano disponível)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"heatmap_orientador_snapshot.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figuras salvas em:\", OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
