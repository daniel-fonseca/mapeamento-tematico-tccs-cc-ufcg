{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85553bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSIONS: {\n",
      "  \"python\": \"3.12.2\",\n",
      "  \"bertopic\": \"0.16.0\",\n",
      "  \"sentence-transformers\": \"2.6.1\",\n",
      "  \"umap-learn\": \"0.5.6\",\n",
      "  \"hdbscan\": \"0.8.33\",\n",
      "  \"scikit-learn\": \"1.4.2\",\n",
      "  \"gensim\": \"4.3.3\",\n",
      "  \"numpy\": \"1.26.4\",\n",
      "  \"pandas\": \"2.2.2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths e seed\n",
    "\n",
    "import os, json, math, platform, warnings, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from itertools import product\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "BASE = Path(\"../../data\")\n",
    "IN_CSV = BASE/\"interim\"/\"bertopic\"/\"prep.csv\"\n",
    "OUT_DIR = BASE/\"processed\"/\"bertopic\"\n",
    "FIGS = Path(\"../../reports/figs\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def _get_ver(pkg):\n",
    "    try:\n",
    "        import importlib.metadata as im\n",
    "        return im.version(pkg)\n",
    "    except Exception:\n",
    "        return \"NA\"\n",
    "\n",
    "versions = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"bertopic\": _get_ver(\"bertopic\"),\n",
    "    \"sentence-transformers\": _get_ver(\"sentence-transformers\"),\n",
    "    \"umap-learn\": _get_ver(\"umap-learn\"),\n",
    "    \"hdbscan\": _get_ver(\"hdbscan\"),\n",
    "    \"scikit-learn\": _get_ver(\"scikit-learn\"),\n",
    "    \"gensim\": _get_ver(\"gensim\"),\n",
    "    \"numpy\": _get_ver(\"numpy\"),\n",
    "    \"pandas\": _get_ver(\"pandas\")\n",
    "}\n",
    "print(\"VERSIONS:\", json.dumps(versions, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5580873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP_WORDS_PT carregadas: 207 termos\n"
     ]
    }
   ],
   "source": [
    "# Stopwords em português (para CountVectorizer)\n",
    "try:\n",
    "    from nltk.corpus import stopwords as nltk_stop\n",
    "    STOP_WORDS_PT = sorted(set(nltk_stop.words('portuguese')))\n",
    "    print(f\"STOP_WORDS_PT carregadas: {len(STOP_WORDS_PT)} termos\")\n",
    "except Exception as e:\n",
    "    # fallback mínimo para não travar caso NLTK falhe (mantém pipeline funcionando)\n",
    "    print(\"[WARN] NLTK indisponível; usando fallback reduzido:\", e)\n",
    "    STOP_WORDS_PT = sorted(set(\"\"\"\n",
    "a à acerca agora ai ainda além algo alguem alguns algumas algum alguma ambos ambas ante antes ao aos após aquela aquelas aquele aqueles aquilo as assim até através cada quase com como contra contudo cujo cuja cujos cujas da das de dela delas dele deles depois desde desta deste disso disto do dos e é ela elas ele eles em entre era eram essa essas esse esses esta estas este estes estou eu foi foram fosse fossem fui há isso isto já la lá lhe lhes mais mas me mesmo mesmoa mesmos mesmas minha minhas meu meus muito muita muitas muitos não na nas nem no nos nós o os ou para pela pelas pelo pelos pouca poucas pouco poucos por porque porém pra qual quais quando que quem se sem sempre sendo ser seu seus sob sobre sua suas também tão tão tem tenho ter teu teus tua tuas tudo um uma umas uns\n",
    "\"\"\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf883dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N documentos: 423\n",
      "Docs vazios (vectorizer): 14\n",
      "Comprimento (chars) — quantis: {0.0: 0.0, 0.25: 894.5, 0.5: 1123.0, 0.75: 1322.5, 0.95: 1729.2999999999997, 1.0: 2309.0}\n"
     ]
    }
   ],
   "source": [
    "# Leitura e preparação dos textos\n",
    "\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "\n",
    "required_cols = {\"DOC_ID\", \"resumo\", \"RESUMO_PREP_BERTOPIC\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"Colunas ausentes: {missing}\"\n",
    "\n",
    "docs_for_embeddings = (\n",
    "    df[\"resumo\"].fillna(df[\"RESUMO_PREP_BERTOPIC\"]).astype(str).tolist()\n",
    ")\n",
    "docs_for_vectorizer = (\n",
    "    df[\"RESUMO_PREP_BERTOPIC\"].fillna(\"\").astype(str).tolist()\n",
    ")\n",
    "\n",
    "lens = pd.Series([len(x) for x in docs_for_vectorizer])\n",
    "print(\"N documentos:\", len(df))\n",
    "print(\"Docs vazios (vectorizer):\", sum([len(x.strip()) == 0 for x in docs_for_vectorizer]))\n",
    "print(\"Comprimento (chars) — quantis:\", lens.quantile([0.0, 0.25, 0.5, 0.75, 0.95, 1.0]).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386851bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4e8b0d499b4d84b8cd782e0c19da95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape=(423, 384) | device=cpu | time=11.2s\n"
     ]
    }
   ],
   "source": [
    "# Embeddings (SBERT)\n",
    "\n",
    "# Modelo multilíngue leve que preserva semântica de sentenças (SBERT)\n",
    "# BERT → base dos embeddings contextuais [Devlin et al.] usados em SBERT\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sbert_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "start = time.time()\n",
    "sbert = SentenceTransformer(sbert_name, device=device)\n",
    "embeddings = sbert.encode(\n",
    "    docs_for_embeddings,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "print(f\"Embeddings shape={embeddings.shape} | device={device} | time={elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2ac2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combos válidos após filtro: 18\n",
      "Baseline stop_words tipo: list | tamanho: 207\n"
     ]
    }
   ],
   "source": [
    "# Grade de hiperparâmetros (mini-sweep)\n",
    "\n",
    "from math import floor\n",
    "N_DOCS = len(docs_for_vectorizer)\n",
    "\n",
    "# --- grade atualizada ---\n",
    "grid_umap = {\n",
    "    \"n_neighbors\": [10, 15, 30],\n",
    "    \"n_components\": [5],\n",
    "    \"min_dist\": [0.0],\n",
    "    \"metric\": [\"cosine\"],\n",
    "    \"random_state\": [SEED],\n",
    "}\n",
    "grid_hdb = {\n",
    "    \"min_cluster_size\": [10, 15, 20],\n",
    "    \"min_samples\": [None],\n",
    "    \"metric\": [\"euclidean\"],\n",
    "    \"cluster_selection_method\": [\"eom\"],\n",
    "    \"prediction_data\": [True],\n",
    "}\n",
    "# >>> use a lista STOP_WORDS_PT (construída no Bloco 1B via NLTK ou spaCy)\n",
    "grid_vec = {\n",
    "    \"ngram_range\": [(1,1), (1,2)],\n",
    "    \"stop_words\": [STOP_WORDS_PT],   # <<< nunca string \"portuguese\"\n",
    "    \"min_df\": [2, 3],\n",
    "    \"max_df\": [0.9],                 # vamos filtrar combos inválidos logo abaixo\n",
    "}\n",
    "\n",
    "# Produto cartesiano\n",
    "from itertools import product\n",
    "combos_all = []\n",
    "for u, h, v in product(\n",
    "    product(*grid_umap.values()),\n",
    "    product(*grid_hdb.values()),\n",
    "    product(*grid_vec.values())\n",
    "):\n",
    "    umap_kwargs = dict(zip(grid_umap.keys(), u))\n",
    "    hdb_kwargs = dict(zip(grid_hdb.keys(), h))\n",
    "    vec_kwargs = dict(zip(grid_vec.keys(), v))\n",
    "    combos_all.append({\"umap\": umap_kwargs, \"hdb\": hdb_kwargs, \"vec\": vec_kwargs})\n",
    "\n",
    "# Baseline com stopwords corretas\n",
    "baseline = {\n",
    "    \"umap\": {\"n_neighbors\": 15, \"n_components\": 5, \"min_dist\": 0.0, \"metric\": \"cosine\", \"random_state\": SEED},\n",
    "    \"hdb\": {\"min_cluster_size\": 15, \"min_samples\": None, \"metric\": \"euclidean\",\n",
    "            \"cluster_selection_method\": \"eom\", \"prediction_data\": True},\n",
    "    \"vec\": {\"ngram_range\": (1,2), \"stop_words\": STOP_WORDS_PT, \"min_df\": 3, \"max_df\": 0.9},\n",
    "}\n",
    "\n",
    "# Move baseline para o início (sem duplicar)\n",
    "def _same(a, b): return json.dumps(a, sort_keys=True) == json.dumps(b, sort_keys=True)\n",
    "combos = [baseline] + [c for c in combos_all if not _same(c, baseline)]\n",
    "\n",
    "# ---- filtro para evitar o erro \"max_df corresponds to < documents than min_df\"\n",
    "def _valid_vec(v, n_docs: int) -> bool:\n",
    "    min_df = v[\"min_df\"]\n",
    "    max_df = v[\"max_df\"]\n",
    "    # max_df como proporção → #docs máximos admitidos\n",
    "    max_docs = floor(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    return max_docs >= int(min_df)\n",
    "\n",
    "combos = [c for c in combos if _valid_vec(c[\"vec\"], N_DOCS)]\n",
    "\n",
    "# Opcional: limitar a 18 amostras determinísticas, mantendo baseline na frente\n",
    "rng = np.random.RandomState(SEED)\n",
    "if len(combos) > 18:\n",
    "    # preserva baseline + 17 amostras\n",
    "    others = combos[1:]\n",
    "    idx = rng.choice(len(others), size=17, replace=False)\n",
    "    combos = [combos[0]] + [others[i] for i in sorted(idx)]\n",
    "\n",
    "print(f\"Combos válidos após filtro: {len(combos)}\")\n",
    "print(\"Baseline stop_words tipo:\", type(combos[0]['vec']['stop_words']).__name__,\n",
    "      \"| tamanho:\", len(combos[0]['vec']['stop_words']) if hasattr(combos[0]['vec']['stop_words'], '__len__') else 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb89d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "def write_json(path: Path, data: Dict[str, Any]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def extract_topic_words(topic_model: BERTopic, topk: int = 10) -> List[List[str]]:\n",
    "    topics_dict = topic_model.get_topics()\n",
    "    topic_words: List[List[str]] = []\n",
    "    for tid, pairs in topics_dict.items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        # BERTopic pode retornar None/[] para tópicos degenerados\n",
    "        if not pairs:\n",
    "            continue\n",
    "        # Garante apenas strings não vazias; respeita topk\n",
    "        toks = [str(w).strip() for (w, _) in pairs[:topk] if isinstance(w, (str, bytes)) and str(w).strip()]\n",
    "        if toks:\n",
    "            topic_words.append(toks)\n",
    "    return topic_words\n",
    "\n",
    "def simple_tokenize_docs(docs: List[str]) -> List[List[str]]:\n",
    "    # Tokenização simples coerente com o CountVectorizer (lower/space)\n",
    "    return [str(d).lower().split() for d in docs]\n",
    "\n",
    "def topic_diversity(topic_words: List[List[str]], topk: int = 10) -> float:\n",
    "    if not topic_words:\n",
    "        return float(\"nan\")\n",
    "    uniq = len(set([w for tw in topic_words for w in tw[:topk]]))\n",
    "    return uniq / (topk * len(topic_words))\n",
    "\n",
    "def outlier_rate(topics: List[int]) -> float:\n",
    "    topics = np.array(topics)\n",
    "    return float(np.mean(topics == -1))\n",
    "\n",
    "def _sanitize_topics_for_gensim(topic_words: List[List[str]]) -> List[List[str]]:\n",
    "    clean: List[List[str]] = []\n",
    "    for tw in topic_words:\n",
    "        if not isinstance(tw, (list, tuple)):\n",
    "            continue\n",
    "        toks = [str(t).strip() for t in tw if isinstance(t, (str, bytes)) and str(t).strip()]\n",
    "        if toks:\n",
    "            clean.append(toks)\n",
    "    return clean\n",
    "\n",
    "def compute_coherences(topic_words, tokenized_docs):\n",
    "    # c_npmi e c_uci com 'texts' (ambas aceitam texts); sanear antes de passar ao gensim\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "    cleaned = _sanitize_topics_for_gensim(topic_words)\n",
    "    if not cleaned:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "    c_npmi = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_npmi\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    c_uci = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_uci\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    return float(c_npmi), float(c_uci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e02cef74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2a3eff795b4d4db53f6ccee266dec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sweep:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Loop do sweep: treino, avaliação e salvamento por trial\n",
    "\n",
    "tokenized_docs = simple_tokenize_docs(docs_for_vectorizer)\n",
    "trials_summary = []\n",
    "errors_log = []\n",
    "\n",
    "N_DOCS_EFF = len(docs_for_vectorizer)  # usado para sanear min_df/max_df\n",
    "\n",
    "for i, cfg in enumerate(tqdm(combos, desc=\"Sweep\")):\n",
    "    trial_id = f\"trial_{i:02d}\"\n",
    "    tdir = OUT_DIR / trial_id\n",
    "    tdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- SANEAMENTO do Vectorizer (evita \"max_df corresponds to < documents than min_df\" e 'portuguese')\n",
    "    vec_kwargs = cfg[\"vec\"].copy()\n",
    "    sw = vec_kwargs.get(\"stop_words\", None)\n",
    "    if isinstance(sw, str):\n",
    "        vec_kwargs[\"stop_words\"] = STOP_WORDS_PT\n",
    "\n",
    "    min_df = vec_kwargs.get(\"min_df\", 1)\n",
    "    max_df = vec_kwargs.get(\"max_df\", 1.0)\n",
    "    min_df_prop = (min_df / max(N_DOCS_EFF, 1)) if isinstance(min_df, int) else float(min_df)\n",
    "    max_df_prop = (max_df / max(N_DOCS_EFF, 1)) if isinstance(max_df, int) else float(max_df)\n",
    "    if max_df_prop < min_df_prop:\n",
    "        max_df_prop = min(1.0, max(min_df_prop + 1e-9, 0.999))\n",
    "    vec_kwargs[\"min_df\"] = min_df_prop\n",
    "    vec_kwargs[\"max_df\"] = max_df_prop\n",
    "\n",
    "    # Instâncias dos componentes\n",
    "    vectorizer_model = CountVectorizer(**vec_kwargs)\n",
    "    umap_model = UMAP(**cfg[\"umap\"])\n",
    "    hdbscan_model = HDBSCAN(**cfg[\"hdb\"])\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    started = datetime.now(timezone.utc).isoformat()\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        topics, probs = topic_model.fit_transform(\n",
    "            documents=docs_for_vectorizer,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        fit_time = time.time() - t0\n",
    "\n",
    "        # Artefatos primários\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info.to_csv(tdir/\"topic_info.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # Doc→tópico\n",
    "        if probs is not None and hasattr(probs, \"__array__\"):\n",
    "            rowmax = np.nanmax(probs, axis=1)\n",
    "            doc_prob = [float(x) if np.isfinite(x) else np.nan for x in rowmax]\n",
    "        else:\n",
    "            doc_prob = [np.nan] * len(topics)\n",
    "\n",
    "        doc_topics = pd.DataFrame({\n",
    "            \"DOC_ID\": df[\"DOC_ID\"].values,\n",
    "            \"topic\": topics,\n",
    "            \"prob\": doc_prob\n",
    "        })\n",
    "        doc_topics.to_csv(tdir/\"doc_topics.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        ctf = topic_model.c_tf_idf_\n",
    "        arr = ctf.toarray() if hasattr(ctf, \"toarray\") else np.asarray(ctf)\n",
    "        np.save(tdir/\"c_tf_idf.npy\", arr)\n",
    "\n",
    "        vocab = vectorizer_model.get_feature_names_out()\n",
    "        with open(tdir/\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(map(str, vocab)))\n",
    "\n",
    "\n",
    "        # Métricas (com blindagem de coerência)\n",
    "        topic_words = extract_topic_words(topic_model, topk=10)\n",
    "        div = topic_diversity(topic_words, topk=10)\n",
    "        try:\n",
    "            c_npmi, c_uci = compute_coherences(topic_words, tokenized_docs)\n",
    "        except Exception as ce:\n",
    "            c_npmi, c_uci = float(\"nan\"), float(\"nan\")\n",
    "            write_json(tdir/\"coherence_error.json\", {\"error\": repr(ce)})\n",
    "\n",
    "        out_rate = outlier_rate(topics)\n",
    "\n",
    "        metrics = {\n",
    "            \"topic_diversity@10\": div,\n",
    "            \"c_npmi\": c_npmi,\n",
    "            \"c_uci\": c_uci,\n",
    "            \"outlier_rate\": out_rate,\n",
    "            \"n_topics_excl_-1\": int((topic_info[\"Topic\"] != -1).sum()),\n",
    "            \"fit_time_sec\": fit_time\n",
    "        }\n",
    "        write_json(tdir/\"metrics.json\", metrics)\n",
    "\n",
    "        # Metadados de execução\n",
    "        run_md = {\n",
    "            \"trial_id\": trial_id,\n",
    "            \"started_utc\": started,\n",
    "            \"finished_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"seed\": SEED,\n",
    "            \"versions\": versions,\n",
    "            \"paths\": {\n",
    "                \"input_csv\": str(IN_CSV.resolve()),\n",
    "                \"trial_dir\": str(tdir.resolve())\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"umap\": cfg[\"umap\"],\n",
    "                \"hdbscan\": cfg[\"hdb\"],\n",
    "                \"vectorizer\": {\n",
    "                    **{k: (list(v) if k==\"ngram_range\" else v) for k, v in cfg[\"vec\"].items()},\n",
    "                    \"_effective_min_df\": vec_kwargs[\"min_df\"],\n",
    "                    \"_effective_max_df\": vec_kwargs[\"max_df\"],\n",
    "                },\n",
    "                \"sbert_model\": sbert_name,\n",
    "            },\n",
    "            \"sizes\": {\n",
    "                \"n_docs\": len(df),\n",
    "                \"emb_dim\": int(embeddings.shape[1])\n",
    "            }\n",
    "        }\n",
    "        write_json(tdir/\"run_metadata.json\", run_md)\n",
    "\n",
    "        trials_summary.append({\n",
    "            \"trial_id\": trial_id,\n",
    "            \"metrics\": metrics,\n",
    "            \"cfg\": cfg\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        err = {\"trial_id\": trial_id, \"error\": repr(e)}\n",
    "        errors_log.append(err)\n",
    "        write_json(tdir/\"error.json\", err)\n",
    "        print(f\"[WARN] {trial_id} falhou: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a6fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-5 trials por critério composto:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>c_npmi</th>\n",
       "      <th>topic_diversity</th>\n",
       "      <th>outlier_rate</th>\n",
       "      <th>n_topics</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>rank_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trial_01</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trial_08</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trial_14</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trial_06</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trial_07</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_id    c_npmi  topic_diversity  outlier_rate  n_topics   r1    r2  \\\n",
       "0  trial_01 -0.054650              1.0           0.0         2  1.0   1.0   \n",
       "1  trial_08 -0.131737              1.0           0.0         2  2.0   1.0   \n",
       "2  trial_14 -0.131737              1.0           0.0         2  2.0   1.0   \n",
       "3  trial_06 -0.131737              0.7           0.0         2  2.0  16.0   \n",
       "4  trial_07 -0.131737              0.7           0.0         2  2.0  16.0   \n",
       "\n",
       "    r3  rank_sum  \n",
       "0  1.0       3.0  \n",
       "1  1.0       4.0  \n",
       "2  1.0       4.0  \n",
       "3  1.0      19.0  \n",
       "4  1.0      19.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vencedora: trial_01 | cfg: {'umap': {'n_neighbors': 10, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'random_state': 42}, 'hdb': {'min_cluster_size': 10, 'min_samples': None, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'vec': {'ngram_range': (1, 1), 'stop_words': ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tém', 'tínhamos', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos'], 'min_df': 2, 'max_df': 0.9}}\n"
     ]
    }
   ],
   "source": [
    "# Ranking e escolha da melhor configuração\n",
    "\n",
    "# Ranking composto (ordenação por: c_npmi desc, topic_diversity desc, outlier_rate asc)\n",
    "if not trials_summary:\n",
    "    raise RuntimeError(\"Nenhum trial concluído. Verifique errors_log e configurações.\")\n",
    "\n",
    "df_rank = pd.DataFrame([{\n",
    "    \"trial_id\": t[\"trial_id\"],\n",
    "    \"c_npmi\": t[\"metrics\"][\"c_npmi\"],\n",
    "    \"topic_diversity\": t[\"metrics\"][\"topic_diversity@10\"],\n",
    "    \"outlier_rate\": t[\"metrics\"][\"outlier_rate\"],\n",
    "    \"n_topics\": t[\"metrics\"][\"n_topics_excl_-1\"]\n",
    "} for t in trials_summary])\n",
    "\n",
    "df_rank[\"r1\"] = df_rank[\"c_npmi\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r2\"] = df_rank[\"topic_diversity\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r3\"] = df_rank[\"outlier_rate\"].rank(ascending=True, method=\"min\")\n",
    "df_rank[\"rank_sum\"] = df_rank[[\"r1\",\"r2\",\"r3\"]].sum(axis=1)\n",
    "\n",
    "df_rank = df_rank.sort_values([\"rank_sum\", \"r1\", \"r2\", \"r3\"]).reset_index(drop=True)\n",
    "best_trial_id = df_rank.iloc[0][\"trial_id\"]\n",
    "best_row = df_rank.iloc[0].to_dict()\n",
    "\n",
    "print(\"TOP-5 trials por critério composto:\")\n",
    "display(df_rank.head(5))\n",
    "\n",
    "best_cfg = next(t[\"cfg\"] for t in trials_summary if t[\"trial_id\"] == best_trial_id)\n",
    "print(\"Vencedora:\", best_trial_id, \"| cfg:\", best_cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic)",
   "language": "python",
   "name": "bertopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
