{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85553bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSIONS: {\n",
      "  \"python\": \"3.12.2\",\n",
      "  \"bertopic\": \"0.16.0\",\n",
      "  \"sentence-transformers\": \"2.6.1\",\n",
      "  \"umap-learn\": \"0.5.6\",\n",
      "  \"hdbscan\": \"0.8.33\",\n",
      "  \"scikit-learn\": \"1.4.2\",\n",
      "  \"gensim\": \"4.3.3\",\n",
      "  \"numpy\": \"1.26.4\",\n",
      "  \"pandas\": \"2.2.2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths e seed\n",
    "\n",
    "import os, json, math, platform, warnings, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from itertools import product\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "BASE = Path(\"../../data\")\n",
    "IN_CSV = BASE/\"interim\"/\"bertopic\"/\"prep.csv\"\n",
    "OUT_DIR = BASE/\"processed\"/\"bertopic\"\n",
    "FIGS = Path(\"../../reports/figs\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def _get_ver(pkg):\n",
    "    try:\n",
    "        import importlib.metadata as im\n",
    "        return im.version(pkg)\n",
    "    except Exception:\n",
    "        return \"NA\"\n",
    "\n",
    "versions = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"bertopic\": _get_ver(\"bertopic\"),\n",
    "    \"sentence-transformers\": _get_ver(\"sentence-transformers\"),\n",
    "    \"umap-learn\": _get_ver(\"umap-learn\"),\n",
    "    \"hdbscan\": _get_ver(\"hdbscan\"),\n",
    "    \"scikit-learn\": _get_ver(\"scikit-learn\"),\n",
    "    \"gensim\": _get_ver(\"gensim\"),\n",
    "    \"numpy\": _get_ver(\"numpy\"),\n",
    "    \"pandas\": _get_ver(\"pandas\")\n",
    "}\n",
    "print(\"VERSIONS:\", json.dumps(versions, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5580873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP_WORDS_PT carregadas: 207 termos\n"
     ]
    }
   ],
   "source": [
    "# Stopwords em português (para CountVectorizer)\n",
    "try:\n",
    "    from nltk.corpus import stopwords as nltk_stop\n",
    "    STOP_WORDS_PT = sorted(set(nltk_stop.words('portuguese')))\n",
    "    print(f\"STOP_WORDS_PT carregadas: {len(STOP_WORDS_PT)} termos\")\n",
    "except Exception as e:\n",
    "    # fallback mínimo para não travar caso NLTK falhe (mantém pipeline funcionando)\n",
    "    print(\"[WARN] NLTK indisponível; usando fallback reduzido:\", e)\n",
    "    STOP_WORDS_PT = sorted(set(\"\"\"\n",
    "a à acerca agora ai ainda além algo alguem alguns algumas algum alguma ambos ambas ante antes ao aos após aquela aquelas aquele aqueles aquilo as assim até através cada quase com como contra contudo cujo cuja cujos cujas da das de dela delas dele deles depois desde desta deste disso disto do dos e é ela elas ele eles em entre era eram essa essas esse esses esta estas este estes estou eu foi foram fosse fossem fui há isso isto já la lá lhe lhes mais mas me mesmo mesmoa mesmos mesmas minha minhas meu meus muito muita muitas muitos não na nas nem no nos nós o os ou para pela pelas pelo pelos pouca poucas pouco poucos por porque porém pra qual quais quando que quem se sem sempre sendo ser seu seus sob sobre sua suas também tão tão tem tenho ter teu teus tua tuas tudo um uma umas uns\n",
    "\"\"\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf883dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N documentos: 423\n",
      "Docs vazios (vectorizer): 14\n",
      "Comprimento (chars) — quantis: {0.0: 0.0, 0.25: 894.5, 0.5: 1123.0, 0.75: 1322.5, 0.95: 1729.2999999999997, 1.0: 2309.0}\n"
     ]
    }
   ],
   "source": [
    "# Leitura e preparação dos textos\n",
    "\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "\n",
    "required_cols = {\"DOC_ID\", \"resumo\", \"RESUMO_PREP_BERTOPIC\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"Colunas ausentes: {missing}\"\n",
    "\n",
    "docs_for_embeddings = (\n",
    "    df[\"resumo\"].fillna(df[\"RESUMO_PREP_BERTOPIC\"]).astype(str).tolist()\n",
    ")\n",
    "docs_for_vectorizer = (\n",
    "    df[\"RESUMO_PREP_BERTOPIC\"].fillna(\"\").astype(str).tolist()\n",
    ")\n",
    "\n",
    "lens = pd.Series([len(x) for x in docs_for_vectorizer])\n",
    "print(\"N documentos:\", len(df))\n",
    "print(\"Docs vazios (vectorizer):\", sum([len(x.strip()) == 0 for x in docs_for_vectorizer]))\n",
    "print(\"Comprimento (chars) — quantis:\", lens.quantile([0.0, 0.25, 0.5, 0.75, 0.95, 1.0]).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "386851bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e816730cc94e72b9e60c972d0ecbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape=(423, 384) | device=cpu | time=12.1s\n"
     ]
    }
   ],
   "source": [
    "# Embeddings (SBERT)\n",
    "\n",
    "# Modelo multilíngue leve que preserva semântica de sentenças (SBERT)\n",
    "# BERT → base dos embeddings contextuais [Devlin et al.] usados em SBERT\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sbert_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "start = time.time()\n",
    "sbert = SentenceTransformer(sbert_name, device=device)\n",
    "embeddings = sbert.encode(\n",
    "    docs_for_embeddings,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "print(f\"Embeddings shape={embeddings.shape} | device={device} | time={elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2ac2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combos válidos após filtro: 18\n",
      "Baseline stop_words tipo: list | tamanho: 207\n"
     ]
    }
   ],
   "source": [
    "# Grade de hiperparâmetros (mini-sweep)\n",
    "\n",
    "from math import floor\n",
    "N_DOCS = len(docs_for_vectorizer)\n",
    "\n",
    "# --- grade atualizada ---\n",
    "grid_umap = {\n",
    "    \"n_neighbors\": [10, 15, 30],\n",
    "    \"n_components\": [5],\n",
    "    \"min_dist\": [0.0],\n",
    "    \"metric\": [\"cosine\"],\n",
    "    \"random_state\": [SEED],\n",
    "}\n",
    "grid_hdb = {\n",
    "    \"min_cluster_size\": [10, 15, 20],\n",
    "    \"min_samples\": [None],\n",
    "    \"metric\": [\"euclidean\"],\n",
    "    \"cluster_selection_method\": [\"eom\"],\n",
    "    \"prediction_data\": [True],\n",
    "}\n",
    "# >>> use a lista STOP_WORDS_PT (construída no Bloco 1B via NLTK ou spaCy)\n",
    "grid_vec = {\n",
    "    \"ngram_range\": [(1,1), (1,2)],\n",
    "    \"stop_words\": [STOP_WORDS_PT],   # <<< nunca string \"portuguese\"\n",
    "    \"min_df\": [2, 3],\n",
    "    \"max_df\": [0.9],                 # vamos filtrar combos inválidos logo abaixo\n",
    "}\n",
    "\n",
    "# Produto cartesiano\n",
    "from itertools import product\n",
    "combos_all = []\n",
    "for u, h, v in product(\n",
    "    product(*grid_umap.values()),\n",
    "    product(*grid_hdb.values()),\n",
    "    product(*grid_vec.values())\n",
    "):\n",
    "    umap_kwargs = dict(zip(grid_umap.keys(), u))\n",
    "    hdb_kwargs = dict(zip(grid_hdb.keys(), h))\n",
    "    vec_kwargs = dict(zip(grid_vec.keys(), v))\n",
    "    combos_all.append({\"umap\": umap_kwargs, \"hdb\": hdb_kwargs, \"vec\": vec_kwargs})\n",
    "\n",
    "# Baseline com stopwords corretas\n",
    "baseline = {\n",
    "    \"umap\": {\"n_neighbors\": 15, \"n_components\": 5, \"min_dist\": 0.0, \"metric\": \"cosine\", \"random_state\": SEED},\n",
    "    \"hdb\": {\"min_cluster_size\": 15, \"min_samples\": None, \"metric\": \"euclidean\",\n",
    "            \"cluster_selection_method\": \"eom\", \"prediction_data\": True},\n",
    "    \"vec\": {\"ngram_range\": (1,2), \"stop_words\": STOP_WORDS_PT, \"min_df\": 3, \"max_df\": 0.9},\n",
    "}\n",
    "\n",
    "# Move baseline para o início (sem duplicar)\n",
    "def _same(a, b): return json.dumps(a, sort_keys=True) == json.dumps(b, sort_keys=True)\n",
    "combos = [baseline] + [c for c in combos_all if not _same(c, baseline)]\n",
    "\n",
    "# ---- filtro para evitar o erro \"max_df corresponds to < documents than min_df\"\n",
    "def _valid_vec(v, n_docs: int) -> bool:\n",
    "    min_df = v[\"min_df\"]\n",
    "    max_df = v[\"max_df\"]\n",
    "    # max_df como proporção → #docs máximos admitidos\n",
    "    max_docs = floor(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    return max_docs >= int(min_df)\n",
    "\n",
    "combos = [c for c in combos if _valid_vec(c[\"vec\"], N_DOCS)]\n",
    "\n",
    "# Opcional: limitar a 18 amostras determinísticas, mantendo baseline na frente\n",
    "rng = np.random.RandomState(SEED)\n",
    "if len(combos) > 18:\n",
    "    # preserva baseline + 17 amostras\n",
    "    others = combos[1:]\n",
    "    idx = rng.choice(len(others), size=17, replace=False)\n",
    "    combos = [combos[0]] + [others[i] for i in sorted(idx)]\n",
    "\n",
    "print(f\"Combos válidos após filtro: {len(combos)}\")\n",
    "print(\"Baseline stop_words tipo:\", type(combos[0]['vec']['stop_words']).__name__,\n",
    "      \"| tamanho:\", len(combos[0]['vec']['stop_words']) if hasattr(combos[0]['vec']['stop_words'], '__len__') else 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb89d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "def write_json(path: Path, data: Dict[str, Any]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def extract_topic_words(topic_model: BERTopic, topk: int = 10) -> List[List[str]]:\n",
    "    topics_dict = topic_model.get_topics()\n",
    "    topic_words: List[List[str]] = []\n",
    "    for tid, pairs in topics_dict.items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        # BERTopic pode retornar None/[] para tópicos degenerados\n",
    "        if not pairs:\n",
    "            continue\n",
    "        # Garante apenas strings não vazias; respeita topk\n",
    "        toks = [str(w).strip() for (w, _) in pairs[:topk] if isinstance(w, (str, bytes)) and str(w).strip()]\n",
    "        if toks:\n",
    "            topic_words.append(toks)\n",
    "    return topic_words\n",
    "\n",
    "def simple_tokenize_docs(docs: List[str]) -> List[List[str]]:\n",
    "    # Tokenização simples coerente com o CountVectorizer (lower/space)\n",
    "    return [str(d).lower().split() for d in docs]\n",
    "\n",
    "def topic_diversity(topic_words: List[List[str]], topk: int = 10) -> float:\n",
    "    if not topic_words:\n",
    "        return float(\"nan\")\n",
    "    uniq = len(set([w for tw in topic_words for w in tw[:topk]]))\n",
    "    return uniq / (topk * len(topic_words))\n",
    "\n",
    "def outlier_rate(topics: List[int]) -> float:\n",
    "    topics = np.array(topics)\n",
    "    return float(np.mean(topics == -1))\n",
    "\n",
    "def _sanitize_topics_for_gensim(topic_words: List[List[str]]) -> List[List[str]]:\n",
    "    clean: List[List[str]] = []\n",
    "    for tw in topic_words:\n",
    "        if not isinstance(tw, (list, tuple)):\n",
    "            continue\n",
    "        toks = [str(t).strip() for t in tw if isinstance(t, (str, bytes)) and str(t).strip()]\n",
    "        if toks:\n",
    "            clean.append(toks)\n",
    "    return clean\n",
    "\n",
    "def compute_coherences(topic_words, tokenized_docs):\n",
    "    # c_npmi e c_uci com 'texts' (ambas aceitam texts); sanear antes de passar ao gensim\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "    cleaned = _sanitize_topics_for_gensim(topic_words)\n",
    "    if not cleaned:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "    c_npmi = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_npmi\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    c_uci = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_uci\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    return float(c_npmi), float(c_uci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02cef74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a690c7dc05488686ed90e2fb57fbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sweep:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Loop do sweep: treino, avaliação e salvamento por trial\n",
    "\n",
    "tokenized_docs = simple_tokenize_docs(docs_for_vectorizer)\n",
    "trials_summary = []\n",
    "errors_log = []\n",
    "\n",
    "N_DOCS_EFF = len(docs_for_vectorizer)  # usado para sanear min_df/max_df\n",
    "\n",
    "for i, cfg in enumerate(tqdm(combos, desc=\"Sweep\")):\n",
    "    trial_id = f\"trial_{i:02d}\"\n",
    "    tdir = OUT_DIR / trial_id\n",
    "    tdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- SANEAMENTO do Vectorizer (evita \"max_df corresponds to < documents than min_df\" e 'portuguese')\n",
    "    vec_kwargs = cfg[\"vec\"].copy()\n",
    "    sw = vec_kwargs.get(\"stop_words\", None)\n",
    "    if isinstance(sw, str):\n",
    "        vec_kwargs[\"stop_words\"] = STOP_WORDS_PT\n",
    "\n",
    "    min_df = vec_kwargs.get(\"min_df\", 1)\n",
    "    max_df = vec_kwargs.get(\"max_df\", 1.0)\n",
    "    min_df_prop = (min_df / max(N_DOCS_EFF, 1)) if isinstance(min_df, int) else float(min_df)\n",
    "    max_df_prop = (max_df / max(N_DOCS_EFF, 1)) if isinstance(max_df, int) else float(max_df)\n",
    "    if max_df_prop < min_df_prop:\n",
    "        max_df_prop = min(1.0, max(min_df_prop + 1e-9, 0.999))\n",
    "    vec_kwargs[\"min_df\"] = min_df_prop\n",
    "    vec_kwargs[\"max_df\"] = max_df_prop\n",
    "\n",
    "    # Instâncias dos componentes\n",
    "    vectorizer_model = CountVectorizer(**vec_kwargs)\n",
    "    umap_model = UMAP(**cfg[\"umap\"])\n",
    "    hdbscan_model = HDBSCAN(**cfg[\"hdb\"])\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    started = datetime.now(timezone.utc).isoformat()\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        topics, probs = topic_model.fit_transform(\n",
    "            documents=docs_for_vectorizer,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        fit_time = time.time() - t0\n",
    "\n",
    "        # Artefatos primários\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info.to_csv(tdir/\"topic_info.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # Doc→tópico\n",
    "        if probs is not None and hasattr(probs, \"__array__\"):\n",
    "            rowmax = np.nanmax(probs, axis=1)\n",
    "            doc_prob = [float(x) if np.isfinite(x) else np.nan for x in rowmax]\n",
    "        else:\n",
    "            doc_prob = [np.nan] * len(topics)\n",
    "\n",
    "        doc_topics = pd.DataFrame({\n",
    "            \"DOC_ID\": df[\"DOC_ID\"].values,\n",
    "            \"topic\": topics,\n",
    "            \"prob\": doc_prob\n",
    "        })\n",
    "        doc_topics.to_csv(tdir/\"doc_topics.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        ctf = topic_model.c_tf_idf_\n",
    "        arr = ctf.toarray() if hasattr(ctf, \"toarray\") else np.asarray(ctf)\n",
    "        np.save(tdir/\"c_tf_idf.npy\", arr)\n",
    "\n",
    "        vocab = vectorizer_model.get_feature_names_out()\n",
    "        with open(tdir/\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(map(str, vocab)))\n",
    "\n",
    "\n",
    "        # Métricas (com blindagem de coerência)\n",
    "        topic_words = extract_topic_words(topic_model, topk=10)\n",
    "        div = topic_diversity(topic_words, topk=10)\n",
    "        try:\n",
    "            c_npmi, c_uci = compute_coherences(topic_words, tokenized_docs)\n",
    "        except Exception as ce:\n",
    "            c_npmi, c_uci = float(\"nan\"), float(\"nan\")\n",
    "            write_json(tdir/\"coherence_error.json\", {\"error\": repr(ce)})\n",
    "\n",
    "        out_rate = outlier_rate(topics)\n",
    "\n",
    "        metrics = {\n",
    "            \"topic_diversity@10\": div,\n",
    "            \"c_npmi\": c_npmi,\n",
    "            \"c_uci\": c_uci,\n",
    "            \"outlier_rate\": out_rate,\n",
    "            \"n_topics_excl_-1\": int((topic_info[\"Topic\"] != -1).sum()),\n",
    "            \"fit_time_sec\": fit_time\n",
    "        }\n",
    "        write_json(tdir/\"metrics.json\", metrics)\n",
    "\n",
    "        # Metadados de execução\n",
    "        run_md = {\n",
    "            \"trial_id\": trial_id,\n",
    "            \"started_utc\": started,\n",
    "            \"finished_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"seed\": SEED,\n",
    "            \"versions\": versions,\n",
    "            \"paths\": {\n",
    "                \"input_csv\": str(IN_CSV.resolve()),\n",
    "                \"trial_dir\": str(tdir.resolve())\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"umap\": cfg[\"umap\"],\n",
    "                \"hdbscan\": cfg[\"hdb\"],\n",
    "                \"vectorizer\": {\n",
    "                    **{k: (list(v) if k==\"ngram_range\" else v) for k, v in cfg[\"vec\"].items()},\n",
    "                    \"_effective_min_df\": vec_kwargs[\"min_df\"],\n",
    "                    \"_effective_max_df\": vec_kwargs[\"max_df\"],\n",
    "                },\n",
    "                \"sbert_model\": sbert_name,\n",
    "            },\n",
    "            \"sizes\": {\n",
    "                \"n_docs\": len(df),\n",
    "                \"emb_dim\": int(embeddings.shape[1])\n",
    "            }\n",
    "        }\n",
    "        write_json(tdir/\"run_metadata.json\", run_md)\n",
    "\n",
    "        trials_summary.append({\n",
    "            \"trial_id\": trial_id,\n",
    "            \"metrics\": metrics,\n",
    "            \"cfg\": cfg\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        err = {\"trial_id\": trial_id, \"error\": repr(e)}\n",
    "        errors_log.append(err)\n",
    "        write_json(tdir/\"error.json\", err)\n",
    "        print(f\"[WARN] {trial_id} falhou: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb1aa24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo em (absoluto): C:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\notebooks\\bertopic\\data\\processed\\bertopic\\trials_summary.csv\n",
      "Linhas: 18 | Colunas: 22\n",
      "\n",
      "Arquivos no diretório de saída:\n",
      " - trials_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>n_docs</th>\n",
       "      <th>n_topics_excl_minus1</th>\n",
       "      <th>outlier_rate</th>\n",
       "      <th>c_npmi</th>\n",
       "      <th>c_v</th>\n",
       "      <th>topic_diversity_at_10</th>\n",
       "      <th>sep_jsd</th>\n",
       "      <th>balance</th>\n",
       "      <th>clarity</th>\n",
       "      <th>...</th>\n",
       "      <th>umap_min_dist</th>\n",
       "      <th>umap_metric</th>\n",
       "      <th>hdbscan_min_cluster_size</th>\n",
       "      <th>hdbscan_min_samples</th>\n",
       "      <th>hdbscan_metric</th>\n",
       "      <th>hdbscan_selection</th>\n",
       "      <th>vectorizer_min_df</th>\n",
       "      <th>vectorizer_max_df</th>\n",
       "      <th>vectorizer_ngram</th>\n",
       "      <th>embedding_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trial_00</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>-0.280830</td>\n",
       "      <td>None</td>\n",
       "      <td>0.975</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>eom</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trial_01</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>None</td>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>eom</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trial_02</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>-0.455045</td>\n",
       "      <td>None</td>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>eom</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trial_03</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.465721</td>\n",
       "      <td>-0.264228</td>\n",
       "      <td>None</td>\n",
       "      <td>1.000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>eom</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trial_04</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.503546</td>\n",
       "      <td>-0.224663</td>\n",
       "      <td>None</td>\n",
       "      <td>0.975</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>eom</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_id n_docs  n_topics_excl_minus1  outlier_rate    c_npmi   c_v  \\\n",
       "0  trial_00   None                     4      0.446809 -0.280830  None   \n",
       "1  trial_01   None                     2      0.000000 -0.054650  None   \n",
       "2  trial_02   None                     2      0.255319 -0.455045  None   \n",
       "3  trial_03   None                     5      0.465721 -0.264228  None   \n",
       "4  trial_04   None                     4      0.503546 -0.224663  None   \n",
       "\n",
       "   topic_diversity_at_10 sep_jsd balance clarity  ...  umap_min_dist  \\\n",
       "0                  0.975    None    None    None  ...            0.0   \n",
       "1                  1.000    None    None    None  ...            0.0   \n",
       "2                  1.000    None    None    None  ...            0.0   \n",
       "3                  1.000    None    None    None  ...            0.0   \n",
       "4                  0.975    None    None    None  ...            0.0   \n",
       "\n",
       "   umap_metric  hdbscan_min_cluster_size hdbscan_min_samples  hdbscan_metric  \\\n",
       "0       cosine                        15                None       euclidean   \n",
       "1       cosine                        10                None       euclidean   \n",
       "2       cosine                        15                None       euclidean   \n",
       "3       cosine                        15                None       euclidean   \n",
       "4       cosine                        20                None       euclidean   \n",
       "\n",
       "  hdbscan_selection vectorizer_min_df vectorizer_max_df  vectorizer_ngram  \\\n",
       "0               eom                 3               0.9            (1, 2)   \n",
       "1               eom                 2               0.9            (1, 1)   \n",
       "2               eom                 2               0.9            (1, 1)   \n",
       "3               eom                 3               0.9            (1, 1)   \n",
       "4               eom                 2               0.9            (1, 1)   \n",
       "\n",
       "   embedding_model  \n",
       "0             None  \n",
       "1             None  \n",
       "2             None  \n",
       "3             None  \n",
       "4             None  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exporta a tabela da varredura de trials (trials_summary.csv) — robusta com raiz do projeto\n",
    "import pandas as pd, os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Descobre a raiz do projeto (procura por uma pasta 'data' subindo diretórios)\n",
    "def find_repo_root(start: Path = None, max_hops: int = 8) -> Path:\n",
    "    p = start or Path.cwd()\n",
    "    for _ in range(max_hops + 1):\n",
    "        if (p / \"data\").is_dir():\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    # fallback: diretório atual\n",
    "    return Path.cwd()\n",
    "\n",
    "ROOT = find_repo_root()\n",
    "OUT_DIR = ROOT / \"data\" / \"processed\" / \"bertopic\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert 'trials_summary' in globals() and len(trials_summary) > 0, \"trials_summary vazio. Rode o sweep antes.\"\n",
    "\n",
    "def row_from_trial(t):\n",
    "    m   = t.get('metrics', {})\n",
    "    cfg = t.get('cfg', {})\n",
    "    um  = cfg.get('umap', {})\n",
    "    hb  = cfg.get('hdb', {})\n",
    "    vc  = cfg.get('vec', {})\n",
    "    return {\n",
    "        'trial_id': t.get('trial_id'),\n",
    "        'n_docs': t.get('n_docs', None),\n",
    "        'n_topics_excl_minus1': m.get('n_topics_excl_-1', None),\n",
    "        'outlier_rate': m.get('outlier_rate', None),\n",
    "        'c_npmi': m.get('c_npmi', None),\n",
    "        'c_v': m.get('c_v', None),\n",
    "        'topic_diversity_at_10': m.get('topic_diversity@10', None),\n",
    "        'sep_jsd': m.get('sep_jsd', None),\n",
    "        'balance': m.get('balance', None),\n",
    "        'clarity': m.get('clarity', None),\n",
    "        'umap_n_neighbors': um.get('n_neighbors'),\n",
    "        'umap_n_components': um.get('n_components'),\n",
    "        'umap_min_dist': um.get('min_dist'),\n",
    "        'umap_metric': um.get('metric'),\n",
    "        'hdbscan_min_cluster_size': hb.get('min_cluster_size'),\n",
    "        'hdbscan_min_samples': hb.get('min_samples'),\n",
    "        'hdbscan_metric': hb.get('metric'),\n",
    "        'hdbscan_selection': hb.get('cluster_selection_method'),\n",
    "        'vectorizer_min_df': vc.get('min_df'),\n",
    "        'vectorizer_max_df': vc.get('max_df'),\n",
    "        'vectorizer_ngram': str(vc.get('ngram_range')),\n",
    "        'embedding_model': cfg.get('sbert_model')\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame([row_from_trial(t) for t in trials_summary])\n",
    "\n",
    "out = OUT_DIR / \"trials_summary.csv\"\n",
    "df.to_csv(out, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Salvo em (absoluto):\", out.resolve())\n",
    "print(\"Linhas:\", len(df), \"| Colunas:\", len(df.columns))\n",
    "print(\"\\nArquivos no diretório de saída:\")\n",
    "for f in sorted(OUT_DIR.iterdir()):\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a6fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-5 trials por critério composto:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>c_npmi</th>\n",
       "      <th>topic_diversity</th>\n",
       "      <th>outlier_rate</th>\n",
       "      <th>n_topics</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>rank_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trial_01</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trial_08</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trial_14</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trial_06</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trial_07</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_id    c_npmi  topic_diversity  outlier_rate  n_topics   r1    r2  \\\n",
       "0  trial_01 -0.054650              1.0           0.0         2  1.0   1.0   \n",
       "1  trial_08 -0.131737              1.0           0.0         2  2.0   1.0   \n",
       "2  trial_14 -0.131737              1.0           0.0         2  2.0   1.0   \n",
       "3  trial_06 -0.131737              0.7           0.0         2  2.0  16.0   \n",
       "4  trial_07 -0.131737              0.7           0.0         2  2.0  16.0   \n",
       "\n",
       "    r3  rank_sum  \n",
       "0  1.0       3.0  \n",
       "1  1.0       4.0  \n",
       "2  1.0       4.0  \n",
       "3  1.0      19.0  \n",
       "4  1.0      19.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vencedora: trial_01 | cfg: {'umap': {'n_neighbors': 10, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'random_state': 42}, 'hdb': {'min_cluster_size': 10, 'min_samples': None, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'vec': {'ngram_range': (1, 1), 'stop_words': ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tém', 'tínhamos', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos'], 'min_df': 2, 'max_df': 0.9}}\n"
     ]
    }
   ],
   "source": [
    "# Ranking e escolha da melhor configuração\n",
    "\n",
    "# Ranking composto (ordenação por: c_npmi desc, topic_diversity desc, outlier_rate asc)\n",
    "if not trials_summary:\n",
    "    raise RuntimeError(\"Nenhum trial concluído. Verifique errors_log e configurações.\")\n",
    "\n",
    "df_rank = pd.DataFrame([{\n",
    "    \"trial_id\": t[\"trial_id\"],\n",
    "    \"c_npmi\": t[\"metrics\"][\"c_npmi\"],\n",
    "    \"topic_diversity\": t[\"metrics\"][\"topic_diversity@10\"],\n",
    "    \"outlier_rate\": t[\"metrics\"][\"outlier_rate\"],\n",
    "    \"n_topics\": t[\"metrics\"][\"n_topics_excl_-1\"]\n",
    "} for t in trials_summary])\n",
    "\n",
    "df_rank[\"r1\"] = df_rank[\"c_npmi\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r2\"] = df_rank[\"topic_diversity\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r3\"] = df_rank[\"outlier_rate\"].rank(ascending=True, method=\"min\")\n",
    "df_rank[\"rank_sum\"] = df_rank[[\"r1\",\"r2\",\"r3\"]].sum(axis=1)\n",
    "\n",
    "df_rank = df_rank.sort_values([\"rank_sum\", \"r1\", \"r2\", \"r3\"]).reset_index(drop=True)\n",
    "best_trial_id = df_rank.iloc[0][\"trial_id\"]\n",
    "best_row = df_rank.iloc[0].to_dict()\n",
    "\n",
    "print(\"TOP-5 trials por critério composto:\")\n",
    "display(df_rank.head(5))\n",
    "\n",
    "best_cfg = next(t[\"cfg\"] for t in trials_summary if t[\"trial_id\"] == best_trial_id)\n",
    "print(\"Vencedora:\", best_trial_id, \"| cfg:\", best_cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic)",
   "language": "python",
   "name": "bertopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
