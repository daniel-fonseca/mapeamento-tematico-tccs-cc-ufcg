{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d9cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSIONS: {\n",
      "  \"python\": \"3.12.2\",\n",
      "  \"bertopic\": \"0.16.0\",\n",
      "  \"sentence-transformers\": \"2.6.1\",\n",
      "  \"umap-learn\": \"0.5.6\",\n",
      "  \"hdbscan\": \"0.8.33\",\n",
      "  \"scikit-learn\": \"1.4.2\",\n",
      "  \"gensim\": \"4.3.3\",\n",
      "  \"numpy\": \"1.26.4\",\n",
      "  \"pandas\": \"2.2.2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths e seed\n",
    "\n",
    "import os, json, math, platform, warnings, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from itertools import product\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "BASE = Path(\"../../data\")\n",
    "IN_CSV = BASE/\"interim\"/\"bertopic\"/\"prep.csv\"\n",
    "OUT_DIR = BASE/\"processed\"/\"bertopic\"\n",
    "FIGS = Path(\"../../reports/figs\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def _get_ver(pkg):\n",
    "    try:\n",
    "        import importlib.metadata as im\n",
    "        return im.version(pkg)\n",
    "    except Exception:\n",
    "        return \"NA\"\n",
    "\n",
    "versions = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"bertopic\": _get_ver(\"bertopic\"),\n",
    "    \"sentence-transformers\": _get_ver(\"sentence-transformers\"),\n",
    "    \"umap-learn\": _get_ver(\"umap-learn\"),\n",
    "    \"hdbscan\": _get_ver(\"hdbscan\"),\n",
    "    \"scikit-learn\": _get_ver(\"scikit-learn\"),\n",
    "    \"gensim\": _get_ver(\"gensim\"),\n",
    "    \"numpy\": _get_ver(\"numpy\"),\n",
    "    \"pandas\": _get_ver(\"pandas\")\n",
    "}\n",
    "print(\"VERSIONS:\", json.dumps(versions, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144b3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP_WORDS_PT carregadas: 207 termos\n"
     ]
    }
   ],
   "source": [
    "# Stopwords em português (para CountVectorizer)\n",
    "try:\n",
    "    from nltk.corpus import stopwords as nltk_stop\n",
    "    STOP_WORDS_PT = sorted(set(nltk_stop.words('portuguese')))\n",
    "    print(f\"STOP_WORDS_PT carregadas: {len(STOP_WORDS_PT)} termos\")\n",
    "except Exception as e:\n",
    "    # fallback mínimo para não travar caso NLTK falhe (mantém pipeline funcionando)\n",
    "    print(\"[WARN] NLTK indisponível; usando fallback reduzido:\", e)\n",
    "    STOP_WORDS_PT = sorted(set(\"\"\"\n",
    "a à acerca agora ai ainda além algo alguem alguns algumas algum alguma ambos ambas ante antes ao aos após aquela aquelas aquele aqueles aquilo as assim até através cada quase com como contra contudo cujo cuja cujos cujas da das de dela delas dele deles depois desde desta deste disso disto do dos e é ela elas ele eles em entre era eram essa essas esse esses esta estas este estes estou eu foi foram fosse fossem fui há isso isto já la lá lhe lhes mais mas me mesmo mesmoa mesmos mesmas minha minhas meu meus muito muita muitas muitos não na nas nem no nos nós o os ou para pela pelas pelo pelos pouca poucas pouco poucos por porque porém pra qual quais quando que quem se sem sempre sendo ser seu seus sob sobre sua suas também tão tão tem tenho ter teu teus tua tuas tudo um uma umas uns\n",
    "\"\"\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9174bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N documentos: 423\n",
      "Docs vazios (vectorizer): 14\n",
      "Comprimento (chars) — quantis: {0.0: 0.0, 0.25: 894.5, 0.5: 1123.0, 0.75: 1322.5, 0.95: 1729.2999999999997, 1.0: 2309.0}\n"
     ]
    }
   ],
   "source": [
    "# Leitura e preparação dos textos\n",
    "\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "\n",
    "required_cols = {\"DOC_ID\", \"resumo\", \"RESUMO_PREP_BERTOPIC\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"Colunas ausentes: {missing}\"\n",
    "\n",
    "docs_for_embeddings = (\n",
    "    df[\"resumo\"].fillna(df[\"RESUMO_PREP_BERTOPIC\"]).astype(str).tolist()\n",
    ")\n",
    "docs_for_vectorizer = (\n",
    "    df[\"RESUMO_PREP_BERTOPIC\"].fillna(\"\").astype(str).tolist()\n",
    ")\n",
    "\n",
    "lens = pd.Series([len(x) for x in docs_for_vectorizer])\n",
    "print(\"N documentos:\", len(df))\n",
    "print(\"Docs vazios (vectorizer):\", sum([len(x.strip()) == 0 for x in docs_for_vectorizer]))\n",
    "print(\"Comprimento (chars) — quantis:\", lens.quantile([0.0, 0.25, 0.5, 0.75, 0.95, 1.0]).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df6eef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd09c56413a24732b99c7869804cafb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape=(423, 384) | device=cpu | time=11.3s\n"
     ]
    }
   ],
   "source": [
    "# Embeddings (SBERT)\n",
    "\n",
    "# Modelo multilíngue leve que preserva semântica de sentenças (SBERT)\n",
    "# BERT → base dos embeddings contextuais [Devlin et al.] usados em SBERT\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sbert_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "start = time.time()\n",
    "sbert = SentenceTransformer(sbert_name, device=device)\n",
    "embeddings = sbert.encode(\n",
    "    docs_for_embeddings,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "print(f\"Embeddings shape={embeddings.shape} | device={device} | time={elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca54006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combos válidos após filtro/amostragem (incluindo sem UMAP): 40\n",
      "Exemplo (0): {'umap': {'n_neighbors': 10, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'random_state': 42}, 'hdb': {'min_cluster_size': 10, 'min_samples': None, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'vec': {'ngram_range': (1, 1), 'stop_words': ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tém', 'tínhamos', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos'], 'min_df': 2, 'max_df': 0.9}}\n"
     ]
    }
   ],
   "source": [
    "# Grade de hiperparâmetros (mini-sweep)\n",
    "\n",
    "from math import floor\n",
    "N_DOCS = len(docs_for_vectorizer)\n",
    "\n",
    "# === EXPANDE A GRADE PARA MAIS GRANULARIDADE ===\n",
    "grid_umap = {\n",
    "    \"n_neighbors\": [5, 10, 15, 30],\n",
    "    \"n_components\": [5, 10, 15],\n",
    "    \"min_dist\": [0.0],\n",
    "    \"metric\": [\"cosine\"],\n",
    "    \"random_state\": [SEED],\n",
    "}\n",
    "\n",
    "grid_hdb = {\n",
    "    \"min_cluster_size\": [5, 8, 10, 15],\n",
    "    \"min_samples\": [1, 5, None],\n",
    "    \"metric\": [\"euclidean\"],\n",
    "    \"cluster_selection_method\": [\"eom\", \"leaf\"],  # incluímos 'leaf' para granularidade\n",
    "    \"prediction_data\": [True],\n",
    "}\n",
    "\n",
    "grid_vec = {\n",
    "    \"ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \"stop_words\": [STOP_WORDS_PT],\n",
    "    \"min_df\": [1, 2, 3],\n",
    "    \"max_df\": [0.95, 0.99],\n",
    "}\n",
    "\n",
    "# Produto cartesiano (com UMAP)\n",
    "from itertools import product\n",
    "combos_all = []\n",
    "for u, h, v in product(\n",
    "    product(*grid_umap.values()),\n",
    "    product(*grid_hdb.values()),\n",
    "    product(*grid_vec.values())\n",
    "):\n",
    "    umap_kwargs = dict(zip(grid_umap.keys(), u))\n",
    "    hdb_kwargs = dict(zip(grid_hdb.keys(), h))\n",
    "    vec_kwargs = dict(zip(grid_vec.keys(), v))\n",
    "    combos_all.append({\"umap\": umap_kwargs, \"hdb\": hdb_kwargs, \"vec\": vec_kwargs})\n",
    "\n",
    "# Baseline (a mesma da sua versão anterior)\n",
    "baseline = {\n",
    "    \"umap\": {\"n_neighbors\": 10, \"n_components\": 5, \"min_dist\": 0.0, \"metric\": \"cosine\", \"random_state\": SEED},\n",
    "    \"hdb\": {\"min_cluster_size\": 10, \"min_samples\": None, \"metric\": \"euclidean\",\n",
    "            \"cluster_selection_method\": \"eom\", \"prediction_data\": True},\n",
    "    \"vec\": {\"ngram_range\": (1,1), \"stop_words\": STOP_WORDS_PT, \"min_df\": 2, \"max_df\": 0.9},\n",
    "}\n",
    "\n",
    "def _same(a, b): \n",
    "    return json.dumps(a, sort_keys=True) == json.dumps(b, sort_keys=True)\n",
    "\n",
    "combos = [baseline] + [c for c in combos_all if not _same(c, baseline)]\n",
    "\n",
    "# ---- filtro para evitar \"max_df corresponds to < documents than min_df\"\n",
    "def _valid_vec(v, n_docs: int) -> bool:\n",
    "    min_df = v[\"min_df\"]\n",
    "    max_df = v[\"max_df\"]\n",
    "    max_docs = floor(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    return max_docs >= int(min_df)\n",
    "\n",
    "combos = [c for c in combos if _valid_vec(c[\"vec\"], N_DOCS)]\n",
    "\n",
    "# === Amostragem reprodutível para conter o tamanho do sweep ===\n",
    "# Mantemos baseline + 39 aleatórios (total 40)\n",
    "N_MAX = 40\n",
    "rng = np.random.RandomState(SEED)\n",
    "if len(combos) > N_MAX:\n",
    "    others = combos[1:]\n",
    "    idx = rng.choice(len(others), size=N_MAX-1, replace=False)\n",
    "    combos = [combos[0]] + [others[i] for i in sorted(idx)]\n",
    "\n",
    "# === LOTE DE CONTROLE \"SEM UMAP\" (opcional, pequeno) ===\n",
    "# Gera combos onde 'umap' é None (HDBSCAN direto nos embeddings)\n",
    "combos_noumap_all = []\n",
    "for h, v in product(\n",
    "    product(*grid_hdb.values()),\n",
    "    product(*grid_vec.values())\n",
    "):\n",
    "    hdb_kwargs = dict(zip(grid_hdb.keys(), h))\n",
    "    vec_kwargs = dict(zip(grid_vec.keys(), v))\n",
    "    combos_noumap_all.append({\"umap\": None, \"hdb\": hdb_kwargs, \"vec\": vec_kwargs})\n",
    "\n",
    "# Aplica o mesmo filtro de min_df/max_df\n",
    "combos_noumap_all = [c for c in combos_noumap_all if _valid_vec(c[\"vec\"], N_DOCS)]\n",
    "\n",
    "# Mescla uma pequena amostra (até 10) de combos sem UMAP\n",
    "take = min(10, len(combos_noumap_all))\n",
    "idx_nu = rng.choice(len(combos_noumap_all), size=take, replace=False) if take > 0 else []\n",
    "combos = combos + [combos_noumap_all[i] for i in sorted(idx_nu)]\n",
    "\n",
    "# Se passou de N_MAX, reamostra mantendo baseline na frente\n",
    "if len(combos) > N_MAX:\n",
    "    others = combos[1:]\n",
    "    # escolhe N_MAX-1 dentre o restante (com UMAP e sem UMAP misturados)\n",
    "    idx = rng.choice(len(others), size=N_MAX-1, replace=False)\n",
    "    combos = [combos[0]] + [others[i] for i in sorted(idx)]\n",
    "\n",
    "print(f\"Combos válidos após filtro/amostragem (incluindo sem UMAP): {len(combos)}\")\n",
    "print(\"Exemplo (0):\", combos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a69e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "def write_json(path: Path, data: Dict[str, Any]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def extract_topic_words(topic_model: BERTopic, topk: int = 10) -> List[List[str]]:\n",
    "    topics_dict = topic_model.get_topics()\n",
    "    topic_words: List[List[str]] = []\n",
    "    for tid, pairs in topics_dict.items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        # BERTopic pode retornar None/[] para tópicos degenerados\n",
    "        if not pairs:\n",
    "            continue\n",
    "        # Garante apenas strings não vazias; respeita topk\n",
    "        toks = [str(w).strip() for (w, _) in pairs[:topk] if isinstance(w, (str, bytes)) and str(w).strip()]\n",
    "        if toks:\n",
    "            topic_words.append(toks)\n",
    "    return topic_words\n",
    "\n",
    "def simple_tokenize_docs(docs: List[str]) -> List[List[str]]:\n",
    "    # Tokenização simples coerente com o CountVectorizer (lower/space)\n",
    "    return [str(d).lower().split() for d in docs]\n",
    "\n",
    "def topic_diversity(topic_words: List[List[str]], topk: int = 10) -> float:\n",
    "    if not topic_words:\n",
    "        return float(\"nan\")\n",
    "    uniq = len(set([w for tw in topic_words for w in tw[:topk]]))\n",
    "    return uniq / (topk * len(topic_words))\n",
    "\n",
    "def outlier_rate(topics: List[int]) -> float:\n",
    "    topics = np.array(topics)\n",
    "    return float(np.mean(topics == -1))\n",
    "\n",
    "def _sanitize_topics_for_gensim(topic_words: List[List[str]]) -> List[List[str]]:\n",
    "    clean: List[List[str]] = []\n",
    "    for tw in topic_words:\n",
    "        if not isinstance(tw, (list, tuple)):\n",
    "            continue\n",
    "        toks = [str(t).strip() for t in tw if isinstance(t, (str, bytes)) and str(t).strip()]\n",
    "        if toks:\n",
    "            clean.append(toks)\n",
    "    return clean\n",
    "\n",
    "def compute_coherences(topic_words, tokenized_docs):\n",
    "    # c_npmi e c_uci com 'texts' (ambas aceitam texts); sanear antes de passar ao gensim\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "    cleaned = _sanitize_topics_for_gensim(topic_words)\n",
    "    if not cleaned:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "    c_npmi = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_npmi\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    c_uci = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_uci\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    return float(c_npmi), float(c_uci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47dcb58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cecce0116d486db3202ca16cb6ed92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sweep:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop do sweep: treino, avaliação e salvamento por trial\n",
    "\n",
    "# === Isolamento por execução: salva em subpasta RUN_ID ===\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"run_%Y%m%dT%H%M%SZ\")\n",
    "RUN_DIR = OUT_DIR / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "write_json(RUN_DIR/\"run_info.json\", {\n",
    "    \"run_id\": RUN_ID, \"seed\": SEED, \"versions\": versions,\n",
    "    \"n_docs\": len(docs_for_vectorizer), \"timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
    "})\n",
    "\n",
    "tokenized_docs = simple_tokenize_docs(docs_for_vectorizer)\n",
    "trials_summary = []\n",
    "errors_log = []\n",
    "\n",
    "N_DOCS_EFF = len(docs_for_vectorizer)  # usado para sanear min_df/max_df\n",
    "\n",
    "for i, cfg in enumerate(tqdm(combos, desc=\"Sweep\")):\n",
    "    trial_id = f\"trial_{i:02d}\"\n",
    "    tdir = RUN_DIR / trial_id\n",
    "    tdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- SANEAMENTO do Vectorizer (evita \"max_df corresponds to < documents than min_df\" e string 'portuguese')\n",
    "    vec_kwargs = cfg[\"vec\"].copy()\n",
    "    sw = vec_kwargs.get(\"stop_words\", None)\n",
    "    if isinstance(sw, str):\n",
    "        vec_kwargs[\"stop_words\"] = STOP_WORDS_PT\n",
    "\n",
    "    min_df = vec_kwargs.get(\"min_df\", 1)\n",
    "    max_df = vec_kwargs.get(\"max_df\", 1.0)\n",
    "    min_df_prop = (min_df / max(N_DOCS_EFF, 1)) if isinstance(min_df, int) else float(min_df)\n",
    "    max_df_prop = (max_df / max(N_DOCS_EFF, 1)) if isinstance(max_df, int) else float(max_df)\n",
    "    if max_df_prop < min_df_prop:\n",
    "        max_df_prop = min(1.0, max(min_df_prop + 1e-9, 0.999))\n",
    "    vec_kwargs[\"min_df\"] = min_df_prop\n",
    "    vec_kwargs[\"max_df\"] = max_df_prop\n",
    "\n",
    "    # Instâncias dos componentes\n",
    "    vectorizer_model = CountVectorizer(**vec_kwargs)\n",
    "    # >>> CORREÇÃO: aceitar combos \"sem UMAP\"\n",
    "    umap_model = None if (cfg[\"umap\"] is None) else UMAP(**cfg[\"umap\"])\n",
    "    hdbscan_model = HDBSCAN(**cfg[\"hdb\"])\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,          # pode ser None\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    started = datetime.now(timezone.utc).isoformat()\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        topics, probs = topic_model.fit_transform(\n",
    "            documents=docs_for_vectorizer,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        fit_time = time.time() - t0\n",
    "\n",
    "        # Artefatos primários\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info.to_csv(tdir/\"topic_info.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # Doc→tópico\n",
    "        if probs is not None and hasattr(probs, \"__array__\"):\n",
    "            rowmax = np.nanmax(probs, axis=1)\n",
    "            doc_prob = [float(x) if np.isfinite(x) else np.nan for x in rowmax]\n",
    "        else:\n",
    "            doc_prob = [np.nan] * len(topics)\n",
    "\n",
    "        doc_topics = pd.DataFrame({\n",
    "            \"DOC_ID\": df[\"DOC_ID\"].values,\n",
    "            \"topic\": topics,\n",
    "            \"prob\": doc_prob\n",
    "        })\n",
    "        # mantém alt e compatível com o comparador\n",
    "        doc_topics.to_csv(tdir/\"doc_topics_alt.csv\", index=False, encoding=\"utf-8\")\n",
    "        doc_topics.to_csv(tdir/\"doc_topics.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # c-TF-IDF e vocabulário\n",
    "        ctf = topic_model.c_tf_idf_\n",
    "        arr = ctf.toarray() if hasattr(ctf, \"toarray\") else np.asarray(ctf)\n",
    "        np.save(tdir/\"c_tf_idf.npy\", arr)\n",
    "\n",
    "        vocab = vectorizer_model.get_feature_names_out()\n",
    "        with open(tdir/\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(map(str, vocab)))\n",
    "\n",
    "        # === Métricas (com blindagens) ===\n",
    "        topic_words = extract_topic_words(topic_model, topk=10)\n",
    "        div = topic_diversity(topic_words, topk=10)\n",
    "\n",
    "        # usa a função já definida no notebook (não 'coherence_npmi', que não existe aqui)\n",
    "        try:\n",
    "            c_npmi, c_uci = compute_coherences(topic_words, tokenized_docs)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Coerência falhou em {trial_id}: {e}\")\n",
    "            c_npmi, c_uci = np.nan, np.nan\n",
    "\n",
    "        # >>> CORREÇÃO: converte para array antes de comparar com -1\n",
    "        topics_arr = np.asarray(topics)\n",
    "        out_pct = float(np.mean(topics_arr == -1))\n",
    "\n",
    "        n_topics_no_outlier = int((topic_info[\"Topic\"] != -1).sum())\n",
    "\n",
    "        metrics = {\n",
    "            \"c_npmi\": c_npmi,\n",
    "            \"c_uci\": c_uci,\n",
    "            \"topic_diversity@10\": div,\n",
    "            \"outlier_rate\": out_pct,\n",
    "            \"n_topics_excl_-1\": n_topics_no_outlier,\n",
    "            \"fit_time_sec\": fit_time\n",
    "        }\n",
    "        write_json(tdir/\"metrics.json\", metrics)\n",
    "\n",
    "        # Metadados\n",
    "        run_md = {\n",
    "            \"trial_id\": trial_id,\n",
    "            \"started_utc\": started,\n",
    "            \"finished_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"seed\": SEED,\n",
    "            \"versions\": versions,\n",
    "            \"paths\": {\n",
    "                \"input_csv\": str(IN_CSV.resolve()),\n",
    "                \"trial_dir\": str(tdir.resolve())\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"umap\": cfg[\"umap\"],\n",
    "                \"hdbscan\": cfg[\"hdb\"],\n",
    "                \"vectorizer\": {\n",
    "                    **{k: (list(v) if k==\"ngram_range\" else v) for k, v in cfg[\"vec\"].items()},\n",
    "                    \"_effective_min_df\": vec_kwargs[\"min_df\"],\n",
    "                    \"_effective_max_df\": vec_kwargs[\"max_df\"],\n",
    "                },\n",
    "                \"sbert_model\": sbert_name,\n",
    "            },\n",
    "            \"sizes\": {\n",
    "                \"n_docs\": len(df),\n",
    "                \"emb_dim\": int(embeddings.shape[1])\n",
    "            }\n",
    "        }\n",
    "        write_json(tdir/\"run_metadata.json\", run_md)\n",
    "\n",
    "        trials_summary.append({\n",
    "            \"trial_id\": trial_id,\n",
    "            \"metrics\": metrics,\n",
    "            \"cfg\": cfg\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        err = {\"trial_id\": trial_id, \"error\": repr(e)}\n",
    "        errors_log.append(err)\n",
    "        write_json(tdir/\"error.json\", err)\n",
    "        print(f\"[WARN] {trial_id} falhou: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "467f87d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>n_topics</th>\n",
       "      <th>out_rate</th>\n",
       "      <th>c_npmi</th>\n",
       "      <th>c_uci</th>\n",
       "      <th>umap_used</th>\n",
       "      <th>umap_n_neighbors</th>\n",
       "      <th>umap_n_components</th>\n",
       "      <th>umap_min_dist</th>\n",
       "      <th>hdb_min_cluster_size</th>\n",
       "      <th>hdb_min_samples</th>\n",
       "      <th>hdb_method</th>\n",
       "      <th>vec_ngr</th>\n",
       "      <th>vec_min_df</th>\n",
       "      <th>vec_max_df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>trial_32</td>\n",
       "      <td>39</td>\n",
       "      <td>0.208038</td>\n",
       "      <td>-0.229013</td>\n",
       "      <td>-8.331270</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trial_03</td>\n",
       "      <td>31</td>\n",
       "      <td>0.229314</td>\n",
       "      <td>-0.263635</td>\n",
       "      <td>-8.896085</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trial_04</td>\n",
       "      <td>31</td>\n",
       "      <td>0.229314</td>\n",
       "      <td>-0.264518</td>\n",
       "      <td>-8.878906</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trial_07</td>\n",
       "      <td>27</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>-0.227258</td>\n",
       "      <td>-8.138980</td>\n",
       "      <td>True</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trial_15</td>\n",
       "      <td>25</td>\n",
       "      <td>0.196217</td>\n",
       "      <td>-0.267536</td>\n",
       "      <td>-9.034093</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>trial_35</td>\n",
       "      <td>23</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.203839</td>\n",
       "      <td>-7.658139</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trial_05</td>\n",
       "      <td>23</td>\n",
       "      <td>0.146572</td>\n",
       "      <td>-0.212162</td>\n",
       "      <td>-7.861008</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eom</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>trial_34</td>\n",
       "      <td>23</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>-0.201547</td>\n",
       "      <td>-7.457811</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>trial_19</td>\n",
       "      <td>22</td>\n",
       "      <td>0.392435</td>\n",
       "      <td>-0.200098</td>\n",
       "      <td>-7.654399</td>\n",
       "      <td>True</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leaf</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trial_06</td>\n",
       "      <td>18</td>\n",
       "      <td>0.113475</td>\n",
       "      <td>-0.184597</td>\n",
       "      <td>-6.233820</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>eom</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trial_id  n_topics  out_rate    c_npmi     c_uci  umap_used  \\\n",
       "32  trial_32        39  0.208038 -0.229013 -8.331270      False   \n",
       "3   trial_03        31  0.229314 -0.263635 -8.896085       True   \n",
       "4   trial_04        31  0.229314 -0.264518 -8.878906       True   \n",
       "7   trial_07        27  0.170213 -0.227258 -8.138980       True   \n",
       "15  trial_15        25  0.196217 -0.267536 -9.034093       True   \n",
       "35  trial_35        23  0.333333 -0.203839 -7.658139      False   \n",
       "5   trial_05        23  0.146572 -0.212162 -7.861008       True   \n",
       "34  trial_34        23  0.302600 -0.201547 -7.457811      False   \n",
       "19  trial_19        22  0.392435 -0.200098 -7.654399       True   \n",
       "6   trial_06        18  0.113475 -0.184597 -6.233820       True   \n",
       "\n",
       "    umap_n_neighbors  umap_n_components  umap_min_dist  hdb_min_cluster_size  \\\n",
       "32               NaN                NaN            NaN                     5   \n",
       "3                5.0               10.0            0.0                     5   \n",
       "4                5.0               10.0            0.0                     5   \n",
       "7               10.0                5.0            0.0                     8   \n",
       "15              15.0                5.0            0.0                     8   \n",
       "35               NaN                NaN            NaN                     5   \n",
       "5                5.0               15.0            0.0                     5   \n",
       "34               NaN                NaN            NaN                     5   \n",
       "19              30.0                5.0            0.0                     5   \n",
       "6                5.0               15.0            0.0                    10   \n",
       "\n",
       "    hdb_min_samples hdb_method vec_ngr  vec_min_df  vec_max_df  \n",
       "32              1.0       leaf  (1, 3)           2        0.95  \n",
       "3               5.0       leaf  (1, 1)           1        0.95  \n",
       "4               5.0       leaf  (1, 1)           3        0.99  \n",
       "7               1.0       leaf  (1, 2)           3        0.95  \n",
       "15              1.0       leaf  (1, 1)           3        0.95  \n",
       "35              NaN       leaf  (1, 3)           2        0.95  \n",
       "5               NaN        eom  (1, 2)           2        0.95  \n",
       "34              NaN       leaf  (1, 3)           1        0.99  \n",
       "19              NaN       leaf  (1, 2)           3        0.99  \n",
       "6               1.0        eom  (1, 3)           2        0.95  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n_topics_mean</th>\n",
       "      <th>n_topics_max</th>\n",
       "      <th>trials</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>umap_used</th>\n",
       "      <th>hdb_method</th>\n",
       "      <th>hdb_min_cluster_size</th>\n",
       "      <th>hdb_min_samples</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <th>leaf</th>\n",
       "      <th>5</th>\n",
       "      <th>1.0</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">leaf</th>\n",
       "      <th>5</th>\n",
       "      <th>5.0</th>\n",
       "      <td>26.666667</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1.0</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eom</th>\n",
       "      <th>10</th>\n",
       "      <th>1.0</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <th>leaf</th>\n",
       "      <th>8</th>\n",
       "      <th>5.0</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>leaf</th>\n",
       "      <th>8</th>\n",
       "      <th>5.0</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <th>leaf</th>\n",
       "      <th>15</th>\n",
       "      <th>1.0</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">leaf</th>\n",
       "      <th>10</th>\n",
       "      <th>5.0</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>1.0</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">eom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">15</th>\n",
       "      <th>1.0</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leaf</th>\n",
       "      <th>15</th>\n",
       "      <th>5.0</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">eom</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>1.0</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>5.0</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           n_topics_mean  \\\n",
       "umap_used hdb_method hdb_min_cluster_size hdb_min_samples                  \n",
       "False     leaf       5                    1.0                  39.000000   \n",
       "True      leaf       5                    5.0                  26.666667   \n",
       "                     8                    1.0                  26.000000   \n",
       "          eom        10                   1.0                  18.000000   \n",
       "False     leaf       8                    5.0                  16.000000   \n",
       "True      leaf       8                    5.0                  15.000000   \n",
       "False     leaf       15                   1.0                  11.000000   \n",
       "True      leaf       10                   5.0                  11.000000   \n",
       "                     15                   1.0                  10.000000   \n",
       "          eom        15                   1.0                  10.000000   \n",
       "                                          5.0                   9.000000   \n",
       "          leaf       15                   5.0                   8.000000   \n",
       "          eom        8                    1.0                   2.000000   \n",
       "                                          5.0                   2.000000   \n",
       "                     10                   5.0                   2.000000   \n",
       "\n",
       "                                                           n_topics_max  \\\n",
       "umap_used hdb_method hdb_min_cluster_size hdb_min_samples                 \n",
       "False     leaf       5                    1.0                        39   \n",
       "True      leaf       5                    5.0                        31   \n",
       "                     8                    1.0                        27   \n",
       "          eom        10                   1.0                        18   \n",
       "False     leaf       8                    5.0                        16   \n",
       "True      leaf       8                    5.0                        15   \n",
       "False     leaf       15                   1.0                        11   \n",
       "True      leaf       10                   5.0                        11   \n",
       "                     15                   1.0                        11   \n",
       "          eom        15                   1.0                        10   \n",
       "                                          5.0                         9   \n",
       "          leaf       15                   5.0                         8   \n",
       "          eom        8                    1.0                         2   \n",
       "                                          5.0                         2   \n",
       "                     10                   5.0                         2   \n",
       "\n",
       "                                                           trials  \n",
       "umap_used hdb_method hdb_min_cluster_size hdb_min_samples          \n",
       "False     leaf       5                    1.0                   1  \n",
       "True      leaf       5                    5.0                   3  \n",
       "                     8                    1.0                   2  \n",
       "          eom        10                   1.0                   1  \n",
       "False     leaf       8                    5.0                   1  \n",
       "True      leaf       8                    5.0                   2  \n",
       "False     leaf       15                   1.0                   1  \n",
       "True      leaf       10                   5.0                   1  \n",
       "                     15                   1.0                   3  \n",
       "          eom        15                   1.0                   1  \n",
       "                                          5.0                   2  \n",
       "          leaf       15                   5.0                   3  \n",
       "          eom        8                    1.0                   1  \n",
       "                                          5.0                   1  \n",
       "                     10                   5.0                   1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# depois do sweep:\n",
    "def _get(d: dict, path, default=np.nan):\n",
    "    \"\"\"Acesso seguro a d[k1][k2]...; retorna default se faltar algo ou se algum nível for None.\"\"\"\n",
    "    cur = d\n",
    "    for k in path:\n",
    "        if cur is None:\n",
    "            return default\n",
    "        if isinstance(cur, dict):\n",
    "            cur = cur.get(k, default)\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "rows = []\n",
    "for t in trials_summary:\n",
    "    cfg = t[\"cfg\"]\n",
    "    rows.append(dict(\n",
    "        trial_id=t[\"trial_id\"],\n",
    "        n_topics=t[\"metrics\"][\"n_topics_excl_-1\"],\n",
    "        out_rate=t[\"metrics\"][\"outlier_rate\"],\n",
    "        c_npmi=t[\"metrics\"][\"c_npmi\"],\n",
    "        c_uci=t[\"metrics\"][\"c_uci\"],\n",
    "\n",
    "        # UMAP pode ser None nos combos \"sem UMAP\"\n",
    "        umap_used = cfg[\"umap\"] is not None,\n",
    "        umap_n_neighbors = _get(cfg, [\"umap\", \"n_neighbors\"]),\n",
    "        umap_n_components = _get(cfg, [\"umap\", \"n_components\"]),\n",
    "        umap_min_dist = _get(cfg, [\"umap\", \"min_dist\"]),\n",
    "\n",
    "        # HDBSCAN\n",
    "        hdb_min_cluster_size = cfg[\"hdb\"][\"min_cluster_size\"],\n",
    "        hdb_min_samples = cfg[\"hdb\"][\"min_samples\"],\n",
    "        hdb_method = cfg[\"hdb\"][\"cluster_selection_method\"],\n",
    "\n",
    "        # Vectorizer (valores conforme definidos na grade original)\n",
    "        vec_ngr = cfg[\"vec\"][\"ngram_range\"],\n",
    "        vec_min_df = cfg[\"vec\"][\"min_df\"],\n",
    "        vec_max_df = cfg[\"vec\"][\"max_df\"],\n",
    "    ))\n",
    "\n",
    "df_sweep = pd.DataFrame(rows)\n",
    "\n",
    "# Salva um resumo desta execução\n",
    "df_sweep.to_csv(RUN_DIR/\"sweep_summary_alt.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Top em nº de tópicos\n",
    "display(df_sweep.sort_values(\"n_topics\", ascending=False).head(10))\n",
    "\n",
    "# Efeito de parâmetros-chave (inclui flag umap_used para comparar com/sem UMAP)\n",
    "display(\n",
    "    df_sweep\n",
    "      .groupby([\"umap_used\", \"hdb_method\",\"hdb_min_cluster_size\",\"hdb_min_samples\"])\n",
    "      .agg(n_topics_mean=(\"n_topics\",\"mean\"),\n",
    "           n_topics_max=(\"n_topics\",\"max\"),\n",
    "           trials=(\"trial_id\",\"count\"))\n",
    "      .sort_values([\"n_topics_max\",\"n_topics_mean\",\"trials\"], ascending=False)\n",
    "      .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded36dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-10 trials por critério composto:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>c_npmi</th>\n",
       "      <th>topic_diversity</th>\n",
       "      <th>outlier_rate</th>\n",
       "      <th>n_topics</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>n_topics_penalty</th>\n",
       "      <th>rank_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trial_00</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trial_01</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trial_10</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trial_11</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trial_13</td>\n",
       "      <td>-0.054650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trial_16</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trial_29</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trial_37</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trial_38</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trial_20</td>\n",
       "      <td>-0.131737</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trial_id    c_npmi  topic_diversity  outlier_rate  n_topics   r1    r2  \\\n",
       "0  trial_00 -0.054650              1.0           0.0         2  1.0   1.0   \n",
       "1  trial_01 -0.054650              1.0           0.0         2  1.0   1.0   \n",
       "2  trial_10 -0.054650              1.0           0.0         2  1.0   1.0   \n",
       "3  trial_11 -0.054650              1.0           0.0         2  1.0   1.0   \n",
       "4  trial_13 -0.054650              1.0           0.0         2  1.0   1.0   \n",
       "5  trial_16 -0.131737              1.0           0.0         2  6.0   1.0   \n",
       "6  trial_29 -0.131737              1.0           0.0         2  6.0   1.0   \n",
       "7  trial_37 -0.131737              1.0           0.0         2  6.0   1.0   \n",
       "8  trial_38 -0.131737              1.0           0.0         2  6.0   1.0   \n",
       "9  trial_20 -0.131737              0.7           0.0         2  6.0  38.0   \n",
       "\n",
       "    r3  n_topics_penalty  rank_sum  \n",
       "0  1.0               0.8       7.0  \n",
       "1  1.0               0.8       7.0  \n",
       "2  1.0               0.8       7.0  \n",
       "3  1.0               0.8       7.0  \n",
       "4  1.0               0.8       7.0  \n",
       "5  1.0               0.8      12.0  \n",
       "6  1.0               0.8      12.0  \n",
       "7  1.0               0.8      12.0  \n",
       "8  1.0               0.8      12.0  \n",
       "9  1.0               0.8      49.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vencedora: trial_00\n"
     ]
    }
   ],
   "source": [
    "# Ranking e escolha da melhor configuração\n",
    "\n",
    "# Ranking composto (ordenação por: c_npmi desc, topic_diversity desc, outlier_rate asc + penalização por n_tópicos)\n",
    "if not trials_summary:\n",
    "    raise RuntimeError(\"Nenhum trial concluído. Verifique errors_log e configurações.\")\n",
    "\n",
    "df_rank = pd.DataFrame([{\n",
    "    \"trial_id\": t[\"trial_id\"],\n",
    "    \"c_npmi\": t[\"metrics\"][\"c_npmi\"],\n",
    "    \"topic_diversity\": t[\"metrics\"][\"topic_diversity@10\"],\n",
    "    \"outlier_rate\": t[\"metrics\"][\"outlier_rate\"],\n",
    "    \"n_topics\": t[\"metrics\"][\"n_topics_excl_-1\"]\n",
    "} for t in trials_summary])\n",
    "\n",
    "# Garante numéricos para evitar NaNs silenciosos em rank\n",
    "for col in [\"c_npmi\", \"topic_diversity\", \"outlier_rate\", \"n_topics\"]:\n",
    "    df_rank[col] = pd.to_numeric(df_rank[col], errors=\"coerce\")\n",
    "\n",
    "# Ranks principais\n",
    "df_rank[\"r1\"] = df_rank[\"c_npmi\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r2\"] = df_rank[\"topic_diversity\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r3\"] = df_rank[\"outlier_rate\"].rank(ascending=True, method=\"min\")\n",
    "\n",
    "# Penalização por distância a uma faixa alvo (ajuste conforme o domínio)\n",
    "LOW, HIGH = 10, 40\n",
    "def penalty(n):\n",
    "    if pd.isna(n):\n",
    "        return 1.0\n",
    "    n = float(n)\n",
    "    if n < LOW:\n",
    "        return (LOW - n) / LOW\n",
    "    if n > HIGH:\n",
    "        return (n - HIGH) / HIGH\n",
    "    return 0.0\n",
    "\n",
    "df_rank[\"n_topics_penalty\"] = df_rank[\"n_topics\"].apply(penalty)\n",
    "\n",
    "# Peso da penalização (ajustável)\n",
    "PENALTY_WEIGHT = 5.0\n",
    "df_rank[\"rank_sum\"] = df_rank[[\"r1\", \"r2\", \"r3\"]].sum(axis=1) + PENALTY_WEIGHT * df_rank[\"n_topics_penalty\"]\n",
    "\n",
    "# Ordena, escolhe vencedora e persiste\n",
    "df_rank = df_rank.sort_values([\"rank_sum\", \"r1\", \"r2\", \"r3\"]).reset_index(drop=True)\n",
    "best_trial_id = df_rank.loc[0, \"trial_id\"]\n",
    "best_row = df_rank.loc[0].to_dict()\n",
    "\n",
    "print(\"TOP-10 trials por critério composto:\")\n",
    "display(df_rank.head(10))\n",
    "print(\"Vencedora:\", best_trial_id)\n",
    "\n",
    "# Persistência no RUN_DIR\n",
    "df_rank.to_csv(RUN_DIR/\"ranking_alt.csv\", index=False, encoding=\"utf-8\")\n",
    "write_json(RUN_DIR/\"winner_alt.json\", {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"best_trial_id\": best_trial_id,\n",
    "    \"best_row\": best_row,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic)",
   "language": "python",
   "name": "bertopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
