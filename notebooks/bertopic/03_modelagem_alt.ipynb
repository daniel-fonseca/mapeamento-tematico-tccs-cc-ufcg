{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57d9cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSIONS: {\n",
      "  \"python\": \"3.12.2\",\n",
      "  \"bertopic\": \"0.16.0\",\n",
      "  \"sentence-transformers\": \"2.6.1\",\n",
      "  \"umap-learn\": \"0.5.6\",\n",
      "  \"hdbscan\": \"0.8.33\",\n",
      "  \"scikit-learn\": \"1.4.2\",\n",
      "  \"gensim\": \"4.3.3\",\n",
      "  \"numpy\": \"1.26.4\",\n",
      "  \"pandas\": \"2.2.2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths e seed\n",
    "\n",
    "import os, json, math, platform, warnings, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from itertools import product\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "BASE = Path(\"../../data\")\n",
    "IN_CSV = BASE/\"interim\"/\"bertopic\"/\"prep.csv\"\n",
    "OUT_DIR = BASE/\"processed\"/\"bertopic\"\n",
    "FIGS = Path(\"../../reports/figs\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def _get_ver(pkg):\n",
    "    try:\n",
    "        import importlib.metadata as im\n",
    "        return im.version(pkg)\n",
    "    except Exception:\n",
    "        return \"NA\"\n",
    "\n",
    "versions = {\n",
    "    \"python\": platform.python_version(),\n",
    "    \"bertopic\": _get_ver(\"bertopic\"),\n",
    "    \"sentence-transformers\": _get_ver(\"sentence-transformers\"),\n",
    "    \"umap-learn\": _get_ver(\"umap-learn\"),\n",
    "    \"hdbscan\": _get_ver(\"hdbscan\"),\n",
    "    \"scikit-learn\": _get_ver(\"scikit-learn\"),\n",
    "    \"gensim\": _get_ver(\"gensim\"),\n",
    "    \"numpy\": _get_ver(\"numpy\"),\n",
    "    \"pandas\": _get_ver(\"pandas\")\n",
    "}\n",
    "print(\"VERSIONS:\", json.dumps(versions, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "144b3a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOP_WORDS_PT carregadas: 207 termos\n"
     ]
    }
   ],
   "source": [
    "# Stopwords em português (para CountVectorizer)\n",
    "try:\n",
    "    from nltk.corpus import stopwords as nltk_stop\n",
    "    STOP_WORDS_PT = sorted(set(nltk_stop.words('portuguese')))\n",
    "    print(f\"STOP_WORDS_PT carregadas: {len(STOP_WORDS_PT)} termos\")\n",
    "except Exception as e:\n",
    "    # fallback mínimo para não travar caso NLTK falhe (mantém pipeline funcionando)\n",
    "    print(\"[WARN] NLTK indisponível; usando fallback reduzido:\", e)\n",
    "    STOP_WORDS_PT = sorted(set(\"\"\"\n",
    "a à acerca agora ai ainda além algo alguem alguns algumas algum alguma ambos ambas ante antes ao aos após aquela aquelas aquele aqueles aquilo as assim até através cada quase com como contra contudo cujo cuja cujos cujas da das de dela delas dele deles depois desde desta deste disso disto do dos e é ela elas ele eles em entre era eram essa essas esse esses esta estas este estes estou eu foi foram fosse fossem fui há isso isto já la lá lhe lhes mais mas me mesmo mesmoa mesmos mesmas minha minhas meu meus muito muita muitas muitos não na nas nem no nos nós o os ou para pela pelas pelo pelos pouca poucas pouco poucos por porque porém pra qual quais quando que quem se sem sempre sendo ser seu seus sob sobre sua suas também tão tão tem tenho ter teu teus tua tuas tudo um uma umas uns\n",
    "\"\"\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9174bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N documentos: 423\n",
      "Docs vazios (vectorizer): 14\n",
      "Comprimento (chars) — quantis: {0.0: 0.0, 0.25: 894.5, 0.5: 1123.0, 0.75: 1322.5, 0.95: 1729.2999999999997, 1.0: 2309.0}\n"
     ]
    }
   ],
   "source": [
    "# Leitura e preparação dos textos\n",
    "\n",
    "df = pd.read_csv(IN_CSV, encoding=\"utf-8\")\n",
    "\n",
    "required_cols = {\"DOC_ID\", \"resumo\", \"RESUMO_PREP_BERTOPIC\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"Colunas ausentes: {missing}\"\n",
    "\n",
    "docs_for_embeddings = (\n",
    "    df[\"resumo\"].fillna(df[\"RESUMO_PREP_BERTOPIC\"]).astype(str).tolist()\n",
    ")\n",
    "docs_for_vectorizer = (\n",
    "    df[\"RESUMO_PREP_BERTOPIC\"].fillna(\"\").astype(str).tolist()\n",
    ")\n",
    "\n",
    "lens = pd.Series([len(x) for x in docs_for_vectorizer])\n",
    "print(\"N documentos:\", len(df))\n",
    "print(\"Docs vazios (vectorizer):\", sum([len(x.strip()) == 0 for x in docs_for_vectorizer]))\n",
    "print(\"Comprimento (chars) — quantis:\", lens.quantile([0.0, 0.25, 0.5, 0.75, 0.95, 1.0]).to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4df6eef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de76d17323e497ba88c5a36a3f493f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape=(423, 384) | device=cpu | time=11.4s\n"
     ]
    }
   ],
   "source": [
    "# Embeddings (SBERT)\n",
    "\n",
    "# Modelo multilíngue leve que preserva semântica de sentenças (SBERT)\n",
    "# BERT → base dos embeddings contextuais [Devlin et al.] usados em SBERT\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sbert_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "start = time.time()\n",
    "sbert = SentenceTransformer(sbert_name, device=device)\n",
    "embeddings = sbert.encode(\n",
    "    docs_for_embeddings,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "print(f\"Embeddings shape={embeddings.shape} | device={device} | time={elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fca54006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combos válidos após filtro/amostragem (incluindo sem UMAP): 40\n",
      "Exemplo (0): {'umap': {'n_neighbors': 10, 'n_components': 5, 'min_dist': 0.0, 'metric': 'cosine', 'random_state': 42}, 'hdb': {'min_cluster_size': 10, 'min_samples': None, 'metric': 'euclidean', 'cluster_selection_method': 'eom', 'prediction_data': True}, 'vec': {'ngram_range': (1, 1), 'stop_words': ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'essa', 'essas', 'esse', 'esses', 'esta', 'estamos', 'estar', 'estas', 'estava', 'estavam', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéramos', 'estivéssemos', 'estou', 'está', 'estávamos', 'estão', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'forem', 'formos', 'fosse', 'fossem', 'fui', 'fôramos', 'fôssemos', 'haja', 'hajam', 'hajamos', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houveram', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houvermos', 'houverá', 'houverão', 'houveríamos', 'houvesse', 'houvessem', 'houvéramos', 'houvéssemos', 'há', 'hão', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'nas', 'nem', 'no', 'nos', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'não', 'nós', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'serei', 'seremos', 'seria', 'seriam', 'será', 'serão', 'seríamos', 'seu', 'seus', 'somos', 'sou', 'sua', 'suas', 'são', 'só', 'também', 'te', 'tem', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terei', 'teremos', 'teria', 'teriam', 'terá', 'terão', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéramos', 'tivéssemos', 'tu', 'tua', 'tuas', 'tém', 'tínhamos', 'um', 'uma', 'você', 'vocês', 'vos', 'à', 'às', 'é', 'éramos'], 'min_df': 2, 'max_df': 0.9}}\n"
     ]
    }
   ],
   "source": [
    "# Grade de hiperparâmetros (mini-sweep)\n",
    "\n",
    "from math import floor\n",
    "N_DOCS = len(docs_for_vectorizer)\n",
    "\n",
    "# === EXPANDE A GRADE PARA MAIS GRANULARIDADE ===\n",
    "grid_umap = {\n",
    "    \"n_neighbors\": [5, 10, 15, 30],\n",
    "    \"n_components\": [5, 10, 15],\n",
    "    \"min_dist\": [0.0],\n",
    "    \"metric\": [\"cosine\"],\n",
    "    \"random_state\": [SEED],\n",
    "}\n",
    "\n",
    "grid_hdb = {\n",
    "    \"min_cluster_size\": [5, 8, 10, 15],\n",
    "    \"min_samples\": [1, 5, None],\n",
    "    \"metric\": [\"euclidean\"],\n",
    "    \"cluster_selection_method\": [\"eom\", \"leaf\"],  # incluímos 'leaf' para granularidade\n",
    "    \"prediction_data\": [True],\n",
    "}\n",
    "\n",
    "grid_vec = {\n",
    "    \"ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \"stop_words\": [STOP_WORDS_PT],\n",
    "    \"min_df\": [1, 2, 3],\n",
    "    \"max_df\": [0.95, 0.99],\n",
    "}\n",
    "\n",
    "# Produto cartesiano (com UMAP)\n",
    "from itertools import product\n",
    "combos_all = []\n",
    "for u, h, v in product(\n",
    "    product(*grid_umap.values()),\n",
    "    product(*grid_hdb.values()),\n",
    "    product(*grid_vec.values())\n",
    "):\n",
    "    umap_kwargs = dict(zip(grid_umap.keys(), u))\n",
    "    hdb_kwargs = dict(zip(grid_hdb.keys(), h))\n",
    "    vec_kwargs = dict(zip(grid_vec.keys(), v))\n",
    "    combos_all.append({\"umap\": umap_kwargs, \"hdb\": hdb_kwargs, \"vec\": vec_kwargs})\n",
    "\n",
    "# Baseline (a mesma da sua versão anterior)\n",
    "baseline = {\n",
    "    \"umap\": {\"n_neighbors\": 10, \"n_components\": 5, \"min_dist\": 0.0, \"metric\": \"cosine\", \"random_state\": SEED},\n",
    "    \"hdb\": {\"min_cluster_size\": 10, \"min_samples\": None, \"metric\": \"euclidean\",\n",
    "            \"cluster_selection_method\": \"eom\", \"prediction_data\": True},\n",
    "    \"vec\": {\"ngram_range\": (1,1), \"stop_words\": STOP_WORDS_PT, \"min_df\": 2, \"max_df\": 0.9},\n",
    "}\n",
    "\n",
    "def _same(a, b): \n",
    "    return json.dumps(a, sort_keys=True) == json.dumps(b, sort_keys=True)\n",
    "\n",
    "combos = [baseline] + [c for c in combos_all if not _same(c, baseline)]\n",
    "\n",
    "# ---- filtro para evitar \"max_df corresponds to < documents than min_df\"\n",
    "def _valid_vec(v, n_docs: int) -> bool:\n",
    "    min_df = v[\"min_df\"]\n",
    "    max_df = v[\"max_df\"]\n",
    "    max_docs = floor(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    return max_docs >= int(min_df)\n",
    "\n",
    "combos = [c for c in combos if _valid_vec(c[\"vec\"], N_DOCS)]\n",
    "\n",
    "# === Amostragem reprodutível para conter o tamanho do sweep ===\n",
    "# Mantemos baseline + 39 aleatórios (total 40)\n",
    "N_MAX = 40\n",
    "rng = np.random.RandomState(SEED)\n",
    "if len(combos) > N_MAX:\n",
    "    others = combos[1:]\n",
    "    idx = rng.choice(len(others), size=N_MAX-1, replace=False)\n",
    "    combos = [combos[0]] + [others[i] for i in sorted(idx)]\n",
    "\n",
    "# === LOTE DE CONTROLE \"SEM UMAP\" (opcional, pequeno) ===\n",
    "# Gera combos onde 'umap' é None (HDBSCAN direto nos embeddings)\n",
    "combos_noumap_all = []\n",
    "for h, v in product(\n",
    "    product(*grid_hdb.values()),\n",
    "    product(*grid_vec.values())\n",
    "):\n",
    "    hdb_kwargs = dict(zip(grid_hdb.keys(), h))\n",
    "    vec_kwargs = dict(zip(grid_vec.keys(), v))\n",
    "    combos_noumap_all.append({\"umap\": None, \"hdb\": hdb_kwargs, \"vec\": vec_kwargs})\n",
    "\n",
    "# Aplica o mesmo filtro de min_df/max_df\n",
    "combos_noumap_all = [c for c in combos_noumap_all if _valid_vec(c[\"vec\"], N_DOCS)]\n",
    "\n",
    "# Mescla uma pequena amostra (até 10) de combos sem UMAP\n",
    "take = min(10, len(combos_noumap_all))\n",
    "idx_nu = rng.choice(len(combos_noumap_all), size=take, replace=False) if take > 0 else []\n",
    "combos = combos + [combos_noumap_all[i] for i in sorted(idx_nu)]\n",
    "\n",
    "# Se passou de N_MAX, reamostra mantendo baseline na frente\n",
    "if len(combos) > N_MAX:\n",
    "    others = combos[1:]\n",
    "    # escolhe N_MAX-1 dentre o restante (com UMAP e sem UMAP misturados)\n",
    "    idx = rng.choice(len(others), size=N_MAX-1, replace=False)\n",
    "    combos = [combos[0]] + [others[i] for i in sorted(idx)]\n",
    "\n",
    "print(f\"Combos válidos após filtro/amostragem (incluindo sem UMAP): {len(combos)}\")\n",
    "print(\"Exemplo (0):\", combos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a69e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares\n",
    "\n",
    "def write_json(path: Path, data: Dict[str, Any]):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def extract_topic_words(topic_model: BERTopic, topk: int = 10) -> List[List[str]]:\n",
    "    topics_dict = topic_model.get_topics()\n",
    "    topic_words: List[List[str]] = []\n",
    "    for tid, pairs in topics_dict.items():\n",
    "        if tid == -1:\n",
    "            continue\n",
    "        # BERTopic pode retornar None/[] para tópicos degenerados\n",
    "        if not pairs:\n",
    "            continue\n",
    "        # Garante apenas strings não vazias; respeita topk\n",
    "        toks = [str(w).strip() for (w, _) in pairs[:topk] if isinstance(w, (str, bytes)) and str(w).strip()]\n",
    "        if toks:\n",
    "            topic_words.append(toks)\n",
    "    return topic_words\n",
    "\n",
    "def simple_tokenize_docs(docs: List[str]) -> List[List[str]]:\n",
    "    # Tokenização simples coerente com o CountVectorizer (lower/space)\n",
    "    return [str(d).lower().split() for d in docs]\n",
    "\n",
    "def topic_diversity(topic_words: List[List[str]], topk: int = 10) -> float:\n",
    "    if not topic_words:\n",
    "        return float(\"nan\")\n",
    "    uniq = len(set([w for tw in topic_words for w in tw[:topk]]))\n",
    "    return uniq / (topk * len(topic_words))\n",
    "\n",
    "def outlier_rate(topics: List[int]) -> float:\n",
    "    topics = np.array(topics)\n",
    "    return float(np.mean(topics == -1))\n",
    "\n",
    "def _sanitize_topics_for_gensim(topic_words: List[List[str]]) -> List[List[str]]:\n",
    "    clean: List[List[str]] = []\n",
    "    for tw in topic_words:\n",
    "        if not isinstance(tw, (list, tuple)):\n",
    "            continue\n",
    "        toks = [str(t).strip() for t in tw if isinstance(t, (str, bytes)) and str(t).strip()]\n",
    "        if toks:\n",
    "            clean.append(toks)\n",
    "    return clean\n",
    "\n",
    "def compute_coherences(topic_words, tokenized_docs):\n",
    "    # c_npmi e c_uci com 'texts' (ambas aceitam texts); sanear antes de passar ao gensim\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "    cleaned = _sanitize_topics_for_gensim(topic_words)\n",
    "    if not cleaned:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "    c_npmi = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_npmi\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    c_uci = CoherenceModel(\n",
    "        topics=cleaned, texts=tokenized_docs, dictionary=dictionary, coherence=\"c_uci\"\n",
    "    ).get_coherence()\n",
    "\n",
    "    return float(c_npmi), float(c_uci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47dcb58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886febdd449f490f8be0693b9576ebad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sweep:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_ID: run_20250828T203157Z | trials OK: 40 | trials com erro: 0\n"
     ]
    }
   ],
   "source": [
    "# Loop do sweep: treino, avaliação e salvamento por trial\n",
    "\n",
    "# === Isolamento por execução: salva em subpasta RUN_ID ===\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"run_%Y%m%dT%H%M%SZ\")\n",
    "RUN_DIR = OUT_DIR / RUN_ID\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "write_json(RUN_DIR/\"run_info_alt.json\", {\n",
    "    \"run_id\": RUN_ID, \"seed\": SEED, \"versions\": versions,\n",
    "    \"n_docs\": len(docs_for_vectorizer), \"timestamp_utc\": datetime.now(timezone.utc).isoformat()\n",
    "})\n",
    "\n",
    "trials_summary = []\n",
    "errors_log = []\n",
    "\n",
    "N_DOCS_EFF = len(docs_for_vectorizer)\n",
    "\n",
    "for i, cfg in enumerate(tqdm(combos, desc=\"Sweep\")):\n",
    "    trial_id = f\"trial_{i:02d}\"\n",
    "    tdir = RUN_DIR / trial_id\n",
    "    tdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- SANEAMENTO do Vectorizer ---\n",
    "    vec_kwargs = cfg[\"vec\"].copy()\n",
    "    if isinstance(vec_kwargs.get(\"stop_words\", None), str):\n",
    "        vec_kwargs[\"stop_words\"] = STOP_WORDS_PT\n",
    "\n",
    "    min_df = vec_kwargs.get(\"min_df\", 1)\n",
    "    max_df = vec_kwargs.get(\"max_df\", 1.0)\n",
    "    min_df_prop = (min_df / max(N_DOCS_EFF, 1)) if isinstance(min_df, int) else float(min_df)\n",
    "    max_df_prop = (max_df / max(N_DOCS_EFF, 1)) if isinstance(max_df, int) else float(max_df)\n",
    "    if max_df_prop < min_df_prop:\n",
    "        max_df_prop = min(1.0, max(min_df_prop + 1e-9, 0.999))\n",
    "    vec_kwargs[\"min_df\"] = min_df_prop\n",
    "    vec_kwargs[\"max_df\"] = max_df_prop\n",
    "\n",
    "    # Instâncias\n",
    "    vectorizer_model = CountVectorizer(**vec_kwargs)\n",
    "    # >>> AQUI: permite combos sem UMAP\n",
    "    umap_model = None if (cfg[\"umap\"] is None) else UMAP(**cfg[\"umap\"])\n",
    "    hdbscan_model = HDBSCAN(**cfg[\"hdb\"])\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=None,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    started = datetime.now(timezone.utc).isoformat()\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        topics, probs = topic_model.fit_transform(\n",
    "            documents=docs_for_vectorizer,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        fit_time = time.time() - t0\n",
    "\n",
    "        # Artefatos primários\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info.to_csv(tdir/\"topic_info_alt.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # Doc→tópico\n",
    "        if probs is not None and hasattr(probs, \"__array__\"):\n",
    "            rowmax = np.nanmax(probs, axis=1)\n",
    "            doc_prob = [float(x) if np.isfinite(x) else np.nan for x in rowmax]\n",
    "        else:\n",
    "            doc_prob = [np.nan] * len(topics)\n",
    "\n",
    "        doc_topics = pd.DataFrame({\n",
    "            \"DOC_ID\": df[\"DOC_ID\"].values,\n",
    "            \"topic\": topics,\n",
    "            \"prob\": doc_prob\n",
    "        })\n",
    "        doc_topics.to_csv(tdir/\"doc_topics_alt.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "        # Métricas (usar o analisador do vectorizer deste trial)\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        tokenized_docs_trial = [analyzer(doc) for doc in docs_for_vectorizer]\n",
    "\n",
    "        topic_words = extract_topic_words(topic_model, topk=10)\n",
    "        div = topic_diversity(topic_words, topk=10)\n",
    "        try:\n",
    "            c_npmi, c_uci = compute_coherences(topic_words, tokenized_docs_trial)\n",
    "        except Exception as ce:\n",
    "            c_npmi, c_uci = float(\"nan\"), float(\"nan\")\n",
    "            write_json(tdir/\"coherence_error_alt.json\", {\"error\": repr(ce)})\n",
    "\n",
    "        out_rate = outlier_rate(topics)\n",
    "\n",
    "        metrics = {\n",
    "            \"topic_diversity@10\": div,\n",
    "            \"c_npmi\": c_npmi,\n",
    "            \"c_uci\": c_uci,\n",
    "            \"outlier_rate\": out_rate,\n",
    "            \"n_topics_excl_-1\": int((topic_info[\"Topic\"] != -1).sum()),\n",
    "            \"fit_time_sec\": fit_time\n",
    "        }\n",
    "        write_json(tdir/\"metrics_alt.json\", metrics)\n",
    "\n",
    "        # Metadados\n",
    "        run_md = {\n",
    "            \"trial_id\": trial_id,\n",
    "            \"started_utc\": started,\n",
    "            \"finished_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"seed\": SEED,\n",
    "            \"versions\": versions,\n",
    "            \"paths\": {\n",
    "                \"input_csv\": str(IN_CSV.resolve()),\n",
    "                \"trial_dir\": str(tdir.resolve()),\n",
    "                \"run_dir\": str(RUN_DIR.resolve())\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"umap\": cfg[\"umap\"],\n",
    "                \"hdbscan\": cfg[\"hdb\"],\n",
    "                \"vectorizer\": {\n",
    "                    **{k: (list(v) if k==\"ngram_range\" else v) for k, v in cfg[\"vec\"].items()},\n",
    "                    \"_effective_min_df\": vec_kwargs[\"min_df\"],\n",
    "                    \"_effective_max_df\": vec_kwargs[\"max_df\"],\n",
    "                },\n",
    "                \"sbert_model\": sbert_name,\n",
    "            },\n",
    "            \"sizes\": {\n",
    "                \"n_docs\": len(df),\n",
    "                \"emb_dim\": int(embeddings.shape[1])\n",
    "            }\n",
    "        }\n",
    "        write_json(tdir/\"run_metadata_alt.json\", run_md)\n",
    "\n",
    "        trials_summary.append({\n",
    "            \"trial_id\": trial_id,\n",
    "            \"metrics\": metrics,\n",
    "            \"cfg\": cfg\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        err = {\"trial_id\": trial_id, \"error\": repr(e)}\n",
    "        errors_log.append(err)\n",
    "        write_json(tdir/\"error_alt.json\", err)\n",
    "        print(f\"[WARN] {trial_id} falhou: {e}\")\n",
    "\n",
    "# Salva também a lista completa de combinações usada neste RUN\n",
    "write_json(RUN_DIR/\"combos_used_alt.json\", {\"combos\": combos, \"N\": len(combos)})\n",
    "print(f\"RUN_ID: {RUN_ID} | trials OK: {len(trials_summary)} | trials com erro: {len(errors_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f87d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m rows = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m trials_summary:\n\u001b[32m      4\u001b[39m     r = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m      5\u001b[39m         trial_id=t[\u001b[33m\"\u001b[39m\u001b[33mtrial_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m         n_topics=t[\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mn_topics_excl_-1\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m         out_rate=t[\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33moutlier_rate\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m         c_npmi=t[\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mc_npmi\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      9\u001b[39m         c_uci=t[\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mc_uci\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         umap_n_neighbors=\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcfg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mumap\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_neighbors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m     11\u001b[39m         umap_n_components=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mumap\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mn_components\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     12\u001b[39m         umap_min_dist=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mumap\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmin_dist\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     13\u001b[39m         hdb_min_cluster_size=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mhdb\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmin_cluster_size\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     14\u001b[39m         hdb_min_samples=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mhdb\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmin_samples\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     15\u001b[39m         hdb_method=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mhdb\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcluster_selection_method\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     16\u001b[39m         vec_ngr=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mvec\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mngram_range\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     17\u001b[39m         vec_min_df=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mvec\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmin_df\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     18\u001b[39m         vec_max_df=t[\u001b[33m\"\u001b[39m\u001b[33mcfg\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mvec\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmax_df\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     19\u001b[39m     )\n\u001b[32m     20\u001b[39m     rows.append(r)\n\u001b[32m     21\u001b[39m df_sweep = pd.DataFrame(rows)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# depois do sweep:\n",
    "def _get(d: dict, path, default=np.nan):\n",
    "    \"\"\"Acesso seguro a d[k1][k2]...; retorna default se faltar algo ou se algum nível for None.\"\"\"\n",
    "    cur = d\n",
    "    for k in path:\n",
    "        if cur is None:\n",
    "            return default\n",
    "        if isinstance(cur, dict):\n",
    "            cur = cur.get(k, default)\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "rows = []\n",
    "for t in trials_summary:\n",
    "    cfg = t[\"cfg\"]\n",
    "    rows.append(dict(\n",
    "        trial_id=t[\"trial_id\"],\n",
    "        n_topics=t[\"metrics\"][\"n_topics_excl_-1\"],\n",
    "        out_rate=t[\"metrics\"][\"outlier_rate\"],\n",
    "        c_npmi=t[\"metrics\"][\"c_npmi\"],\n",
    "        c_uci=t[\"metrics\"][\"c_uci\"],\n",
    "\n",
    "        # UMAP pode ser None nos combos \"sem UMAP\"\n",
    "        umap_used = cfg[\"umap\"] is not None,\n",
    "        umap_n_neighbors = _get(cfg, [\"umap\", \"n_neighbors\"]),\n",
    "        umap_n_components = _get(cfg, [\"umap\", \"n_components\"]),\n",
    "        umap_min_dist = _get(cfg, [\"umap\", \"min_dist\"]),\n",
    "\n",
    "        # HDBSCAN\n",
    "        hdb_min_cluster_size = cfg[\"hdb\"][\"min_cluster_size\"],\n",
    "        hdb_min_samples = cfg[\"hdb\"][\"min_samples\"],\n",
    "        hdb_method = cfg[\"hdb\"][\"cluster_selection_method\"],\n",
    "\n",
    "        # Vectorizer (valores conforme definidos na grade original)\n",
    "        vec_ngr = cfg[\"vec\"][\"ngram_range\"],\n",
    "        vec_min_df = cfg[\"vec\"][\"min_df\"],\n",
    "        vec_max_df = cfg[\"vec\"][\"max_df\"],\n",
    "    ))\n",
    "\n",
    "df_sweep = pd.DataFrame(rows)\n",
    "\n",
    "# Salva um resumo desta execução\n",
    "df_sweep.to_csv(RUN_DIR/\"sweep_summary_alt.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Top em nº de tópicos\n",
    "display(df_sweep.sort_values(\"n_topics\", ascending=False).head(10))\n",
    "\n",
    "# Efeito de parâmetros-chave (inclui flag umap_used para comparar com/sem UMAP)\n",
    "display(\n",
    "    df_sweep\n",
    "      .groupby([\"umap_used\", \"hdb_method\",\"hdb_min_cluster_size\",\"hdb_min_samples\"])\n",
    "      .agg(n_topics_mean=(\"n_topics\",\"mean\"),\n",
    "           n_topics_max=(\"n_topics\",\"max\"),\n",
    "           trials=(\"trial_id\",\"count\"))\n",
    "      .sort_values([\"n_topics_max\",\"n_topics_mean\",\"trials\"], ascending=False)\n",
    "      .head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded36dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking e escolha da melhor configuração\n",
    "\n",
    "# Ranking composto (ordenação por: c_npmi desc, topic_diversity desc, outlier_rate asc + penalização por n_tópicos)\n",
    "if not trials_summary:\n",
    "    raise RuntimeError(\"Nenhum trial concluído. Verifique errors_log e configurações.\")\n",
    "\n",
    "df_rank = pd.DataFrame([{\n",
    "    \"trial_id\": t[\"trial_id\"],\n",
    "    \"c_npmi\": t[\"metrics\"][\"c_npmi\"],\n",
    "    \"topic_diversity\": t[\"metrics\"][\"topic_diversity@10\"],\n",
    "    \"outlier_rate\": t[\"metrics\"][\"outlier_rate\"],\n",
    "    \"n_topics\": t[\"metrics\"][\"n_topics_excl_-1\"]\n",
    "} for t in trials_summary])\n",
    "\n",
    "# Penalização por distância a uma faixa alvo (ajuste conforme o domínio)\n",
    "LOW, HIGH = 10, 40\n",
    "def penalty(n):\n",
    "    if pd.isna(n):\n",
    "        return 1.0\n",
    "    n = float(n)\n",
    "    if n < LOW:\n",
    "        return (LOW - n) / LOW\n",
    "    if n > HIGH:\n",
    "        return (n - HIGH) / HIGH\n",
    "    return 0.0\n",
    "\n",
    "df_rank[\"n_topics_penalty\"] = df_rank[\"n_topics\"].apply(penalty)\n",
    "df_rank[\"rank_sum\"] = df_rank[[\"r1\",\"r2\",\"r3\"]].sum(axis=1) + 5.0 * df_rank[\"n_topics_penalty\"]\n",
    "\n",
    "# Ranks principais\n",
    "df_rank[\"r1\"] = df_rank[\"c_npmi\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r2\"] = df_rank[\"topic_diversity\"].rank(ascending=False, method=\"min\")\n",
    "df_rank[\"r3\"] = df_rank[\"outlier_rate\"].rank(ascending=True, method=\"min\")\n",
    "\n",
    "# Combinação com penalização (peso 1.0 na penalização; ajuste se necessário)\n",
    "df_rank[\"rank_sum\"] = df_rank[[\"r1\",\"r2\",\"r3\"]].sum(axis=1) + df_rank[\"n_topics_penalty\"]\n",
    "\n",
    "df_rank = df_rank.sort_values([\"rank_sum\", \"r1\", \"r2\", \"r3\"]).reset_index(drop=True)\n",
    "best_trial_id = df_rank.iloc[0][\"trial_id\"]\n",
    "best_row = df_rank.iloc[0].to_dict()\n",
    "\n",
    "print(\"TOP-10 trials por critério composto:\")\n",
    "display(df_rank.head(10))\n",
    "print(\"Vencedora:\", best_trial_id)\n",
    "\n",
    "# Persistência no RUN_DIR\n",
    "df_rank.to_csv(RUN_DIR/\"ranking_alt.csv\", index=False, encoding=\"utf-8\")\n",
    "write_json(RUN_DIR/\"winner_alt.json\", {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"best_trial_id\": best_trial_id,\n",
    "    \"best_row\": best_row,\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic)",
   "language": "python",
   "name": "bertopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
