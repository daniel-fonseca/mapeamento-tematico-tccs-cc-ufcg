{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5522c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (423, 7) | colunas (10 primeiras): ['DOC_ID', 'ano', 'titulo', 'autor', 'orientador', 'resumo', 'url'] ...\n"
     ]
    }
   ],
   "source": [
    "# Carrega todas as colunas e garante uma chave estável\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/raw/tccs.csv\")\n",
    "\n",
    "assert \"resumo\" in df.columns, \"A coluna 'resumo' não existe no CSV.\"\n",
    "\n",
    "# DOC_ID estável para cruzamentos\n",
    "if \"DOC_ID\" not in df.columns:\n",
    "    df.insert(0, \"DOC_ID\", range(len(df)))\n",
    "\n",
    "print(\"Shape:\", df.shape, \"| colunas (10 primeiras):\", list(df.columns)[:10], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551e6058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords (spaCy) normalizadas: 403\n"
     ]
    }
   ],
   "source": [
    "# Configuração spaCy + stopwords (apenas nativas do spaCy)\n",
    "\n",
    "import spacy, unidecode\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "\n",
    "# Modelo PT do spaCy\n",
    "nlp = spacy.load(\"pt_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# Normaliza as stopwords para o mesmo espaço dos lemas\n",
    "stopwords_norm = {unidecode.unidecode(w.lower()) for w in STOP_WORDS}\n",
    "print(\"Stopwords (spaCy) normalizadas:\", len(stopwords_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89842007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de normalização e lematização (preserva acentos até lematizar)\n",
    "\n",
    "def normalize_text_lower(s: str) -> str:\n",
    "    return s.lower()\n",
    "\n",
    "def tokenize_lemmatize(text: str):\n",
    "    doc = nlp(text)\n",
    "    toks = []\n",
    "    for t in doc:\n",
    "        if not t.is_alpha:\n",
    "            continue\n",
    "        lemma_str = unidecode.unidecode(t.lemma_.lower()).strip()\n",
    "        for sub in lemma_str.split():\n",
    "            if len(sub) <= 2 or sub in stopwords_norm:\n",
    "                continue\n",
    "            toks.append(sub)\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9445ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplo original:\n",
      " Gerenciar a agenda e fornecer informações ao cliente são tarefas que oneram muito o tempo do pequeno empreendedor, que, em muitos casos, gerencia e opera um negócio sozinho, precisando maximizar seu tempo produzindo para obter melhores resultados e manter o seu empreendimento em atividade. Este trabalho tem como objetivo propor o desenvolvimento de um sistema web que oferece ao empreendedor um meio de fornecer as principais informações sobre seu negócio e visualizar os serviços agendados. O sistema provê ao consumidor dos serviços prestados uma forma de visualizar as informações sobre um determinado serviço e efetuar o agendamento em horário disponível e conveniente para si, dispensando a necessidade de qualquer tipo de contato prévio com o prestador do serviço. A solução foi avaliada por 23 usuários, obtendo um índice de satisfação médio de 4.7 entre os dois grupos de usuários (prestadores de serviços e consumidores), constatando ser uma proposta eficaz na solução do problema.\n",
      "\n",
      "Após processamento (30 primeiros tokens):\n",
      " ['gerenciar', 'agenda', 'fornecer', 'informacao', 'cliente', 'tarefa', 'oner', 'pequeno', 'empreendedor', 'caso', 'gerencia', 'operar', 'negocio', 'sozinho', 'precisar', 'maximizar', 'produzir', 'obter', 'resultado', 'manter', 'empreendimento', 'atividade', 'trabalho', 'objetivo', 'propor', 'desenvolvimento', 'web', 'oferecer', 'empreendedor', 'fornecer']\n"
     ]
    }
   ],
   "source": [
    "# Aplicação nos resumos\n",
    "\n",
    "resumos_lower = df[\"resumo\"].fillna(\"\").astype(str).map(normalize_text_lower)\n",
    "docs_tokens = [tokenize_lemmatize(t) for t in resumos_lower]\n",
    "\n",
    "print(\"Exemplo original:\\n\", df[\"resumo\"].iloc[0])\n",
    "print(\"\\nApós processamento (30 primeiros tokens):\\n\", docs_tokens[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59bd312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas aplicados.\n"
     ]
    }
   ],
   "source": [
    "# Bigramas com Gensim Phrases\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "MIN_COUNT = 5\n",
    "THRESHOLD = 10.0\n",
    "\n",
    "phrases = Phrases(docs_tokens, min_count=MIN_COUNT, threshold=THRESHOLD, delimiter=\"_\")\n",
    "bigram_phraser = Phraser(phrases)\n",
    "\n",
    "# Aplica o detector de bigramas\n",
    "docs_tokens = [bigram_phraser[doc] for doc in docs_tokens]\n",
    "print(\"Bigramas aplicados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1565261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tokens: 32892\n",
      "Total de n-gramas (com '_'): 1334\n",
      "Exemplos: ['fornecer_informacao', 'trabalho_objetivo', 'desafio_significativo', 'aprendizado_maquina', 'contribuir_avanco', 'rede_neural', 'resultado_indicar', 'presente_trabalho', 'tomar_decisao', 'estudo_investigar', 'modelo_linguagem', 'linguagem_programacao', 'desenvolvimento_software', 'desenvolvimento_software', 'processo_desenvolvimento', 'fornecer_informacao', 'processo_desenvolvimento', 'desafio_significativo', 'isolamento_social', 'apresentar_solucao']\n"
     ]
    }
   ],
   "source": [
    "# Checagem dos n-gramas aprendidos\n",
    "\n",
    "# Inspeciona quantos tokens com \"_\" surgiram e mostra exemplos\n",
    "tokens_flat = [tok for doc in docs_tokens for tok in doc]\n",
    "ngrams = [t for t in tokens_flat if \"_\" in t]\n",
    "print(\"Total de tokens:\", len(tokens_flat))\n",
    "print(\"Total de n-gramas (com '_'):\", len(ngrams))\n",
    "print(\"Exemplos:\", ngrams[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c5f0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram phraser salvo em: ../../data/interim/lda/bigram_phraser.pkl\n"
     ]
    }
   ],
   "source": [
    "# Salva o modelo de bigramas para reuso\n",
    "\n",
    "import os\n",
    "BIGRAM_PATH = \"../../data/interim/lda/bigram_phraser.pkl\"\n",
    "os.makedirs(os.path.dirname(BIGRAM_PATH), exist_ok=True)\n",
    "bigram_phraser.save(BIGRAM_PATH)\n",
    "print(\"Bigram phraser salvo em:\", BIGRAM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dc19da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> ../../data/interim/lda/prep.csv | docs: 423 | colunas: 8\n",
      "Tokens salvos em: ../../data/interim/lda/tokens.pkl | docs_tokens: 423\n"
     ]
    }
   ],
   "source": [
    "# Salvando resultado (mantém todas as colunas + RESUMO_PREP)\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "df[\"RESUMO_PREP\"] = [\" \".join(t) for t in docs_tokens]\n",
    "\n",
    "OUT_CSV = \"../../data/interim/lda/prep.csv\"\n",
    "OUT_TOKENS = \"../../data/interim/lda/tokens.pkl\"\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
    "\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "with open(OUT_TOKENS, \"wb\") as f:\n",
    "    pickle.dump(docs_tokens, f)\n",
    "\n",
    "print(\"OK ->\", OUT_CSV, \"| docs:\", len(df), \"| colunas:\", len(df.columns))\n",
    "print(\"Tokens salvos em:\", OUT_TOKENS, \"| docs_tokens:\", len(docs_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "755a7529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 423 | tokens/doc (min/med/ max): 0/79/168\n",
      "Top 15 tokens: ['dado', 'utilizar', 'trabalho', 'usuario', 'ferramenta', 'modelo', 'uso', 'aplicacao', 'estudo', 'projeto', 'objetivo', 'resultado', 'realizar', 'desenvolvimento', 'informacao']\n"
     ]
    }
   ],
   "source": [
    "# Checagem de sanidade\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "lens = [len(d) for d in docs_tokens]\n",
    "print(f\"Docs: {len(docs_tokens)} | tokens/doc (min/med/ max): {min(lens)}/{sorted(lens)[len(lens)//2]}/{max(lens)}\")\n",
    "vocab = Counter(tok for d in docs_tokens for tok in d)\n",
    "print(\"Top 15 tokens:\", [w for w,_ in vocab.most_common(15)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lda 3.12)",
   "language": "python",
   "name": "lda-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
