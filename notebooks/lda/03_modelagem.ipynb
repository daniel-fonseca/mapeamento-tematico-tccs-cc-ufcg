{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41d7a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Scripts\\python.exe\n",
      "3.12.2\n"
     ]
    }
   ],
   "source": [
    "import sys, platform\n",
    "print(sys.executable)\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8003818e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of c:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\sklearn\\__check_build:\n_check_build.cp311-win_amd64.pyd__init__.py               __pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\sklearn\\__check_build\\__init__.py:45\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_check_build\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_build  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn.__check_build._check_build'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LdaModel, CoherenceModel\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgensim_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensimvis\u001b[39;00m\n\u001b[32m     18\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\pyLDAvis\\__init__.py:44\u001b[39m\n\u001b[32m     36\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33m__version__\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mprepare\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjs_PCoA\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mPreparedData\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprepared_data_to_html\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mdisplay\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mshow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msave_html\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msave_json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33menable_notebook\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdisable_notebook\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     42\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m3.4.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_display\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prepare\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prepare, js_PCoA, PreparedData\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\pyLDAvis\\_display.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_server\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serve\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_id, write_ipynb_local_js, NumPyEncoder\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prepare\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreparedData\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01murls\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murls\u001b[39;00m\n\u001b[32m     14\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mprepared_data_to_html\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdisplay\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33mshow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msave_html\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msave_json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m            \u001b[33m\"\u001b[39m\u001b[33menable_notebook\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdisable_notebook\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\pyLDAvis\\_prepare.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pdist, squareform\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MDS, TSNE\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyLDAvis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumPyEncoder\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__num_dist_rows__\u001b[39m(array, ndigits=\u001b[32m2\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\sklearn\\__init__.py:83\u001b[39m\n\u001b[32m     72\u001b[39m     sys.stderr.write(\u001b[33m\"\u001b[39m\u001b[33mPartial import of sklearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     84\u001b[39m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     85\u001b[39m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     86\u001b[39m     )\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\sklearn\\__check_build\\__init__.py:47\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_check_build\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_build  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mraise_build_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\sklearn\\__check_build\\__init__.py:31\u001b[39m, in \u001b[36mraise_build_error\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m             dir_content.append(filename + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33m___________________________________________________________________________\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[33mContents of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;132;01m%s\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33m___________________________________________________________________________\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33mIt seems that scikit-learn has not been built correctly.\u001b[39m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[33mIf you have installed scikit-learn from source, please do not forget\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33mto build the package before using it: run `python setup.py install` or\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[33m`make` in the source directory.\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\"\"\u001b[39m % (e, local_dir, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(dir_content).strip(), msg))\n",
      "\u001b[31mImportError\u001b[39m: No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of c:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\envs\\lda\\Lib\\site-packages\\sklearn\\__check_build:\n_check_build.cp311-win_amd64.pyd__init__.py               __pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "# Checagem de ambiente\n",
    "\n",
    "import sys, platform, warnings, os, json, random, math\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"gensim:\", gensim.__version__)\n",
    "print(\"pyLDAvis:\", pyLDAvis.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de paths\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path = Path.cwd()) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        target = p / \"data\" / \"interim\" / \"lda\" / \"vocab_bow.dict\"\n",
    "        if target.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        \"Raiz do projeto não encontrada. \"\n",
    "        \"Verifique se existe 'data/interim/lda/vocab_bow.dict' a partir da raiz.\"\n",
    "    )\n",
    "\n",
    "REPO = find_repo_root()\n",
    "print(\"REPO =\", REPO)\n",
    "\n",
    "# Entradas\n",
    "PATH_BOW_MM         = REPO / \"data\" / \"interim\" / \"lda\" / \"bow.mm\"\n",
    "PATH_VOCAB_DICT     = REPO / \"data\" / \"interim\" / \"lda\" / \"vocab_bow.dict\"\n",
    "PATH_BOW_INDEX_CSV  = REPO / \"data\" / \"interim\" / \"lda\" / \"bow_index.csv\"\n",
    "PATH_VOCAB_TERMS    = REPO / \"data\" / \"interim\" / \"lda\" / \"vocab_terms.csv\"\n",
    "\n",
    "# Saídas\n",
    "DIR_PROCESSED_LDA   = REPO / \"data\" / \"processed\" / \"lda\"\n",
    "DIR_REPORTS_FIGS    = REPO / \"reports\" / \"figs\"\n",
    "DIR_PROCESSED_LDA.mkdir(parents=True, exist_ok=True)\n",
    "DIR_REPORTS_FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for p in [PATH_VOCAB_DICT, PATH_BOW_MM, PATH_BOW_INDEX_CSV, PATH_VOCAB_TERMS]:\n",
    "    print(p, \"EXISTS?\", p.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f5b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar corpus e dicionário\n",
    "\n",
    "dictionary = corpora.Dictionary.load(str(PATH_VOCAB_DICT))\n",
    "corpus = corpora.MmCorpus(str(PATH_BOW_MM))\n",
    "\n",
    "bow_index = pd.read_csv(PATH_BOW_INDEX_CSV)\n",
    "vocab_df  = pd.read_csv(PATH_VOCAB_TERMS)\n",
    "\n",
    "print(dictionary)\n",
    "print(f\"n_docs (corpus): {len(corpus)}\")\n",
    "bow_index.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a2d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitários (treino, coerências, estabilidade)\n",
    "\n",
    "from typing import Dict, List\n",
    "from itertools import combinations\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def train_lda(k:int,\n",
    "              corpus,\n",
    "              dictionary,\n",
    "              passes:int=10,\n",
    "              iterations:int=400,\n",
    "              chunksize:int=2000,\n",
    "              alpha='auto',\n",
    "              eta='auto',\n",
    "              random_state:int=RANDOM_STATE) -> LdaModel:\n",
    "    return LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=k,\n",
    "        random_state=random_state,\n",
    "        chunksize=chunksize,\n",
    "        passes=passes,\n",
    "        iterations=iterations,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        eval_every=None\n",
    "    )\n",
    "\n",
    "def compute_coherences(model:LdaModel,\n",
    "                       texts_like_tokens:List[List[str]]|None,\n",
    "                       corpus,\n",
    "                       dictionary) -> Dict[str, float]:\n",
    "    coherences = {}\n",
    "\n",
    "    # UMass\n",
    "    cm_umass = CoherenceModel(model=model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "    coherences['u_mass'] = float(cm_umass.get_coherence())\n",
    "\n",
    "    # C_v\n",
    "    if texts_like_tokens is not None:\n",
    "        cm_cv = CoherenceModel(model=model, texts=texts_like_tokens, dictionary=dictionary, coherence='c_v')\n",
    "    else:\n",
    "        cm_cv = CoherenceModel(model=model, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "    coherences['c_v'] = float(cm_cv.get_coherence())\n",
    "\n",
    "    # C_npmi\n",
    "    if texts_like_tokens is not None:\n",
    "        cm_npmi = CoherenceModel(model=model, texts=texts_like_tokens, dictionary=dictionary, coherence='c_npmi')\n",
    "        coherences['c_npmi'] = float(cm_npmi.get_coherence())\n",
    "    else:\n",
    "        coherences['c_npmi'] = float('nan')\n",
    "\n",
    "    return coherences\n",
    "\n",
    "def topn_terms_per_topic(model:LdaModel, topn:int=10) -> Dict[int, List[str]]:\n",
    "    return {t: [w for (w, _) in model.show_topic(t, topn=topn)] for t in range(model.num_topics)}\n",
    "\n",
    "def jaccard(a:set, b:set) -> float:\n",
    "    u = len(a | b)\n",
    "    return len(a & b)/u if u else 0.0\n",
    "\n",
    "def stability_score(models:List[LdaModel], topn:int=10) -> float:\n",
    "    if len(models) < 2:\n",
    "        return float('nan')\n",
    "\n",
    "    def pairwise_best_match(m1, m2):\n",
    "        T1 = topn_terms_per_topic(m1, topn=topn)\n",
    "        T2 = topn_terms_per_topic(m2, topn=topn)\n",
    "        used = set()\n",
    "        scores = []\n",
    "        for t1, terms1 in T1.items():\n",
    "            s1 = set(terms1)\n",
    "            best, best_t = 0.0, None\n",
    "            for t2, terms2 in T2.items():\n",
    "                if t2 in used: \n",
    "                    continue\n",
    "                sc = jaccard(s1, set(terms2))\n",
    "                if sc > best:\n",
    "                    best, best_t = sc, t2\n",
    "            if best_t is not None:\n",
    "                used.add(best_t)\n",
    "                scores.append(best)\n",
    "        return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "    vals = []\n",
    "    for i, j in combinations(range(len(models)), 2):\n",
    "        vals.append(pairwise_best_match(models[i], models[j]))\n",
    "    return float(np.mean(vals)) if vals else float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar tokens para coerência\n",
    "\n",
    "import pickle\n",
    "\n",
    "TOKENS_PATH = REPO / \"data\" / \"interim\" / \"lda\" / \"tokens.pkl\"\n",
    "\n",
    "if TOKENS_PATH.exists():\n",
    "    with open(TOKENS_PATH, \"rb\") as f:\n",
    "        texts_like_tokens = pickle.load(f)\n",
    "    print(\"Tokens carregados:\", len(texts_like_tokens))\n",
    "else:\n",
    "    texts_like_tokens = None\n",
    "    print(\"Aviso: tokens.pkl não encontrado, coerência C_v/C_npmi pode ficar limitada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varredura de k\n",
    "\n",
    "K_VALUES = list(range(5, 35, 5))\n",
    "PASSES = 10\n",
    "ITERATIONS = 400\n",
    "CHUNKSIZE = 2000\n",
    "N_RESTARTS = 3\n",
    "random_seeds = [RANDOM_STATE + i for i in range(N_RESTARTS)]\n",
    "\n",
    "results = []\n",
    "best_models_per_k = {}\n",
    "\n",
    "for k in K_VALUES:\n",
    "    models_k, metrics_k = [], []\n",
    "    for seed in random_seeds:\n",
    "        model = train_lda(k, corpus, dictionary,\n",
    "                          passes=PASSES, iterations=ITERATIONS,\n",
    "                          chunksize=CHUNKSIZE, random_state=seed)\n",
    "        coh = compute_coherences(model, texts_like_tokens, corpus, dictionary)\n",
    "        models_k.append(model)\n",
    "        metrics_k.append(coh)\n",
    "    stab = stability_score(models_k, topn=10)\n",
    "    dfm = pd.DataFrame(metrics_k)\n",
    "    idx_best = dfm[\"c_v\"].idxmax() if dfm[\"c_v\"].notna().any() else 0\n",
    "    best_models_per_k[k] = models_k[int(idx_best)]\n",
    "    mean_metrics = dfm.mean(numeric_only=True).to_dict()\n",
    "    mean_metrics[\"stability_jaccard_top10\"] = stab\n",
    "    mean_metrics[\"k\"] = k\n",
    "    results.append(mean_metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(results).sort_values(\"k\").reset_index(drop=True)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b756b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de métricas de coerência e estabilidade em função de k\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_df[\"k\"], metrics_df[\"c_v\"], marker=\"o\", label=\"C_v\")\n",
    "plt.plot(metrics_df[\"k\"], metrics_df[\"c_npmi\"], marker=\"o\", label=\"C_npmi\")\n",
    "plt.plot(metrics_df[\"k\"], metrics_df[\"u_mass\"], marker=\"o\", label=\"U_Mass\")\n",
    "plt.plot(metrics_df[\"k\"], metrics_df[\"stability_jaccard_top10\"], marker=\"o\", label=\"Estabilidade (Jaccard)\")\n",
    "\n",
    "plt.xlabel(\"Número de tópicos (k)\")\n",
    "plt.ylabel(\"Valor da métrica\")\n",
    "plt.title(\"Métricas de coerência e estabilidade vs k\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "DIAG_PNG = DIR_REPORTS_FIGS / \"lda_k_diagnostics.png\"\n",
    "plt.savefig(DIAG_PNG, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Figura de diagnóstico salva em:\", DIAG_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21daadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleção do melhor k e salvamento do modelo\n",
    "\n",
    "def pick_k(df:pd.DataFrame) -> int:\n",
    "    if df[\"c_v\"].notna().any():\n",
    "        top_cv = df[\"c_v\"].max()\n",
    "        cands = df[df[\"c_v\"] == top_cv]\n",
    "        if \"c_npmi\" in df and cands[\"c_npmi\"].notna().any():\n",
    "            return int(cands.loc[cands[\"c_npmi\"].idxmax(), \"k\"])\n",
    "        return int(cands.iloc[0][\"k\"])\n",
    "    elif \"c_npmi\" in df and df[\"c_npmi\"].notna().any():\n",
    "        return int(df.loc[df[\"c_npmi\"].idxmax(), \"k\"])\n",
    "    return int(df[\"k\"].max())\n",
    "\n",
    "k_star = pick_k(metrics_df)\n",
    "best_model = best_models_per_k[k_star]\n",
    "print(f\"Melhor k selecionado: {k_star}\")\n",
    "\n",
    "MODEL_PATH = DIR_PROCESSED_LDA / \"model.lda\"\n",
    "best_model.save(str(MODEL_PATH))\n",
    "\n",
    "METRICS_CSV = DIR_PROCESSED_LDA / \"coherences.csv\"\n",
    "metrics_df.to_csv(METRICS_CSV, index=False)\n",
    "\n",
    "print(\"Modelo salvo em:\", MODEL_PATH)\n",
    "print(\"Métricas salvas em:\", METRICS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de robustez de k: inspeção da estabilidade relativa\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(metrics_df[\"k\"], metrics_df[\"stability_jaccard_top10\"], marker=\"s\", color=\"darkred\")\n",
    "plt.xlabel(\"Número de tópicos (k)\")\n",
    "plt.ylabel(\"Estabilidade média (Jaccard top-10 termos)\")\n",
    "plt.title(\"Robustez da escolha de k: estabilidade entre reinicializações\")\n",
    "plt.grid(True)\n",
    "\n",
    "ROBUST_PNG = DIR_REPORTS_FIGS / \"lda_k_robustness.png\"\n",
    "plt.savefig(ROBUST_PNG, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Figura de robustez salva em:\", ROBUST_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b07a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações estáticas (tópicos e distribuição doc-tópico)\n",
    "\n",
    "# Top termos por tópico\n",
    "def plot_top_terms(model:LdaModel, topn:int=10, maxcols:int=3, figsize=(14, 10), savepath=None):\n",
    "    topics = [model.show_topic(t, topn=topn) for t in range(model.num_topics)]\n",
    "    n = len(topics); ncols = min(maxcols, n); nrows = math.ceil(n / ncols)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i, terms in enumerate(topics, start=1):\n",
    "        ax = plt.subplot(nrows, ncols, i)\n",
    "        labels  = [w for w,_ in terms][::-1]\n",
    "        weights = [v for _,v in terms][::-1]\n",
    "        ax.barh(range(len(labels)), weights)\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.set_title(f\"Tópico {i-1}\")\n",
    "        ax.set_xlabel(\"Peso\")\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "TOPICS_PNG = DIR_REPORTS_FIGS / \"lda_topics.png\"\n",
    "plot_top_terms(best_model, topn=10, maxcols=3, figsize=(14, 10), savepath=TOPICS_PNG)\n",
    "print(\"Figura salva em:\", TOPICS_PNG)\n",
    "\n",
    "# Distribuição de tópicos por documento (heatmap amostra)\n",
    "def doc_topic_matrix(model:LdaModel, corpus):\n",
    "    n_topics = model.num_topics\n",
    "    rows = []\n",
    "    for bow in corpus:\n",
    "        dist = [0.0]*n_topics\n",
    "        for t, p in model.get_document_topics(bow, minimum_probability=0.0):\n",
    "            dist[t] = p\n",
    "        rows.append(dist)\n",
    "    return np.array(rows)\n",
    "\n",
    "M = doc_topic_matrix(best_model, corpus)\n",
    "print(\"Matriz doc x tópicos:\", M.shape)\n",
    "\n",
    "DOC_DIST_PNG = DIR_REPORTS_FIGS / \"lda_doc_topic_dist.png\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(M[:min(200, M.shape[0]), :], aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Distribuição de tópicos por documento (amostra)\")\n",
    "plt.xlabel(\"Tópicos\"); plt.ylabel(\"Documentos\")\n",
    "plt.savefig(DOC_DIST_PNG, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Figura salva em:\", DOC_DIST_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAvis interativo\n",
    "\n",
    "vis_data = gensimvis.prepare(best_model, corpus, dictionary)\n",
    "LDAVIS_HTML = DIR_REPORTS_FIGS / \"lda_vis.html\"\n",
    "pyLDAvis.save_html(vis_data, str(LDAVIS_HTML))\n",
    "print(\"LDAvis salvo em:\", LDAVIS_HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifesto de auditoria\n",
    "\n",
    "audit = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"random_state_base\": RANDOM_STATE,\n",
    "    \"k_grid\": list(K_VALUES),\n",
    "    \"k_selected\": int(k_star),\n",
    "    \"training_params\": {\n",
    "        \"passes\": PASSES, \"iterations\": ITERATIONS, \"chunksize\": CHUNKSIZE,\n",
    "        \"alpha\": \"auto\", \"eta\": \"auto\", \"n_restarts\": N_RESTARTS\n",
    "    },\n",
    "    \"coherence_metrics\": {\n",
    "        \"table_csv\": str(METRICS_CSV.relative_to(REPO)),\n",
    "        \"primary_metric\": \"c_v\", \"tie_breaker\": \"c_npmi\", \"umass_included\": True\n",
    "    },\n",
    "    \"stability\": {\"method\": \"mean_jaccard_top10_best_matching\"},\n",
    "    \"inputs\": {\n",
    "        \"corpus_mm\": str(PATH_BOW_MM.relative_to(REPO)),\n",
    "        \"vocab_dict\": str(PATH_VOCAB_DICT.relative_to(REPO)),\n",
    "        \"bow_index_csv\": str(PATH_BOW_INDEX_CSV.relative_to(REPO)),\n",
    "        \"vocab_terms_csv\": str(PATH_VOCAB_TERMS.relative_to(REPO)),\n",
    "        \"tokens_used_for_cv_npmi\": bool(texts_like_tokens is not None)\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"model_lda\": str(MODEL_PATH.relative_to(REPO)),\n",
    "        \"topics_png\": str(TOPICS_PNG.relative_to(REPO)),\n",
    "        \"doc_dist_png\": str(DOC_DIST_PNG.relative_to(REPO)),\n",
    "        \"ldavis_html\": str(LDAVIS_HTML.relative_to(REPO))\n",
    "    },\n",
    "    \"env\": {\n",
    "        \"python\": sys.version,\n",
    "        \"gensim\": gensim.__version__,\n",
    "        \"pyLDAvis\": pyLDAvis.__version__,\n",
    "        \"numpy\": np.__version__\n",
    "    }\n",
    "}\n",
    "AUDIT_JSON = DIR_PROCESSED_LDA / \"audit_modelagem.json\"\n",
    "with open(AUDIT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(audit, f, ensure_ascii=False, indent=2)\n",
    "print(\"Manifesto de auditoria salvo em:\", AUDIT_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a5e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportação de tabela de tópicos\n",
    "\n",
    "def topics_table(model:LdaModel, topn:int=15) -> pd.DataFrame:\n",
    "    return pd.DataFrame([\n",
    "        {\"topic_id\": t, \"top_terms\": \", \".join([w for w,_ in model.show_topic(t, topn=topn)])}\n",
    "        for t in range(model.num_topics)\n",
    "    ])\n",
    "\n",
    "TOPICS_CSV = DIR_PROCESSED_LDA / \"topics_top_terms.csv\"\n",
    "topics_df = topics_table(best_model, topn=15)\n",
    "topics_df.to_csv(TOPICS_CSV, index=False)\n",
    "print(\"CSV de tópicos salvo em:\", TOPICS_CSV)\n",
    "topics_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lda 3.12)",
   "language": "python",
   "name": "lda-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
