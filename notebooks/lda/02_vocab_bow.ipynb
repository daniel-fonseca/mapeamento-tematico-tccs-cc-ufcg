{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2163beaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando base: C:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\data\n",
      "Arquivos de saída serão gravados em: C:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\data\\interim\\lda\n"
     ]
    }
   ],
   "source": [
    "# Imports e paths\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "BASE = Path(\"../../data\")\n",
    "RAW = BASE / \"raw\"\n",
    "INTERIM_LDA = BASE / \"interim\" / \"lda\"\n",
    "PROCESSED_LDA = BASE / \"processed\" / \"lda\"\n",
    "\n",
    "PREP_CSV = INTERIM_LDA / \"prep.csv\"\n",
    "\n",
    "DICT_PATH = INTERIM_LDA / \"vocab_bow.dict\"\n",
    "BOW_MM_PATH = INTERIM_LDA / \"bow.mm\"\n",
    "VOCAB_TERMS_CSV = INTERIM_LDA / \"vocab_terms.csv\"\n",
    "BOW_INDEX_CSV = INTERIM_LDA / \"bow_index.csv\"\n",
    "\n",
    "for p in [INTERIM_LDA]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Usando base:\", BASE.resolve())\n",
    "print(\"Arquivos de saída serão gravados em:\", INTERIM_LDA.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce9fe8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep.csv carregado. Linhas totais = 423\n",
      "Colunas: ['DOC_ID', 'ano', 'titulo', 'autor', 'orientador', 'resumo', 'url', 'RESUMO_PREP']\n"
     ]
    }
   ],
   "source": [
    "# Leitura (prep.csv) e sanity checks\n",
    "\n",
    "assert PREP_CSV.exists(), f\"Arquivo não encontrado: {PREP_CSV}\"\n",
    "\n",
    "df = pd.read_csv(PREP_CSV)\n",
    "cols_ok = {\"DOC_ID\", \"RESUMO_PREP\"}\n",
    "missing = cols_ok - set(df.columns)\n",
    "assert not missing, f\"Colunas ausentes no prep.csv: {missing}\"\n",
    "\n",
    "total_docs = len(df)\n",
    "print(f\"prep.csv carregado. Linhas totais = {total_docs}\")\n",
    "print(\"Colunas:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1b9782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critério de seleção: RESUMO_PREP != \"\" (strip)\n",
      "Docs usados (após seleção) = 423 / 423 (totais)\n",
      "Primeiros 3 DOC_ID após ordenação: [0, 1, 2]\n",
      "Exemplo de tokens: ['gerenciar', 'agenda', 'fornecer_informacao', 'cliente', 'tarefa', 'oner', 'pequeno', 'empreendedor', 'caso', 'gerencia', 'operar', 'negocio', 'sozinho', 'precisar', 'maximizar']\n"
     ]
    }
   ],
   "source": [
    "# Seleção do subconjunto válido de trabalho e tokenização\n",
    "\n",
    "if \"TOKENS_LEN\" in df.columns:\n",
    "    df_work = df[df[\"TOKENS_LEN\"] > 0].copy()\n",
    "    criterio = 'TOKENS_LEN > 0'\n",
    "else:\n",
    "    df_work = df[df[\"RESUMO_PREP\"].astype(str).str.strip().ne(\"\")].copy()\n",
    "    criterio = 'RESUMO_PREP != \"\" (strip)'\n",
    "\n",
    "assert \"DOC_ID\" in df_work.columns, \"DOC_ID ausente no prep.csv\"\n",
    "df_work = df_work.sort_values(\"DOC_ID\").reset_index(drop=True)\n",
    "\n",
    "docs_total = len(df_work)\n",
    "print(f\"Critério de seleção: {criterio}\")\n",
    "print(f\"Docs usados (após seleção) = {docs_total} / {total_docs} (totais)\")\n",
    "\n",
    "df_work[\"__TOKENS__\"] = df_work[\"RESUMO_PREP\"].astype(str).str.split()\n",
    "\n",
    "print(\"Primeiros 3 DOC_ID após ordenação:\", df_work[\"DOC_ID\"].head(3).tolist())\n",
    "print(\"Exemplo de tokens:\", df_work[\"__TOKENS__\"].iloc[0][:15] if docs_total > 0 else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65fe740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|V| inicial (antes do filtro): 4968\n",
      "|V| após filter_extremes: 1218\n",
      "DF pós-filtro: min=5 | max=186 | limite teórico max ≤ 211\n"
     ]
    }
   ],
   "source": [
    "# Vocabulário e filtragem por frequência\n",
    "\n",
    "docs_list = df_work[\"__TOKENS__\"].tolist()\n",
    "doc_ids = df_work[\"DOC_ID\"].tolist()\n",
    "n_docs_for_df = len(docs_list)\n",
    "\n",
    "dictionary = Dictionary(docs_list)\n",
    "vocab_init = len(dictionary)\n",
    "print(f\"|V| inicial (antes do filtro): {vocab_init}\")\n",
    "\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=None)\n",
    "dictionary.compactify()\n",
    "vocab_final = len(dictionary)\n",
    "print(f\"|V| após filter_extremes: {vocab_final}\")\n",
    "\n",
    "if vocab_final == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Após o filtro de frequência, o vocabulário ficou vazio. \"\n",
    "        \"Revise o 01_prep ou os limiares no_below/no_above para este corpus.\"\n",
    "    )\n",
    "\n",
    "dfs = dictionary.dfs\n",
    "if len(dfs) == 0:\n",
    "    raise RuntimeError(\"dictionary.dfs vazio após filtro — algo inconsistente com o vocabulário.\")\n",
    "\n",
    "min_df = min(dfs.values())\n",
    "max_df = max(dfs.values())\n",
    "max_df_expected = int(n_docs_for_df * 0.5)\n",
    "\n",
    "print(f\"DF pós-filtro: min={min_df} | max={max_df} | limite teórico max ≤ {max_df_expected}\")\n",
    "\n",
    "assert min_df >= 5, f\"min_df={min_df} < 5 — inesperado após no_below=5\"\n",
    "assert max_df <= max_df_expected, (\n",
    "    f\"max_df={max_df} > {max_df_expected} — inesperado após no_above=0.5 \"\n",
    "    f\"(n_docs_considerados={n_docs_for_df})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "268fad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs com BoW != vazio: 423 (removidos por ficarem vazios: 0)\n",
      "nnz = 20066 | sparsidade = 0.038947\n",
      "Arquivos gravados:\n",
      " - vocab_bow.dict\n",
      " - bow.mm\n",
      " - vocab_terms.csv\n",
      " - bow_index.csv\n",
      "bow_index: checagens OK (monotonicidade estrita e abrangência confirmadas).\n",
      "CSR salvo em: bow_csr.npz (shape=(423, 1218), nnz=20066, densidade=0.038947)\n"
     ]
    }
   ],
   "source": [
    "# Construção do corpus BoW e remoção de docs vazios\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Construção de BoW para todos e filtrar vazios após filtro de vocabulário\n",
    "corpus_bow_all = [dictionary.doc2bow(doc) for doc in docs_list]\n",
    "non_empty_mask = [len(b) > 0 for b in corpus_bow_all]\n",
    "\n",
    "n_nonempty = sum(non_empty_mask)\n",
    "n_empty = len(non_empty_mask) - n_nonempty\n",
    "if n_nonempty == 0:\n",
    "    raise RuntimeError(\"Todos os documentos ficaram vazios após a filtragem do vocabulário.\")\n",
    "\n",
    "corpus_bow = [b for b, keep in zip(corpus_bow_all, non_empty_mask) if keep]\n",
    "doc_ids_used = [d for d, keep in zip(doc_ids, non_empty_mask) if keep]\n",
    "\n",
    "# Métricas de sparsidade\n",
    "nnz = sum(len(b) for b in corpus_bow)\n",
    "n_docs_used = len(corpus_bow)\n",
    "vocab_final = len(dictionary)  # reuso\n",
    "sparsity = nnz / (n_docs_used * vocab_final) if vocab_final > 0 else float(\"nan\")\n",
    "\n",
    "print(f\"Docs com BoW != vazio: {n_docs_used} (removidos por ficarem vazios: {n_empty})\")\n",
    "print(f\"nnz = {nnz} | sparsidade = {sparsity:.6f}\")\n",
    "\n",
    "# Salvamentos básicos: dicionário, Matrix Market, vocab_terms.csv, bow_index.csv\n",
    "# Dicionário\n",
    "dictionary.save(str(DICT_PATH))\n",
    "\n",
    "# MmCorpus\n",
    "MmCorpus.serialize(str(BOW_MM_PATH), corpus_bow)\n",
    "\n",
    "# vocab_terms.csv\n",
    "dfs = dictionary.dfs\n",
    "records = [(token_id, dictionary[token_id], dfreq) for token_id, dfreq in dfs.items()]\n",
    "vocab_df = pd.DataFrame(records, columns=[\"token_id\", \"token\", \"df\"])\\\n",
    "            .sort_values(by=[\"df\", \"token\"], ascending=[False, True])\n",
    "vocab_df.to_csv(VOCAB_TERMS_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# bow_index.csv (row_idx <-> DOC_ID)\n",
    "bow_index_df = pd.DataFrame({\"row_idx\": range(n_docs_used), \"DOC_ID\": doc_ids_used})\n",
    "bow_index_df.to_csv(BOW_INDEX_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Arquivos gravados:\")\n",
    "print(\" -\", DICT_PATH.name)\n",
    "print(\" -\", BOW_MM_PATH.name)\n",
    "print(\" -\", VOCAB_TERMS_CSV.name)\n",
    "print(\" -\", BOW_INDEX_CSV.name)\n",
    "\n",
    "# Checagens do bow_index: monotonicidade e abrangência\n",
    "# Monotonicidade estrita em DOC_ID\n",
    "is_monotonic_strict = bow_index_df[\"DOC_ID\"].is_monotonic_increasing and bow_index_df[\"DOC_ID\"].is_unique\n",
    "assert is_monotonic_strict, (\n",
    "    \"DOC_ID em bow_index não está estritamente crescente/único — verifique ordenação e filtragem.\"\n",
    ")\n",
    "\n",
    "# Abrangência: todos os DOC_ID usados estão contidos nos DOC_ID selecionados originalmente (pós-ordenação)\n",
    "set_used = set(doc_ids_used)\n",
    "set_selected = set(doc_ids)\n",
    "assert set_used.issubset(set_selected), \"Existem DOC_ID em bow_index que não pertencem ao conjunto selecionado.\"\n",
    "assert len(set_used) == n_docs_used, \"Tamanho do conjunto de DOC_ID usados difere de n_docs_used.\"\n",
    "\n",
    "print(\"bow_index: checagens OK (monotonicidade estrita e abrangência confirmadas).\")\n",
    "\n",
    "# Exportar CSR: bow_csr.npz para diagnósticos auxiliares\n",
    "try:\n",
    "    from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "    if vocab_final > 0 and n_docs_used > 0:\n",
    "        data = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        for r, doc in enumerate(corpus_bow):\n",
    "            if doc:  # lista de (term_id, count)\n",
    "                c_idx, c_val = zip(*doc)\n",
    "                rows.extend([r] * len(c_idx))\n",
    "                cols.extend(c_idx)\n",
    "                data.extend(c_val)\n",
    "        csr = csr_matrix((data, (rows, cols)), shape=(n_docs_used, vocab_final), dtype=np.float64)\n",
    "\n",
    "        CSR_PATH = INTERIM_LDA / \"bow_csr.npz\"\n",
    "        save_npz(str(CSR_PATH), csr)\n",
    "        # Validação rápida\n",
    "        assert csr.nnz == nnz, f\"CSR nnz ({csr.nnz}) != nnz do corpus_bow ({nnz})\"\n",
    "        print(f\"CSR salvo em: {CSR_PATH.name} (shape={csr.shape}, nnz={csr.nnz}, densidade={csr.nnz/(csr.shape[0]*csr.shape[1]):.6f})\")\n",
    "    else:\n",
    "        print(\"CSR não gerado (vocab_final ou n_docs_used = 0).\")\n",
    "except ImportError:\n",
    "    print(\"scipy.sparse não disponível — pulando exportação de bow_csr.npz (opcional).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14fed598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 termos por DF:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>utilizar</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>trabalho</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>objetivo</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>dado</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>uso</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>usuario</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>estudo</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ferramenta</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>resultado</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>contexto</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>realizar</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>aplicacao</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>modelo</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>analise</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>desenvolvimento</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               token   df\n",
       "65          utilizar  186\n",
       "159         trabalho  180\n",
       "126         objetivo  142\n",
       "141             dado  138\n",
       "131              uso  134\n",
       "35           usuario  121\n",
       "81            estudo  117\n",
       "58        ferramenta  113\n",
       "11         resultado  110\n",
       "55          contexto  109\n",
       "179         realizar  109\n",
       "279        aplicacao   99\n",
       "79            modelo   93\n",
       "71           analise   92\n",
       "16   desenvolvimento   90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manifest salvo em: C:\\Users\\User\\Desktop\\TCC\\Notebooks locais\\analise_topicos_tcc\\data\\interim\\lda\\bow_manifest.json\n",
      "{\n",
      "  \"docs_totais\": 423,\n",
      "  \"docs_usados\": 423,\n",
      "  \"vocab_inicial\": 4968,\n",
      "  \"vocab_final\": 1218,\n",
      "  \"nnz\": 20066,\n",
      "  \"sparsidade\": 0.038946923026160006,\n",
      "  \"no_below\": 5,\n",
      "  \"no_above\": 0.5,\n",
      "  \"df_min\": 5,\n",
      "  \"df_max\": 186,\n",
      "  \"bow_shape\": [\n",
      "    423,\n",
      "    1218\n",
      "  ],\n",
      "  \"artefatos\": {\n",
      "    \"dictionary\": \"..\\\\..\\\\data\\\\interim\\\\lda\\\\vocab_bow.dict\",\n",
      "    \"mm_corpus\": \"..\\\\..\\\\data\\\\interim\\\\lda\\\\bow.mm\",\n",
      "    \"vocab_terms\": \"..\\\\..\\\\data\\\\interim\\\\lda\\\\vocab_terms.csv\",\n",
      "    \"bow_index\": \"..\\\\..\\\\data\\\\interim\\\\lda\\\\bow_index.csv\",\n",
      "    \"csr_npz\": \"..\\\\..\\\\data\\\\interim\\\\lda\\\\bow_csr.npz\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Relatório rápido\n",
    "import json\n",
    "\n",
    "# Top 15 termos por DF\n",
    "print(\"\\nTop 15 termos por DF:\")\n",
    "display(vocab_df.head(15)[[\"token\", \"df\"]])\n",
    "\n",
    "# Manifest de auditoria para próxima etapa (LDA)\n",
    "manifest = {\n",
    "    \"docs_totais\": int(total_docs),\n",
    "    \"docs_usados\": int(n_docs_used),\n",
    "    \"vocab_inicial\": int(vocab_init),\n",
    "    \"vocab_final\": int(vocab_final),\n",
    "    \"nnz\": int(nnz),\n",
    "    \"sparsidade\": float(sparsity),\n",
    "    \"no_below\": 5,\n",
    "    \"no_above\": 0.5,\n",
    "    \"df_min\": int(vocab_df[\"df\"].min()),\n",
    "    \"df_max\": int(vocab_df[\"df\"].max()),\n",
    "    \"bow_shape\": [int(n_docs_used), int(vocab_final)],\n",
    "    \"artefatos\": {\n",
    "        \"dictionary\": str(DICT_PATH),\n",
    "        \"mm_corpus\": str(BOW_MM_PATH),\n",
    "        \"vocab_terms\": str(VOCAB_TERMS_CSV),\n",
    "        \"bow_index\": str(BOW_INDEX_CSV),\n",
    "        \"csr_npz\": str(INTERIM_LDA / \"bow_csr.npz\")\n",
    "    }\n",
    "}\n",
    "with open(INTERIM_LDA / \"bow_manifest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nManifest salvo em:\", (INTERIM_LDA / \"bow_manifest.json\").resolve())\n",
    "print(json.dumps(manifest, ensure_ascii=False, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lda 3.12)",
   "language": "python",
   "name": "lda-312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
